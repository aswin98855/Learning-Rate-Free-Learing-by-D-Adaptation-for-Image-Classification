{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba63e19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T13:57:02.290342Z",
     "iopub.status.busy": "2024-10-23T13:57:02.290023Z",
     "iopub.status.idle": "2024-10-23T13:57:05.505300Z",
     "shell.execute_reply": "2024-10-23T13:57:05.504529Z"
    },
    "papermill": {
     "duration": 3.221669,
     "end_time": "2024-10-23T13:57:05.507657",
     "exception": false,
     "start_time": "2024-10-23T13:57:02.285988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import TYPE_CHECKING, Any, Callable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import pdb\n",
    "import logging\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from torch.optim.optimizer import _params_t\n",
    "else:\n",
    "    _params_t = Any\n",
    "\n",
    "class DAdaptAdam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1.0,\n",
    "                betas=(0.9, 0.999), eps=1e-8,\n",
    "                weight_decay=0, log_every=0,\n",
    "                decouple=False,\n",
    "                use_bias_correction=False,\n",
    "                d0=1e-6, growth_rate=float('inf'),\n",
    "                fsdp_in_use=False):\n",
    "        if not 0.0 < d0:\n",
    "            raise ValueError(\"Invalid d0 value: {}\".format(d0))\n",
    "        if not 0.0 < lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 < eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        if decouple:\n",
    "            print(f\"Using decoupled weight decay\")\n",
    "\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        d = d0,\n",
    "                        k=0,\n",
    "                        layer_scale=1.0,\n",
    "                        numerator_weighted=0.0,\n",
    "                        log_every=log_every,\n",
    "                        growth_rate=growth_rate,\n",
    "                        use_bias_correction=use_bias_correction,\n",
    "                        decouple=decouple,\n",
    "                        fsdp_in_use=fsdp_in_use)\n",
    "        self.d0 = d0\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @property\n",
    "    def supports_memory_efficient_fp16(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def supports_flat_params(self):\n",
    "        return True\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        sk_l1 = 0.0\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        use_bias_correction = group['use_bias_correction']\n",
    "        numerator_weighted = group['numerator_weighted']\n",
    "        beta1, beta2 = group['betas']\n",
    "        k = group['k']\n",
    "\n",
    "        d = group['d']\n",
    "        lr = max(group['lr'] for group in self.param_groups)\n",
    "\n",
    "        if use_bias_correction:\n",
    "            bias_correction = ((1-beta2**(k+1))**0.5)/(1-beta1**(k+1))\n",
    "        else:\n",
    "            bias_correction = 1\n",
    "\n",
    "        dlr = d*lr*bias_correction\n",
    "\n",
    "        growth_rate = group['growth_rate']\n",
    "        decouple = group['decouple']\n",
    "        log_every = group['log_every']\n",
    "        fsdp_in_use = group['fsdp_in_use']\n",
    "\n",
    "\n",
    "        sqrt_beta2 = beta2**(0.5)\n",
    "\n",
    "        numerator_acum = 0.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            decay = group['weight_decay']\n",
    "            k = group['k']\n",
    "            eps = group['eps']\n",
    "            group_lr = group['lr']\n",
    "            r = group['layer_scale']\n",
    "\n",
    "            if group_lr not in [lr, 0.0]:\n",
    "                raise RuntimeError(f\"Setting different lr values in different parameter groups \"\n",
    "                                \"is only supported for values of 0. To scale the learning \"\n",
    "                                \"rate differently for each layer, set the 'layer_scale' value instead.\")\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if hasattr(p, \"_fsdp_flattened\"):\n",
    "                    fsdp_in_use = True\n",
    "\n",
    "                grad = p.grad.data\n",
    "\n",
    "                if decay != 0 and not decouple:\n",
    "                    grad.add_(p.data, alpha=decay)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if 'step' not in state:\n",
    "                    state['step'] = 0\n",
    "                    state['s'] = torch.zeros_like(p.data).detach()\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data).detach()\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data).detach()\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                s = state['s']\n",
    "\n",
    "                if group_lr > 0.0:\n",
    "                    denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                    numerator_acum += r * dlr * torch.dot(grad.flatten(), s.div(denom).flatten()).item()\n",
    "\n",
    "                    exp_avg.mul_(beta1).add_(grad, alpha=r*dlr*(1-beta1))\n",
    "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "\n",
    "                    s.mul_(sqrt_beta2).add_(grad, alpha=dlr*(1-sqrt_beta2))\n",
    "                    sk_l1 += r * s.abs().sum().item()\n",
    "\n",
    "\n",
    "        numerator_weighted = sqrt_beta2*numerator_weighted + (1-sqrt_beta2)*numerator_acum\n",
    "        d_hat = d\n",
    "        if sk_l1 == 0:\n",
    "            return loss\n",
    "\n",
    "        if lr > 0.0:\n",
    "            if fsdp_in_use:\n",
    "                dist_tensor = torch.zeros(2).cuda()\n",
    "                dist_tensor[0] = numerator_weighted\n",
    "                dist_tensor[1] = sk_l1\n",
    "                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)\n",
    "                global_numerator_weighted = dist_tensor[0]\n",
    "                global_sk_l1 = dist_tensor[1]\n",
    "            else:\n",
    "                global_numerator_weighted = numerator_weighted\n",
    "                global_sk_l1 = sk_l1\n",
    "\n",
    "\n",
    "            d_hat = global_numerator_weighted/((1-sqrt_beta2)*global_sk_l1)\n",
    "            d = max(d, min(d_hat, d*growth_rate))\n",
    "\n",
    "        if log_every > 0 and k % log_every == 0:\n",
    "            logging.info(f\"lr: {lr} dlr: {dlr} d_hat: {d_hat}, d: {d}. sk_l1={global_sk_l1:1.1e} numerator_weighted={global_numerator_weighted:1.1e}\")\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            group['numerator_weighted'] = numerator_weighted\n",
    "            group['d'] = d\n",
    "\n",
    "            decay = group['weight_decay']\n",
    "            k = group['k']\n",
    "            eps = group['eps']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "\n",
    "                if decay != 0 and decouple:\n",
    "                    p.data.add_(p.data, alpha=-decay * dlr)\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-1)\n",
    "\n",
    "            group['k'] = k + 1\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30218ae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T13:57:05.513341Z",
     "iopub.status.busy": "2024-10-23T13:57:05.512967Z",
     "iopub.status.idle": "2024-10-23T23:41:48.667525Z",
     "shell.execute_reply": "2024-10-23T23:41:48.666553Z"
    },
    "papermill": {
     "duration": 35083.16033,
     "end_time": "2024-10-23T23:41:48.670275",
     "exception": false,
     "start_time": "2024-10-23T13:57:05.509945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:11<00:00, 15469334.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n",
      "Starting training for hyperparameter configuration 1/3\n",
      "Epoch 1, Loss: 1.8479507211833963, Training Accuracy: 30.64%\n",
      "Epoch 2, Loss: 1.5265347352418144, Training Accuracy: 43.35%\n",
      "Epoch 3, Loss: 1.419721240735115, Training Accuracy: 47.86%\n",
      "Epoch 4, Loss: 1.3564787116806831, Training Accuracy: 50.18%\n",
      "Epoch 5, Loss: 1.3204516827907709, Training Accuracy: 51.68%\n",
      "Epoch 6, Loss: 1.298067217592693, Training Accuracy: 52.95%\n",
      "Epoch 7, Loss: 1.2745878688057366, Training Accuracy: 53.90%\n",
      "Epoch 8, Loss: 1.2567446345410993, Training Accuracy: 54.18%\n",
      "Epoch 9, Loss: 1.2421544243765, Training Accuracy: 55.23%\n",
      "Epoch 10, Loss: 1.2313954300435304, Training Accuracy: 55.47%\n",
      "Epoch 11, Loss: 1.221115350951929, Training Accuracy: 55.98%\n",
      "Epoch 12, Loss: 1.2071787481722625, Training Accuracy: 56.33%\n",
      "Epoch 13, Loss: 1.2027929135600623, Training Accuracy: 56.63%\n",
      "Epoch 14, Loss: 1.1948651246859898, Training Accuracy: 56.82%\n",
      "Epoch 15, Loss: 1.186678238003455, Training Accuracy: 57.17%\n",
      "Epoch 16, Loss: 1.1869474725650095, Training Accuracy: 57.28%\n",
      "Epoch 17, Loss: 1.1786602595273186, Training Accuracy: 57.27%\n",
      "Epoch 18, Loss: 1.175839531299708, Training Accuracy: 57.35%\n",
      "Epoch 19, Loss: 1.1710104604663751, Training Accuracy: 57.90%\n",
      "Epoch 20, Loss: 1.1673450604881472, Training Accuracy: 57.71%\n",
      "Epoch 21, Loss: 1.1583116325118659, Training Accuracy: 58.38%\n",
      "Epoch 22, Loss: 1.1598225934883517, Training Accuracy: 58.08%\n",
      "Epoch 23, Loss: 1.1527916955216158, Training Accuracy: 58.47%\n",
      "Epoch 24, Loss: 1.1470227051726387, Training Accuracy: 58.36%\n",
      "Epoch 25, Loss: 1.150961915016784, Training Accuracy: 58.44%\n",
      "Epoch 26, Loss: 1.145664689790867, Training Accuracy: 58.72%\n",
      "Epoch 27, Loss: 1.1405440420293442, Training Accuracy: 58.89%\n",
      "Epoch 28, Loss: 1.140157161771184, Training Accuracy: 58.68%\n",
      "Epoch 29, Loss: 1.1340052894771557, Training Accuracy: 59.20%\n",
      "Epoch 30, Loss: 1.1372594780019483, Training Accuracy: 59.24%\n",
      "Epoch 31, Loss: 1.1344812657979444, Training Accuracy: 59.02%\n",
      "Epoch 32, Loss: 1.1346591295641097, Training Accuracy: 59.30%\n",
      "Epoch 33, Loss: 1.1322994586604331, Training Accuracy: 59.23%\n",
      "Epoch 34, Loss: 1.1270716751323027, Training Accuracy: 59.50%\n",
      "Epoch 35, Loss: 1.1218384756608997, Training Accuracy: 59.63%\n",
      "Epoch 36, Loss: 1.1216827057054282, Training Accuracy: 59.68%\n",
      "Epoch 37, Loss: 1.121374555179835, Training Accuracy: 59.85%\n",
      "Epoch 38, Loss: 1.12269100165733, Training Accuracy: 59.66%\n",
      "Epoch 39, Loss: 1.1155306484235827, Training Accuracy: 59.81%\n",
      "Epoch 40, Loss: 1.1173060450255108, Training Accuracy: 59.74%\n",
      "Epoch 41, Loss: 1.1158657420779128, Training Accuracy: 59.92%\n",
      "Epoch 42, Loss: 1.1123871775082006, Training Accuracy: 60.08%\n",
      "Epoch 43, Loss: 1.1153409137292896, Training Accuracy: 59.92%\n",
      "Epoch 44, Loss: 1.1102872019838494, Training Accuracy: 60.10%\n",
      "Epoch 45, Loss: 1.1099401338935813, Training Accuracy: 60.02%\n",
      "Epoch 46, Loss: 1.1148794988537079, Training Accuracy: 60.08%\n",
      "Epoch 47, Loss: 1.105471264675755, Training Accuracy: 60.14%\n",
      "Epoch 48, Loss: 1.1069911348697778, Training Accuracy: 59.99%\n",
      "Epoch 49, Loss: 1.1064069258892322, Training Accuracy: 60.20%\n",
      "Epoch 50, Loss: 1.1048422592222844, Training Accuracy: 60.23%\n",
      "Epoch 51, Loss: 1.104379793795783, Training Accuracy: 60.44%\n",
      "Epoch 52, Loss: 1.099921219336712, Training Accuracy: 60.37%\n",
      "Epoch 53, Loss: 1.1036967237282287, Training Accuracy: 60.35%\n",
      "Epoch 54, Loss: 1.101808676024532, Training Accuracy: 60.31%\n",
      "Epoch 55, Loss: 1.1000795822466731, Training Accuracy: 60.59%\n",
      "Epoch 56, Loss: 1.099272246251021, Training Accuracy: 60.46%\n",
      "Epoch 57, Loss: 1.1012950654682296, Training Accuracy: 60.38%\n",
      "Epoch 58, Loss: 1.1013945761848898, Training Accuracy: 60.41%\n",
      "Epoch 59, Loss: 1.0994710264455936, Training Accuracy: 60.31%\n",
      "Epoch 60, Loss: 1.0951515761635187, Training Accuracy: 60.67%\n",
      "Epoch 61, Loss: 1.0947661161270288, Training Accuracy: 60.60%\n",
      "Epoch 62, Loss: 1.0928573098481464, Training Accuracy: 60.71%\n",
      "Epoch 63, Loss: 1.094311254484879, Training Accuracy: 60.58%\n",
      "Epoch 64, Loss: 1.0908639232825745, Training Accuracy: 60.98%\n",
      "Epoch 65, Loss: 1.0941929491737006, Training Accuracy: 60.70%\n",
      "Epoch 66, Loss: 1.0887858318093488, Training Accuracy: 60.67%\n",
      "Epoch 67, Loss: 1.091845234184314, Training Accuracy: 60.91%\n",
      "Epoch 68, Loss: 1.09424511223193, Training Accuracy: 60.52%\n",
      "Epoch 69, Loss: 1.08929171739027, Training Accuracy: 60.93%\n",
      "Epoch 70, Loss: 1.0883624748043392, Training Accuracy: 60.84%\n",
      "Epoch 71, Loss: 1.0843616430564305, Training Accuracy: 61.04%\n",
      "Epoch 72, Loss: 1.0853564005221248, Training Accuracy: 61.18%\n",
      "Epoch 73, Loss: 1.0929694950123272, Training Accuracy: 60.83%\n",
      "Epoch 74, Loss: 1.0855534995150993, Training Accuracy: 60.80%\n",
      "Epoch 75, Loss: 1.0845518146481965, Training Accuracy: 60.86%\n",
      "Epoch 76, Loss: 1.0845175356511265, Training Accuracy: 61.15%\n",
      "Epoch 77, Loss: 1.0879138279753877, Training Accuracy: 60.80%\n",
      "Epoch 78, Loss: 1.087166239233578, Training Accuracy: 60.82%\n",
      "Epoch 79, Loss: 1.0835101666962703, Training Accuracy: 60.87%\n",
      "Epoch 80, Loss: 1.0838990123832928, Training Accuracy: 60.95%\n",
      "Epoch 81, Loss: 1.0798831425054605, Training Accuracy: 61.10%\n",
      "Epoch 82, Loss: 1.084252298030707, Training Accuracy: 60.99%\n",
      "Epoch 83, Loss: 1.0827605367621498, Training Accuracy: 61.00%\n",
      "Epoch 84, Loss: 1.0797453832900739, Training Accuracy: 61.19%\n",
      "Epoch 85, Loss: 1.0837748732103412, Training Accuracy: 60.89%\n",
      "Epoch 86, Loss: 1.0899686383469331, Training Accuracy: 60.88%\n",
      "Epoch 87, Loss: 1.0790151916349027, Training Accuracy: 61.35%\n",
      "Epoch 88, Loss: 1.0807192360653597, Training Accuracy: 61.00%\n",
      "Epoch 89, Loss: 1.079398474196339, Training Accuracy: 61.24%\n",
      "Epoch 90, Loss: 1.0803205059160053, Training Accuracy: 61.44%\n",
      "Epoch 91, Loss: 1.0846017235532746, Training Accuracy: 61.19%\n",
      "Epoch 92, Loss: 1.07813286087702, Training Accuracy: 61.24%\n",
      "Epoch 93, Loss: 1.078188469800193, Training Accuracy: 61.39%\n",
      "Epoch 94, Loss: 1.0814114960715593, Training Accuracy: 61.23%\n",
      "Epoch 95, Loss: 1.0761591676251052, Training Accuracy: 61.29%\n",
      "Epoch 96, Loss: 1.071987851730088, Training Accuracy: 61.44%\n",
      "Epoch 97, Loss: 1.0800060183190934, Training Accuracy: 61.10%\n",
      "Epoch 98, Loss: 1.0755297373170438, Training Accuracy: 61.42%\n",
      "Epoch 99, Loss: 1.0758783579482447, Training Accuracy: 61.17%\n",
      "Epoch 100, Loss: 1.0782419549077369, Training Accuracy: 61.27%\n",
      "Epoch 101, Loss: 1.074809359391327, Training Accuracy: 61.31%\n",
      "Epoch 102, Loss: 1.0751930486668102, Training Accuracy: 61.37%\n",
      "Epoch 103, Loss: 1.0743770700738864, Training Accuracy: 61.47%\n",
      "Epoch 104, Loss: 1.072207456659478, Training Accuracy: 61.45%\n",
      "Epoch 105, Loss: 1.070670651276703, Training Accuracy: 61.59%\n",
      "Epoch 106, Loss: 1.071425804549166, Training Accuracy: 61.75%\n",
      "Epoch 107, Loss: 1.0723123649502044, Training Accuracy: 61.62%\n",
      "Epoch 108, Loss: 1.0702339499197957, Training Accuracy: 61.78%\n",
      "Epoch 109, Loss: 1.0756404713139205, Training Accuracy: 61.37%\n",
      "Epoch 110, Loss: 1.0719755903229384, Training Accuracy: 61.35%\n",
      "Epoch 111, Loss: 1.0704939002576082, Training Accuracy: 61.54%\n",
      "Epoch 112, Loss: 1.0702881786372045, Training Accuracy: 61.69%\n",
      "Epoch 113, Loss: 1.071290040869847, Training Accuracy: 61.72%\n",
      "Epoch 114, Loss: 1.076049292102799, Training Accuracy: 61.37%\n",
      "Epoch 115, Loss: 1.0691027196929277, Training Accuracy: 61.74%\n",
      "Epoch 116, Loss: 1.0700802613249825, Training Accuracy: 61.48%\n",
      "Epoch 117, Loss: 1.069082008145959, Training Accuracy: 61.65%\n",
      "Epoch 118, Loss: 1.0658365475094838, Training Accuracy: 61.88%\n",
      "Epoch 119, Loss: 1.071520121582329, Training Accuracy: 61.49%\n",
      "Epoch 120, Loss: 1.0706925413492696, Training Accuracy: 61.73%\n",
      "Epoch 121, Loss: 1.0719373794772742, Training Accuracy: 61.61%\n",
      "Epoch 122, Loss: 1.0677229514359818, Training Accuracy: 61.72%\n",
      "Epoch 123, Loss: 1.0667245004640515, Training Accuracy: 61.78%\n",
      "Epoch 124, Loss: 1.0682566302359258, Training Accuracy: 61.71%\n",
      "Epoch 125, Loss: 1.0648348014373, Training Accuracy: 61.85%\n",
      "Epoch 126, Loss: 1.0712971771921953, Training Accuracy: 61.49%\n",
      "Epoch 127, Loss: 1.0680476004815163, Training Accuracy: 61.55%\n",
      "Epoch 128, Loss: 1.0679833712175375, Training Accuracy: 61.53%\n",
      "Epoch 129, Loss: 1.0664966508860478, Training Accuracy: 61.64%\n",
      "Epoch 130, Loss: 1.0644584023739065, Training Accuracy: 61.65%\n",
      "Epoch 131, Loss: 1.0670168620088827, Training Accuracy: 61.64%\n",
      "Epoch 132, Loss: 1.0640764660237696, Training Accuracy: 61.79%\n",
      "Epoch 133, Loss: 1.0691208212881746, Training Accuracy: 61.81%\n",
      "Epoch 134, Loss: 1.0633707675330168, Training Accuracy: 61.89%\n",
      "Epoch 135, Loss: 1.0681614208099481, Training Accuracy: 61.58%\n",
      "Epoch 136, Loss: 1.065816977139934, Training Accuracy: 61.81%\n",
      "Epoch 137, Loss: 1.0642477315100258, Training Accuracy: 61.84%\n",
      "Epoch 138, Loss: 1.0599992934547726, Training Accuracy: 61.90%\n",
      "Epoch 139, Loss: 1.0622452168208558, Training Accuracy: 61.94%\n",
      "Epoch 140, Loss: 1.0628068381563172, Training Accuracy: 61.94%\n",
      "Epoch 141, Loss: 1.0587401021929348, Training Accuracy: 61.91%\n",
      "Epoch 142, Loss: 1.0649153091718473, Training Accuracy: 61.77%\n",
      "Epoch 143, Loss: 1.061656613865167, Training Accuracy: 61.77%\n",
      "Epoch 144, Loss: 1.0654991102950346, Training Accuracy: 61.59%\n",
      "Epoch 145, Loss: 1.064739555196689, Training Accuracy: 61.63%\n",
      "Epoch 146, Loss: 1.0615776010486475, Training Accuracy: 61.99%\n",
      "Epoch 147, Loss: 1.0667731216191636, Training Accuracy: 61.68%\n",
      "Epoch 148, Loss: 1.0607436490638176, Training Accuracy: 62.03%\n",
      "Epoch 149, Loss: 1.0635750590992705, Training Accuracy: 61.96%\n",
      "Epoch 150, Loss: 1.0623835467773934, Training Accuracy: 61.77%\n",
      "Epoch 151, Loss: 1.0617221333943974, Training Accuracy: 61.83%\n",
      "Epoch 152, Loss: 1.0620858495497643, Training Accuracy: 61.96%\n",
      "Epoch 153, Loss: 1.0619666060370863, Training Accuracy: 61.93%\n",
      "Epoch 154, Loss: 1.062699728914539, Training Accuracy: 61.91%\n",
      "Epoch 155, Loss: 1.0616067109053091, Training Accuracy: 61.95%\n",
      "Epoch 156, Loss: 1.0626018688349468, Training Accuracy: 61.87%\n",
      "Epoch 157, Loss: 1.0618292490387207, Training Accuracy: 61.85%\n",
      "Epoch 158, Loss: 1.0603561726829889, Training Accuracy: 61.54%\n",
      "Epoch 159, Loss: 1.0583237127574814, Training Accuracy: 62.08%\n",
      "Epoch 160, Loss: 1.058973122283321, Training Accuracy: 61.92%\n",
      "Epoch 161, Loss: 1.0550066228108028, Training Accuracy: 61.97%\n",
      "Epoch 162, Loss: 1.059263868100198, Training Accuracy: 61.92%\n",
      "Epoch 163, Loss: 1.0573374636642767, Training Accuracy: 62.02%\n",
      "Epoch 164, Loss: 1.0584712450004294, Training Accuracy: 61.91%\n",
      "Epoch 165, Loss: 1.0562658998210106, Training Accuracy: 61.97%\n",
      "Epoch 166, Loss: 1.0567552233900865, Training Accuracy: 62.07%\n",
      "Epoch 167, Loss: 1.0554779293134695, Training Accuracy: 62.11%\n",
      "Epoch 168, Loss: 1.0539173247564175, Training Accuracy: 62.14%\n",
      "Epoch 169, Loss: 1.0589022998462247, Training Accuracy: 61.88%\n",
      "Epoch 170, Loss: 1.0566758545463348, Training Accuracy: 62.17%\n",
      "Epoch 171, Loss: 1.0563384227435608, Training Accuracy: 61.94%\n",
      "Epoch 172, Loss: 1.0541836546205194, Training Accuracy: 62.09%\n",
      "Epoch 173, Loss: 1.0554842892510201, Training Accuracy: 62.18%\n",
      "Epoch 174, Loss: 1.054143121785215, Training Accuracy: 62.09%\n",
      "Epoch 175, Loss: 1.0617827864559106, Training Accuracy: 61.80%\n",
      "Accuracy of the network on the 10000 test images: 54.58%\n",
      "Starting training for hyperparameter configuration 2/3\n",
      "Epoch 1, Loss: 1.5603151472328265, Training Accuracy: 42.77%\n",
      "Epoch 2, Loss: 1.3083750213046208, Training Accuracy: 52.95%\n",
      "Epoch 3, Loss: 1.24148625257375, Training Accuracy: 55.73%\n",
      "Epoch 4, Loss: 1.2027536389010642, Training Accuracy: 56.85%\n",
      "Epoch 5, Loss: 1.1863758313991224, Training Accuracy: 57.86%\n",
      "Epoch 6, Loss: 1.1664956109907927, Training Accuracy: 58.49%\n",
      "Epoch 7, Loss: 1.1584037338071467, Training Accuracy: 59.03%\n",
      "Epoch 8, Loss: 1.147161890250033, Training Accuracy: 59.46%\n",
      "Epoch 9, Loss: 1.1396099607962782, Training Accuracy: 59.82%\n",
      "Epoch 10, Loss: 1.1346743889629383, Training Accuracy: 60.04%\n",
      "Epoch 11, Loss: 1.1219556605267098, Training Accuracy: 60.62%\n",
      "Epoch 12, Loss: 1.1175013638823235, Training Accuracy: 60.55%\n",
      "Epoch 13, Loss: 1.1113548102738606, Training Accuracy: 60.99%\n",
      "Epoch 14, Loss: 1.1083804270647981, Training Accuracy: 61.19%\n",
      "Epoch 15, Loss: 1.106956692379149, Training Accuracy: 61.10%\n",
      "Epoch 16, Loss: 1.0984847929776478, Training Accuracy: 61.69%\n",
      "Epoch 17, Loss: 1.0950344243775243, Training Accuracy: 61.45%\n",
      "Epoch 18, Loss: 1.0910816992182866, Training Accuracy: 61.58%\n",
      "Epoch 19, Loss: 1.0864033796598234, Training Accuracy: 61.82%\n",
      "Epoch 20, Loss: 1.0814727709421417, Training Accuracy: 62.26%\n",
      "Epoch 21, Loss: 1.0817570152032712, Training Accuracy: 62.19%\n",
      "Epoch 22, Loss: 1.0730524143904372, Training Accuracy: 62.38%\n",
      "Epoch 23, Loss: 1.0785507522428128, Training Accuracy: 62.27%\n",
      "Epoch 24, Loss: 1.0769160831980693, Training Accuracy: 62.48%\n",
      "Epoch 25, Loss: 1.0753997219035694, Training Accuracy: 62.67%\n",
      "Epoch 26, Loss: 1.0740401140411797, Training Accuracy: 62.51%\n",
      "Epoch 27, Loss: 1.072813135050142, Training Accuracy: 62.47%\n",
      "Epoch 28, Loss: 1.0732400368546586, Training Accuracy: 62.55%\n",
      "Epoch 29, Loss: 1.0670025584947727, Training Accuracy: 62.97%\n",
      "Epoch 30, Loss: 1.0672451067153754, Training Accuracy: 62.68%\n",
      "Epoch 31, Loss: 1.065520968126214, Training Accuracy: 62.85%\n",
      "Epoch 32, Loss: 1.0664976467104519, Training Accuracy: 63.04%\n",
      "Epoch 33, Loss: 1.0702259948339, Training Accuracy: 62.78%\n",
      "Epoch 34, Loss: 1.0631760627107547, Training Accuracy: 62.95%\n",
      "Epoch 35, Loss: 1.0673106630592395, Training Accuracy: 62.82%\n",
      "Epoch 36, Loss: 1.0665899335270952, Training Accuracy: 62.88%\n",
      "Epoch 37, Loss: 1.0659646103754068, Training Accuracy: 62.69%\n",
      "Epoch 38, Loss: 1.0624033958100907, Training Accuracy: 62.95%\n",
      "Epoch 39, Loss: 1.0654890991537773, Training Accuracy: 62.92%\n",
      "Epoch 40, Loss: 1.0666847123819239, Training Accuracy: 62.73%\n",
      "Epoch 41, Loss: 1.0629789894041808, Training Accuracy: 62.99%\n",
      "Epoch 42, Loss: 1.0654706544888295, Training Accuracy: 63.04%\n",
      "Epoch 43, Loss: 1.0654344905520339, Training Accuracy: 62.83%\n",
      "Epoch 44, Loss: 1.0574971487759934, Training Accuracy: 63.22%\n",
      "Epoch 45, Loss: 1.0620737062085925, Training Accuracy: 63.00%\n",
      "Epoch 46, Loss: 1.060517362697655, Training Accuracy: 63.07%\n",
      "Epoch 47, Loss: 1.0620052426519906, Training Accuracy: 63.13%\n",
      "Epoch 48, Loss: 1.0622915504381174, Training Accuracy: 63.06%\n",
      "Epoch 49, Loss: 1.0608526302877899, Training Accuracy: 63.15%\n",
      "Epoch 50, Loss: 1.0605135594335053, Training Accuracy: 63.00%\n",
      "Epoch 51, Loss: 1.0610372678703055, Training Accuracy: 62.89%\n",
      "Epoch 52, Loss: 1.0619225843483224, Training Accuracy: 63.07%\n",
      "Epoch 53, Loss: 1.0600309660062766, Training Accuracy: 63.16%\n",
      "Epoch 54, Loss: 1.0602746310136508, Training Accuracy: 63.33%\n",
      "Epoch 55, Loss: 1.0588871761965934, Training Accuracy: 63.29%\n",
      "Epoch 56, Loss: 1.0557450104095136, Training Accuracy: 62.88%\n",
      "Epoch 57, Loss: 1.058730301649674, Training Accuracy: 63.20%\n",
      "Epoch 58, Loss: 1.061842754673775, Training Accuracy: 62.84%\n",
      "Epoch 59, Loss: 1.0594158579626352, Training Accuracy: 63.01%\n",
      "Epoch 60, Loss: 1.060446462820253, Training Accuracy: 63.00%\n",
      "Epoch 61, Loss: 1.0594445615625747, Training Accuracy: 63.08%\n",
      "Epoch 62, Loss: 1.0564300708301233, Training Accuracy: 63.22%\n",
      "Epoch 63, Loss: 1.058578570099438, Training Accuracy: 62.99%\n",
      "Epoch 64, Loss: 1.0567513083100624, Training Accuracy: 63.03%\n",
      "Epoch 65, Loss: 1.056748740279766, Training Accuracy: 63.20%\n",
      "Epoch 66, Loss: 1.0563483389137347, Training Accuracy: 63.17%\n",
      "Epoch 67, Loss: 1.0570560435352423, Training Accuracy: 63.34%\n",
      "Epoch 68, Loss: 1.0531646437047388, Training Accuracy: 63.41%\n",
      "Epoch 69, Loss: 1.0576748613963651, Training Accuracy: 63.33%\n",
      "Epoch 70, Loss: 1.0592176979002745, Training Accuracy: 63.01%\n",
      "Epoch 71, Loss: 1.0571658221047249, Training Accuracy: 63.23%\n",
      "Epoch 72, Loss: 1.0566042644136093, Training Accuracy: 62.90%\n",
      "Epoch 73, Loss: 1.0583737005510598, Training Accuracy: 63.10%\n",
      "Epoch 74, Loss: 1.054837727257053, Training Accuracy: 63.26%\n",
      "Epoch 75, Loss: 1.056259393920679, Training Accuracy: 63.10%\n",
      "Epoch 76, Loss: 1.0568269333418678, Training Accuracy: 63.33%\n",
      "Epoch 77, Loss: 1.0512135223964292, Training Accuracy: 63.34%\n",
      "Epoch 78, Loss: 1.0562892863360207, Training Accuracy: 63.40%\n",
      "Epoch 79, Loss: 1.0558878290836158, Training Accuracy: 63.36%\n",
      "Epoch 80, Loss: 1.0586858740090714, Training Accuracy: 63.04%\n",
      "Epoch 81, Loss: 1.054865491664623, Training Accuracy: 63.55%\n",
      "Epoch 82, Loss: 1.0608287915549315, Training Accuracy: 63.16%\n",
      "Epoch 83, Loss: 1.05288563779248, Training Accuracy: 63.47%\n",
      "Epoch 84, Loss: 1.0566922705953994, Training Accuracy: 63.35%\n",
      "Epoch 85, Loss: 1.0581810084145393, Training Accuracy: 63.06%\n",
      "Epoch 86, Loss: 1.0543813726023945, Training Accuracy: 63.28%\n",
      "Epoch 87, Loss: 1.0537680373015001, Training Accuracy: 63.26%\n",
      "Epoch 88, Loss: 1.0561831078261061, Training Accuracy: 63.30%\n",
      "Epoch 89, Loss: 1.0550709848513689, Training Accuracy: 63.18%\n",
      "Epoch 90, Loss: 1.055812742018029, Training Accuracy: 63.26%\n",
      "Epoch 91, Loss: 1.053371259089931, Training Accuracy: 63.47%\n",
      "Epoch 92, Loss: 1.0533194299549093, Training Accuracy: 63.21%\n",
      "Epoch 93, Loss: 1.0540856353919525, Training Accuracy: 63.26%\n",
      "Epoch 94, Loss: 1.0532745062695135, Training Accuracy: 63.36%\n",
      "Epoch 95, Loss: 1.0565787758821112, Training Accuracy: 63.20%\n",
      "Epoch 96, Loss: 1.0496384838353032, Training Accuracy: 63.23%\n",
      "Epoch 97, Loss: 1.052399441714177, Training Accuracy: 63.38%\n",
      "Epoch 98, Loss: 1.0532265467107143, Training Accuracy: 63.59%\n",
      "Epoch 99, Loss: 1.0524343610419642, Training Accuracy: 63.21%\n",
      "Epoch 100, Loss: 1.054925509121107, Training Accuracy: 63.39%\n",
      "Epoch 101, Loss: 1.0558101681949537, Training Accuracy: 63.27%\n",
      "Epoch 102, Loss: 1.055055571410357, Training Accuracy: 63.28%\n",
      "Epoch 103, Loss: 1.0523656084561896, Training Accuracy: 63.22%\n",
      "Epoch 104, Loss: 1.05435118917614, Training Accuracy: 63.33%\n",
      "Epoch 105, Loss: 1.0556562973562713, Training Accuracy: 63.28%\n",
      "Epoch 106, Loss: 1.0530500607874693, Training Accuracy: 63.50%\n",
      "Epoch 107, Loss: 1.0525971523026372, Training Accuracy: 63.27%\n",
      "Epoch 108, Loss: 1.0475690029466245, Training Accuracy: 63.55%\n",
      "Epoch 109, Loss: 1.0517784043803544, Training Accuracy: 63.34%\n",
      "Epoch 110, Loss: 1.0490049093275728, Training Accuracy: 63.46%\n",
      "Epoch 111, Loss: 1.0490029082273888, Training Accuracy: 63.18%\n",
      "Epoch 112, Loss: 1.0481826522008841, Training Accuracy: 63.27%\n",
      "Epoch 113, Loss: 1.0524481374132053, Training Accuracy: 63.49%\n",
      "Epoch 114, Loss: 1.0473931932540805, Training Accuracy: 63.39%\n",
      "Epoch 115, Loss: 1.0498873374193831, Training Accuracy: 63.26%\n",
      "Epoch 116, Loss: 1.0504711688022175, Training Accuracy: 63.55%\n",
      "Epoch 117, Loss: 1.0498612872170061, Training Accuracy: 63.41%\n",
      "Epoch 118, Loss: 1.0510902653264877, Training Accuracy: 63.33%\n",
      "Epoch 119, Loss: 1.0485738253654422, Training Accuracy: 63.35%\n",
      "Epoch 120, Loss: 1.0501720394624774, Training Accuracy: 63.28%\n",
      "Epoch 121, Loss: 1.050188032410029, Training Accuracy: 63.37%\n",
      "Epoch 122, Loss: 1.0492221251930423, Training Accuracy: 63.47%\n",
      "Epoch 123, Loss: 1.047771761560684, Training Accuracy: 63.34%\n",
      "Epoch 124, Loss: 1.050055964218686, Training Accuracy: 63.42%\n",
      "Epoch 125, Loss: 1.0484530273300912, Training Accuracy: 63.37%\n",
      "Epoch 126, Loss: 1.0481150101517778, Training Accuracy: 63.31%\n",
      "Epoch 127, Loss: 1.0496352004730487, Training Accuracy: 63.33%\n",
      "Epoch 128, Loss: 1.0483978785517272, Training Accuracy: 63.42%\n",
      "Epoch 129, Loss: 1.0449227646488668, Training Accuracy: 63.40%\n",
      "Epoch 130, Loss: 1.043783421940206, Training Accuracy: 63.52%\n",
      "Epoch 131, Loss: 1.0452157118741203, Training Accuracy: 63.41%\n",
      "Epoch 132, Loss: 1.0465179068962935, Training Accuracy: 63.36%\n",
      "Epoch 133, Loss: 1.0481314775736437, Training Accuracy: 63.23%\n",
      "Epoch 134, Loss: 1.0441807045808533, Training Accuracy: 63.61%\n",
      "Epoch 135, Loss: 1.0432969808121166, Training Accuracy: 63.61%\n",
      "Epoch 136, Loss: 1.0455020678317761, Training Accuracy: 63.38%\n",
      "Epoch 137, Loss: 1.0467331883547557, Training Accuracy: 63.51%\n",
      "Epoch 138, Loss: 1.045536485398212, Training Accuracy: 63.47%\n",
      "Epoch 139, Loss: 1.0430107361367902, Training Accuracy: 63.47%\n",
      "Epoch 140, Loss: 1.0453421261609364, Training Accuracy: 63.62%\n",
      "Epoch 141, Loss: 1.0449684392613219, Training Accuracy: 63.34%\n",
      "Epoch 142, Loss: 1.044227000849936, Training Accuracy: 63.73%\n",
      "Epoch 143, Loss: 1.0446874445966443, Training Accuracy: 63.28%\n",
      "Epoch 144, Loss: 1.0416077768711178, Training Accuracy: 63.60%\n",
      "Epoch 145, Loss: 1.0405147923990283, Training Accuracy: 63.80%\n",
      "Epoch 146, Loss: 1.0409196538236134, Training Accuracy: 63.70%\n",
      "Epoch 147, Loss: 1.0463442156839249, Training Accuracy: 63.51%\n",
      "Epoch 148, Loss: 1.0415745284551245, Training Accuracy: 63.67%\n",
      "Epoch 149, Loss: 1.0402542411366387, Training Accuracy: 63.96%\n",
      "Epoch 150, Loss: 1.0422480687156053, Training Accuracy: 63.26%\n",
      "Epoch 151, Loss: 1.0404323014761785, Training Accuracy: 63.65%\n",
      "Epoch 152, Loss: 1.0382172857861385, Training Accuracy: 63.87%\n",
      "Epoch 153, Loss: 1.0447592425834187, Training Accuracy: 63.68%\n",
      "Epoch 154, Loss: 1.0414546872191417, Training Accuracy: 63.47%\n",
      "Epoch 155, Loss: 1.039639078366482, Training Accuracy: 63.69%\n",
      "Epoch 156, Loss: 1.0393127204512087, Training Accuracy: 63.93%\n",
      "Epoch 157, Loss: 1.0379179062135995, Training Accuracy: 64.04%\n",
      "Epoch 158, Loss: 1.0403749797960071, Training Accuracy: 63.73%\n",
      "Epoch 159, Loss: 1.0433312349612145, Training Accuracy: 63.49%\n",
      "Epoch 160, Loss: 1.0403648005117236, Training Accuracy: 63.74%\n",
      "Epoch 161, Loss: 1.0400401481886958, Training Accuracy: 63.81%\n",
      "Epoch 162, Loss: 1.0379659506061194, Training Accuracy: 63.82%\n",
      "Epoch 163, Loss: 1.0379083357808534, Training Accuracy: 63.90%\n",
      "Epoch 164, Loss: 1.0395014792909403, Training Accuracy: 63.66%\n",
      "Epoch 165, Loss: 1.0385234588399872, Training Accuracy: 63.93%\n",
      "Epoch 166, Loss: 1.0399374563218382, Training Accuracy: 63.70%\n",
      "Epoch 167, Loss: 1.0383428455618642, Training Accuracy: 63.88%\n",
      "Epoch 168, Loss: 1.0402956654501083, Training Accuracy: 63.53%\n",
      "Epoch 169, Loss: 1.0390911330195034, Training Accuracy: 63.78%\n",
      "Epoch 170, Loss: 1.0388091161580342, Training Accuracy: 63.84%\n",
      "Epoch 171, Loss: 1.0381862108817186, Training Accuracy: 63.83%\n",
      "Epoch 172, Loss: 1.0375628889826558, Training Accuracy: 63.89%\n",
      "Epoch 173, Loss: 1.034972264333759, Training Accuracy: 63.72%\n",
      "Epoch 174, Loss: 1.0396456526368476, Training Accuracy: 63.74%\n",
      "Epoch 175, Loss: 1.034646744389668, Training Accuracy: 63.95%\n",
      "Accuracy of the network on the 10000 test images: 64.84%\n",
      "Starting training for hyperparameter configuration 3/3\n",
      "Epoch 1, Loss: 1.5264830869024673, Training Accuracy: 44.48%\n",
      "Epoch 2, Loss: 1.2433681347790886, Training Accuracy: 55.48%\n",
      "Epoch 3, Loss: 1.1442264966342761, Training Accuracy: 59.07%\n",
      "Epoch 4, Loss: 1.08553676859802, Training Accuracy: 61.45%\n",
      "Epoch 5, Loss: 1.0448706252952975, Training Accuracy: 62.98%\n",
      "Epoch 6, Loss: 1.0108817975082056, Training Accuracy: 64.21%\n",
      "Epoch 7, Loss: 0.9774886767577637, Training Accuracy: 65.55%\n",
      "Epoch 8, Loss: 0.9524622967328562, Training Accuracy: 66.23%\n",
      "Epoch 9, Loss: 0.9313303649882831, Training Accuracy: 67.14%\n",
      "Epoch 10, Loss: 0.9103208574492608, Training Accuracy: 68.02%\n",
      "Epoch 11, Loss: 0.8915222053180265, Training Accuracy: 68.55%\n",
      "Epoch 12, Loss: 0.8754205786815995, Training Accuracy: 69.19%\n",
      "Epoch 13, Loss: 0.8585325711218598, Training Accuracy: 69.68%\n",
      "Epoch 14, Loss: 0.8502994378661866, Training Accuracy: 70.09%\n",
      "Epoch 15, Loss: 0.8370256989889437, Training Accuracy: 70.47%\n",
      "Epoch 16, Loss: 0.8233938262514446, Training Accuracy: 71.14%\n",
      "Epoch 17, Loss: 0.8190951356116463, Training Accuracy: 71.20%\n",
      "Epoch 18, Loss: 0.8088283617325756, Training Accuracy: 71.46%\n",
      "Epoch 19, Loss: 0.7952348891731418, Training Accuracy: 71.96%\n",
      "Epoch 20, Loss: 0.793094054550466, Training Accuracy: 72.21%\n",
      "Epoch 21, Loss: 0.7804890691166948, Training Accuracy: 72.56%\n",
      "Epoch 22, Loss: 0.7778245481993536, Training Accuracy: 72.85%\n",
      "Epoch 23, Loss: 0.7716113686790247, Training Accuracy: 72.80%\n",
      "Epoch 24, Loss: 0.7585567081599589, Training Accuracy: 73.44%\n",
      "Epoch 25, Loss: 0.7567295964492862, Training Accuracy: 73.24%\n",
      "Epoch 26, Loss: 0.7487960316412284, Training Accuracy: 73.61%\n",
      "Epoch 27, Loss: 0.7432403414679305, Training Accuracy: 73.92%\n",
      "Epoch 28, Loss: 0.7422379400495374, Training Accuracy: 73.76%\n",
      "Epoch 29, Loss: 0.7360997374176674, Training Accuracy: 74.09%\n",
      "Epoch 30, Loss: 0.7305674522810275, Training Accuracy: 74.09%\n",
      "Epoch 31, Loss: 0.7248697480201112, Training Accuracy: 74.34%\n",
      "Epoch 32, Loss: 0.7254234242164875, Training Accuracy: 74.25%\n",
      "Epoch 33, Loss: 0.7153040910011057, Training Accuracy: 74.91%\n",
      "Epoch 34, Loss: 0.7154307539963052, Training Accuracy: 74.67%\n",
      "Epoch 35, Loss: 0.7087326393941479, Training Accuracy: 74.90%\n",
      "Epoch 36, Loss: 0.7078094479754148, Training Accuracy: 75.04%\n",
      "Epoch 37, Loss: 0.7062311277670019, Training Accuracy: 75.10%\n",
      "Epoch 38, Loss: 0.6997956725413842, Training Accuracy: 75.39%\n",
      "Epoch 39, Loss: 0.69814061402055, Training Accuracy: 75.36%\n",
      "Epoch 40, Loss: 0.6967686664722764, Training Accuracy: 75.45%\n",
      "Epoch 41, Loss: 0.6953599281856776, Training Accuracy: 75.38%\n",
      "Epoch 42, Loss: 0.6897856681167013, Training Accuracy: 75.73%\n",
      "Epoch 43, Loss: 0.6899750856182459, Training Accuracy: 75.39%\n",
      "Epoch 44, Loss: 0.6856034231536529, Training Accuracy: 75.84%\n",
      "Epoch 45, Loss: 0.6863846446547057, Training Accuracy: 75.69%\n",
      "Epoch 46, Loss: 0.6829886959336907, Training Accuracy: 75.78%\n",
      "Epoch 47, Loss: 0.6778079468728332, Training Accuracy: 76.08%\n",
      "Epoch 48, Loss: 0.6784676218505405, Training Accuracy: 76.06%\n",
      "Epoch 49, Loss: 0.674472922025739, Training Accuracy: 76.10%\n",
      "Epoch 50, Loss: 0.6726633783648995, Training Accuracy: 76.23%\n",
      "Epoch 51, Loss: 0.6715302152173294, Training Accuracy: 76.19%\n",
      "Epoch 52, Loss: 0.6687977843729737, Training Accuracy: 76.35%\n",
      "Epoch 53, Loss: 0.6684633565452093, Training Accuracy: 76.44%\n",
      "Epoch 54, Loss: 0.6675581095544884, Training Accuracy: 76.38%\n",
      "Epoch 55, Loss: 0.6639084114747889, Training Accuracy: 76.68%\n",
      "Epoch 56, Loss: 0.6624366874280183, Training Accuracy: 76.51%\n",
      "Epoch 57, Loss: 0.6611835197414584, Training Accuracy: 76.42%\n",
      "Epoch 58, Loss: 0.6621126734539676, Training Accuracy: 76.57%\n",
      "Epoch 59, Loss: 0.6578318100527424, Training Accuracy: 76.72%\n",
      "Epoch 60, Loss: 0.6565088098463805, Training Accuracy: 76.84%\n",
      "Epoch 61, Loss: 0.6538112802273782, Training Accuracy: 76.91%\n",
      "Epoch 62, Loss: 0.6542333691854916, Training Accuracy: 76.74%\n",
      "Epoch 63, Loss: 0.6478356630600932, Training Accuracy: 77.14%\n",
      "Epoch 64, Loss: 0.6491684183059141, Training Accuracy: 77.17%\n",
      "Epoch 65, Loss: 0.651223119834195, Training Accuracy: 76.95%\n",
      "Epoch 66, Loss: 0.6525044453799572, Training Accuracy: 77.02%\n",
      "Epoch 67, Loss: 0.6469011900141416, Training Accuracy: 77.08%\n",
      "Epoch 68, Loss: 0.6421441153034835, Training Accuracy: 77.29%\n",
      "Epoch 69, Loss: 0.641606143902025, Training Accuracy: 77.33%\n",
      "Epoch 70, Loss: 0.6397959282788475, Training Accuracy: 77.24%\n",
      "Epoch 71, Loss: 0.6358093048453026, Training Accuracy: 77.35%\n",
      "Epoch 72, Loss: 0.6330998236947047, Training Accuracy: 77.52%\n",
      "Epoch 73, Loss: 0.6292855605063841, Training Accuracy: 77.69%\n",
      "Epoch 74, Loss: 0.6237336400600956, Training Accuracy: 77.84%\n",
      "Epoch 75, Loss: 0.6204156207154169, Training Accuracy: 78.01%\n",
      "Epoch 76, Loss: 0.6160820726009891, Training Accuracy: 78.08%\n",
      "Epoch 77, Loss: 0.6087246214032478, Training Accuracy: 78.42%\n",
      "Epoch 78, Loss: 0.6070054546951333, Training Accuracy: 78.59%\n",
      "Epoch 79, Loss: 0.6079200942764806, Training Accuracy: 78.38%\n",
      "Epoch 80, Loss: 0.6016834574892088, Training Accuracy: 78.52%\n",
      "Epoch 81, Loss: 0.5962264670816528, Training Accuracy: 78.90%\n",
      "Epoch 82, Loss: 0.5980734400012914, Training Accuracy: 78.88%\n",
      "Epoch 83, Loss: 0.5907420504962086, Training Accuracy: 78.97%\n",
      "Epoch 84, Loss: 0.5920675675315625, Training Accuracy: 78.85%\n",
      "Epoch 85, Loss: 0.585393174301328, Training Accuracy: 79.28%\n",
      "Epoch 86, Loss: 0.5852129610679339, Training Accuracy: 79.28%\n",
      "Epoch 87, Loss: 0.5820802120144105, Training Accuracy: 79.37%\n",
      "Epoch 88, Loss: 0.5824328457074397, Training Accuracy: 79.26%\n",
      "Epoch 89, Loss: 0.5843232200883538, Training Accuracy: 79.03%\n",
      "Epoch 90, Loss: 0.5780311059349638, Training Accuracy: 79.38%\n",
      "Epoch 91, Loss: 0.5745940349824593, Training Accuracy: 79.64%\n",
      "Epoch 92, Loss: 0.5763353258371353, Training Accuracy: 79.40%\n",
      "Epoch 93, Loss: 0.5786131686719177, Training Accuracy: 79.60%\n",
      "Epoch 94, Loss: 0.5732624491919642, Training Accuracy: 79.60%\n",
      "Epoch 95, Loss: 0.5705409295037579, Training Accuracy: 79.72%\n",
      "Epoch 96, Loss: 0.5698575534479088, Training Accuracy: 79.80%\n",
      "Epoch 97, Loss: 0.5699150658324551, Training Accuracy: 79.77%\n",
      "Epoch 98, Loss: 0.5620185699685455, Training Accuracy: 79.97%\n",
      "Epoch 99, Loss: 0.5628182369729747, Training Accuracy: 80.06%\n",
      "Epoch 100, Loss: 0.5607591299221034, Training Accuracy: 80.07%\n",
      "Epoch 101, Loss: 0.5595119358862147, Training Accuracy: 80.03%\n",
      "Epoch 102, Loss: 0.5548574549843893, Training Accuracy: 80.28%\n",
      "Epoch 103, Loss: 0.5565357646711951, Training Accuracy: 80.33%\n",
      "Epoch 104, Loss: 0.558329776424886, Training Accuracy: 80.08%\n",
      "Epoch 105, Loss: 0.555354221931199, Training Accuracy: 80.24%\n",
      "Epoch 106, Loss: 0.5525017099459762, Training Accuracy: 80.41%\n",
      "Epoch 107, Loss: 0.5538238183311794, Training Accuracy: 80.29%\n",
      "Epoch 108, Loss: 0.555658675196683, Training Accuracy: 80.27%\n",
      "Epoch 109, Loss: 0.548763701468325, Training Accuracy: 80.47%\n",
      "Epoch 110, Loss: 0.5476490142933853, Training Accuracy: 80.58%\n",
      "Epoch 111, Loss: 0.5501574055694253, Training Accuracy: 80.45%\n",
      "Epoch 112, Loss: 0.551252998468821, Training Accuracy: 80.26%\n",
      "Epoch 113, Loss: 0.545463355853582, Training Accuracy: 80.57%\n",
      "Epoch 114, Loss: 0.5460091745075972, Training Accuracy: 80.61%\n",
      "Epoch 115, Loss: 0.5439478575878436, Training Accuracy: 80.65%\n",
      "Epoch 116, Loss: 0.5432262243822102, Training Accuracy: 80.63%\n",
      "Epoch 117, Loss: 0.5428456606729256, Training Accuracy: 80.79%\n",
      "Epoch 118, Loss: 0.5401974633488509, Training Accuracy: 80.75%\n",
      "Epoch 119, Loss: 0.5403465903209298, Training Accuracy: 80.86%\n",
      "Epoch 120, Loss: 0.5394318281384685, Training Accuracy: 80.81%\n",
      "Epoch 121, Loss: 0.5368300792963608, Training Accuracy: 80.90%\n",
      "Epoch 122, Loss: 0.5382275816119845, Training Accuracy: 80.71%\n",
      "Epoch 123, Loss: 0.5369770315563892, Training Accuracy: 80.89%\n",
      "Epoch 124, Loss: 0.5353511110557925, Training Accuracy: 80.85%\n",
      "Epoch 125, Loss: 0.5359054512875464, Training Accuracy: 80.97%\n",
      "Epoch 126, Loss: 0.5304012647294023, Training Accuracy: 81.04%\n",
      "Epoch 127, Loss: 0.5294901151067156, Training Accuracy: 81.20%\n",
      "Epoch 128, Loss: 0.5298399396069214, Training Accuracy: 81.14%\n",
      "Epoch 129, Loss: 0.5290992048847706, Training Accuracy: 81.14%\n",
      "Epoch 130, Loss: 0.5291288249632892, Training Accuracy: 81.04%\n",
      "Epoch 131, Loss: 0.5323781941438575, Training Accuracy: 80.93%\n",
      "Epoch 132, Loss: 0.5247488739278615, Training Accuracy: 81.49%\n",
      "Epoch 133, Loss: 0.5253312540481158, Training Accuracy: 81.29%\n",
      "Epoch 134, Loss: 0.5278694706461619, Training Accuracy: 81.24%\n",
      "Epoch 135, Loss: 0.5272808505979645, Training Accuracy: 81.19%\n",
      "Epoch 136, Loss: 0.5267093321094123, Training Accuracy: 81.26%\n",
      "Epoch 137, Loss: 0.522936418500093, Training Accuracy: 81.48%\n",
      "Epoch 138, Loss: 0.524592054945886, Training Accuracy: 81.36%\n",
      "Epoch 139, Loss: 0.5207773848720219, Training Accuracy: 81.49%\n",
      "Epoch 140, Loss: 0.5216448125250809, Training Accuracy: 81.39%\n",
      "Epoch 141, Loss: 0.5207117592815853, Training Accuracy: 81.59%\n",
      "Epoch 142, Loss: 0.5243049588273553, Training Accuracy: 81.36%\n",
      "Epoch 143, Loss: 0.5202693690919815, Training Accuracy: 81.48%\n",
      "Epoch 144, Loss: 0.5162342888543673, Training Accuracy: 81.56%\n",
      "Epoch 145, Loss: 0.5184845227910124, Training Accuracy: 81.59%\n",
      "Epoch 146, Loss: 0.5182959482531109, Training Accuracy: 81.55%\n",
      "Epoch 147, Loss: 0.5189496765623007, Training Accuracy: 81.44%\n",
      "Epoch 148, Loss: 0.5157748192663083, Training Accuracy: 81.67%\n",
      "Epoch 149, Loss: 0.5177327210412306, Training Accuracy: 81.81%\n",
      "Epoch 150, Loss: 0.5116292778640756, Training Accuracy: 81.98%\n",
      "Epoch 151, Loss: 0.5145171249804594, Training Accuracy: 81.67%\n",
      "Epoch 152, Loss: 0.5135245823189426, Training Accuracy: 81.70%\n",
      "Epoch 153, Loss: 0.5106274492829047, Training Accuracy: 81.77%\n",
      "Epoch 154, Loss: 0.5167786093890819, Training Accuracy: 81.59%\n",
      "Epoch 155, Loss: 0.5134553068014972, Training Accuracy: 81.66%\n",
      "Epoch 156, Loss: 0.5137421621957703, Training Accuracy: 81.60%\n",
      "Epoch 157, Loss: 0.513501460976003, Training Accuracy: 81.56%\n",
      "Epoch 158, Loss: 0.5124584125245318, Training Accuracy: 81.83%\n",
      "Epoch 159, Loss: 0.5126474085633103, Training Accuracy: 81.81%\n",
      "Epoch 160, Loss: 0.5064932462352011, Training Accuracy: 81.99%\n",
      "Epoch 161, Loss: 0.5122825362721978, Training Accuracy: 81.74%\n",
      "Epoch 162, Loss: 0.5108333342253705, Training Accuracy: 81.61%\n",
      "Epoch 163, Loss: 0.5069299647417824, Training Accuracy: 81.91%\n",
      "Epoch 164, Loss: 0.5082498828849524, Training Accuracy: 82.01%\n",
      "Epoch 165, Loss: 0.5070293740085934, Training Accuracy: 81.96%\n",
      "Epoch 166, Loss: 0.5071378269463854, Training Accuracy: 82.02%\n",
      "Epoch 167, Loss: 0.5063068245339881, Training Accuracy: 81.89%\n",
      "Epoch 168, Loss: 0.5066333867209342, Training Accuracy: 81.92%\n",
      "Epoch 169, Loss: 0.5047390385509452, Training Accuracy: 82.07%\n",
      "Epoch 170, Loss: 0.5006337404022436, Training Accuracy: 82.15%\n",
      "Epoch 171, Loss: 0.5030887318998957, Training Accuracy: 81.99%\n",
      "Epoch 172, Loss: 0.5028442330372608, Training Accuracy: 82.10%\n",
      "Epoch 173, Loss: 0.501861239733446, Training Accuracy: 82.23%\n",
      "Epoch 174, Loss: 0.5061201296003578, Training Accuracy: 81.93%\n",
      "Epoch 175, Loss: 0.5026478125616107, Training Accuracy: 82.06%\n",
      "Accuracy of the network on the 10000 test images: 67.04%\n",
      "All Training Accuracies over Epochs for each config: [[30.638, 43.35, 47.864, 50.176, 51.682, 52.948, 53.898, 54.182, 55.23, 55.466, 55.984, 56.326, 56.632, 56.816, 57.174, 57.28, 57.274, 57.35, 57.904, 57.71, 58.382, 58.08, 58.466, 58.358, 58.438, 58.724, 58.89, 58.682, 59.198, 59.238, 59.024, 59.3, 59.228, 59.5, 59.628, 59.676, 59.85, 59.658, 59.814, 59.736, 59.918, 60.076, 59.924, 60.096, 60.024, 60.08, 60.144, 59.986, 60.204, 60.228, 60.44, 60.374, 60.348, 60.308, 60.588, 60.46, 60.378, 60.414, 60.314, 60.672, 60.598, 60.71, 60.582, 60.982, 60.7, 60.67, 60.908, 60.522, 60.926, 60.836, 61.042, 61.176, 60.828, 60.802, 60.858, 61.148, 60.798, 60.818, 60.874, 60.946, 61.104, 60.988, 60.998, 61.19, 60.888, 60.878, 61.354, 61.002, 61.24, 61.442, 61.188, 61.238, 61.394, 61.23, 61.286, 61.436, 61.102, 61.42, 61.174, 61.27, 61.314, 61.368, 61.474, 61.45, 61.592, 61.746, 61.616, 61.784, 61.372, 61.346, 61.544, 61.692, 61.716, 61.374, 61.74, 61.482, 61.652, 61.876, 61.486, 61.732, 61.606, 61.716, 61.78, 61.708, 61.854, 61.49, 61.546, 61.526, 61.64, 61.648, 61.638, 61.786, 61.812, 61.886, 61.582, 61.806, 61.838, 61.904, 61.944, 61.944, 61.914, 61.774, 61.766, 61.59, 61.632, 61.99, 61.676, 62.032, 61.958, 61.774, 61.826, 61.964, 61.928, 61.914, 61.952, 61.874, 61.848, 61.538, 62.082, 61.922, 61.966, 61.916, 62.022, 61.912, 61.97, 62.066, 62.112, 62.14, 61.876, 62.172, 61.942, 62.092, 62.184, 62.088, 61.804], [42.77, 52.946, 55.728, 56.854, 57.86, 58.486, 59.026, 59.462, 59.818, 60.044, 60.624, 60.554, 60.994, 61.186, 61.102, 61.694, 61.446, 61.58, 61.822, 62.264, 62.192, 62.38, 62.268, 62.478, 62.67, 62.51, 62.466, 62.552, 62.97, 62.676, 62.852, 63.038, 62.776, 62.948, 62.816, 62.876, 62.69, 62.948, 62.924, 62.728, 62.99, 63.04, 62.832, 63.218, 63.0, 63.07, 63.126, 63.06, 63.15, 62.998, 62.886, 63.072, 63.162, 63.33, 63.294, 62.88, 63.198, 62.84, 63.01, 63.0, 63.084, 63.218, 62.986, 63.026, 63.196, 63.168, 63.34, 63.41, 63.33, 63.006, 63.228, 62.9, 63.104, 63.262, 63.104, 63.332, 63.342, 63.402, 63.364, 63.04, 63.548, 63.156, 63.474, 63.346, 63.06, 63.278, 63.258, 63.298, 63.178, 63.258, 63.466, 63.214, 63.256, 63.356, 63.2, 63.232, 63.376, 63.594, 63.206, 63.388, 63.266, 63.278, 63.222, 63.328, 63.282, 63.504, 63.27, 63.546, 63.34, 63.462, 63.178, 63.268, 63.49, 63.39, 63.256, 63.552, 63.408, 63.332, 63.35, 63.278, 63.366, 63.47, 63.338, 63.416, 63.37, 63.314, 63.332, 63.422, 63.398, 63.516, 63.412, 63.356, 63.226, 63.61, 63.614, 63.384, 63.51, 63.474, 63.47, 63.618, 63.336, 63.732, 63.278, 63.604, 63.804, 63.696, 63.506, 63.672, 63.96, 63.26, 63.648, 63.874, 63.676, 63.47, 63.692, 63.932, 64.044, 63.73, 63.488, 63.738, 63.806, 63.822, 63.9, 63.662, 63.926, 63.702, 63.882, 63.526, 63.784, 63.842, 63.828, 63.886, 63.716, 63.738, 63.948], [44.478, 55.478, 59.072, 61.452, 62.98, 64.208, 65.554, 66.232, 67.142, 68.024, 68.55, 69.188, 69.682, 70.086, 70.472, 71.144, 71.204, 71.464, 71.962, 72.21, 72.562, 72.848, 72.802, 73.436, 73.24, 73.61, 73.918, 73.762, 74.086, 74.09, 74.336, 74.25, 74.908, 74.668, 74.896, 75.038, 75.104, 75.394, 75.358, 75.45, 75.382, 75.734, 75.388, 75.844, 75.688, 75.776, 76.078, 76.062, 76.102, 76.234, 76.19, 76.35, 76.436, 76.38, 76.676, 76.512, 76.424, 76.568, 76.724, 76.836, 76.906, 76.738, 77.138, 77.17, 76.95, 77.02, 77.076, 77.294, 77.33, 77.244, 77.35, 77.52, 77.694, 77.84, 78.006, 78.084, 78.422, 78.59, 78.384, 78.524, 78.9, 78.88, 78.968, 78.854, 79.28, 79.276, 79.368, 79.256, 79.034, 79.378, 79.64, 79.398, 79.604, 79.602, 79.722, 79.8, 79.774, 79.97, 80.056, 80.074, 80.028, 80.282, 80.326, 80.078, 80.242, 80.41, 80.294, 80.27, 80.474, 80.578, 80.448, 80.256, 80.574, 80.612, 80.646, 80.632, 80.792, 80.752, 80.86, 80.808, 80.898, 80.708, 80.894, 80.852, 80.966, 81.04, 81.196, 81.138, 81.136, 81.04, 80.932, 81.492, 81.288, 81.244, 81.192, 81.262, 81.482, 81.362, 81.494, 81.392, 81.586, 81.36, 81.482, 81.562, 81.592, 81.55, 81.444, 81.666, 81.812, 81.976, 81.672, 81.7, 81.766, 81.594, 81.658, 81.6, 81.564, 81.832, 81.81, 81.994, 81.742, 81.612, 81.908, 82.01, 81.962, 82.02, 81.888, 81.916, 82.066, 82.146, 81.988, 82.104, 82.228, 81.926, 82.06]]\n",
      "All Test Accuracies after each config: [[54.58], [64.84], [67.04]]\n",
      "All Losses over Epochs for each config: [[1.8479507211833963, 1.5265347352418144, 1.419721240735115, 1.3564787116806831, 1.3204516827907709, 1.298067217592693, 1.2745878688057366, 1.2567446345410993, 1.2421544243765, 1.2313954300435304, 1.221115350951929, 1.2071787481722625, 1.2027929135600623, 1.1948651246859898, 1.186678238003455, 1.1869474725650095, 1.1786602595273186, 1.175839531299708, 1.1710104604663751, 1.1673450604881472, 1.1583116325118659, 1.1598225934883517, 1.1527916955216158, 1.1470227051726387, 1.150961915016784, 1.145664689790867, 1.1405440420293442, 1.140157161771184, 1.1340052894771557, 1.1372594780019483, 1.1344812657979444, 1.1346591295641097, 1.1322994586604331, 1.1270716751323027, 1.1218384756608997, 1.1216827057054282, 1.121374555179835, 1.12269100165733, 1.1155306484235827, 1.1173060450255108, 1.1158657420779128, 1.1123871775082006, 1.1153409137292896, 1.1102872019838494, 1.1099401338935813, 1.1148794988537079, 1.105471264675755, 1.1069911348697778, 1.1064069258892322, 1.1048422592222844, 1.104379793795783, 1.099921219336712, 1.1036967237282287, 1.101808676024532, 1.1000795822466731, 1.099272246251021, 1.1012950654682296, 1.1013945761848898, 1.0994710264455936, 1.0951515761635187, 1.0947661161270288, 1.0928573098481464, 1.094311254484879, 1.0908639232825745, 1.0941929491737006, 1.0887858318093488, 1.091845234184314, 1.09424511223193, 1.08929171739027, 1.0883624748043392, 1.0843616430564305, 1.0853564005221248, 1.0929694950123272, 1.0855534995150993, 1.0845518146481965, 1.0845175356511265, 1.0879138279753877, 1.087166239233578, 1.0835101666962703, 1.0838990123832928, 1.0798831425054605, 1.084252298030707, 1.0827605367621498, 1.0797453832900739, 1.0837748732103412, 1.0899686383469331, 1.0790151916349027, 1.0807192360653597, 1.079398474196339, 1.0803205059160053, 1.0846017235532746, 1.07813286087702, 1.078188469800193, 1.0814114960715593, 1.0761591676251052, 1.071987851730088, 1.0800060183190934, 1.0755297373170438, 1.0758783579482447, 1.0782419549077369, 1.074809359391327, 1.0751930486668102, 1.0743770700738864, 1.072207456659478, 1.070670651276703, 1.071425804549166, 1.0723123649502044, 1.0702339499197957, 1.0756404713139205, 1.0719755903229384, 1.0704939002576082, 1.0702881786372045, 1.071290040869847, 1.076049292102799, 1.0691027196929277, 1.0700802613249825, 1.069082008145959, 1.0658365475094838, 1.071520121582329, 1.0706925413492696, 1.0719373794772742, 1.0677229514359818, 1.0667245004640515, 1.0682566302359258, 1.0648348014373, 1.0712971771921953, 1.0680476004815163, 1.0679833712175375, 1.0664966508860478, 1.0644584023739065, 1.0670168620088827, 1.0640764660237696, 1.0691208212881746, 1.0633707675330168, 1.0681614208099481, 1.065816977139934, 1.0642477315100258, 1.0599992934547726, 1.0622452168208558, 1.0628068381563172, 1.0587401021929348, 1.0649153091718473, 1.061656613865167, 1.0654991102950346, 1.064739555196689, 1.0615776010486475, 1.0667731216191636, 1.0607436490638176, 1.0635750590992705, 1.0623835467773934, 1.0617221333943974, 1.0620858495497643, 1.0619666060370863, 1.062699728914539, 1.0616067109053091, 1.0626018688349468, 1.0618292490387207, 1.0603561726829889, 1.0583237127574814, 1.058973122283321, 1.0550066228108028, 1.059263868100198, 1.0573374636642767, 1.0584712450004294, 1.0562658998210106, 1.0567552233900865, 1.0554779293134695, 1.0539173247564175, 1.0589022998462247, 1.0566758545463348, 1.0563384227435608, 1.0541836546205194, 1.0554842892510201, 1.054143121785215, 1.0617827864559106], [1.5603151472328265, 1.3083750213046208, 1.24148625257375, 1.2027536389010642, 1.1863758313991224, 1.1664956109907927, 1.1584037338071467, 1.147161890250033, 1.1396099607962782, 1.1346743889629383, 1.1219556605267098, 1.1175013638823235, 1.1113548102738606, 1.1083804270647981, 1.106956692379149, 1.0984847929776478, 1.0950344243775243, 1.0910816992182866, 1.0864033796598234, 1.0814727709421417, 1.0817570152032712, 1.0730524143904372, 1.0785507522428128, 1.0769160831980693, 1.0753997219035694, 1.0740401140411797, 1.072813135050142, 1.0732400368546586, 1.0670025584947727, 1.0672451067153754, 1.065520968126214, 1.0664976467104519, 1.0702259948339, 1.0631760627107547, 1.0673106630592395, 1.0665899335270952, 1.0659646103754068, 1.0624033958100907, 1.0654890991537773, 1.0666847123819239, 1.0629789894041808, 1.0654706544888295, 1.0654344905520339, 1.0574971487759934, 1.0620737062085925, 1.060517362697655, 1.0620052426519906, 1.0622915504381174, 1.0608526302877899, 1.0605135594335053, 1.0610372678703055, 1.0619225843483224, 1.0600309660062766, 1.0602746310136508, 1.0588871761965934, 1.0557450104095136, 1.058730301649674, 1.061842754673775, 1.0594158579626352, 1.060446462820253, 1.0594445615625747, 1.0564300708301233, 1.058578570099438, 1.0567513083100624, 1.056748740279766, 1.0563483389137347, 1.0570560435352423, 1.0531646437047388, 1.0576748613963651, 1.0592176979002745, 1.0571658221047249, 1.0566042644136093, 1.0583737005510598, 1.054837727257053, 1.056259393920679, 1.0568269333418678, 1.0512135223964292, 1.0562892863360207, 1.0558878290836158, 1.0586858740090714, 1.054865491664623, 1.0608287915549315, 1.05288563779248, 1.0566922705953994, 1.0581810084145393, 1.0543813726023945, 1.0537680373015001, 1.0561831078261061, 1.0550709848513689, 1.055812742018029, 1.053371259089931, 1.0533194299549093, 1.0540856353919525, 1.0532745062695135, 1.0565787758821112, 1.0496384838353032, 1.052399441714177, 1.0532265467107143, 1.0524343610419642, 1.054925509121107, 1.0558101681949537, 1.055055571410357, 1.0523656084561896, 1.05435118917614, 1.0556562973562713, 1.0530500607874693, 1.0525971523026372, 1.0475690029466245, 1.0517784043803544, 1.0490049093275728, 1.0490029082273888, 1.0481826522008841, 1.0524481374132053, 1.0473931932540805, 1.0498873374193831, 1.0504711688022175, 1.0498612872170061, 1.0510902653264877, 1.0485738253654422, 1.0501720394624774, 1.050188032410029, 1.0492221251930423, 1.047771761560684, 1.050055964218686, 1.0484530273300912, 1.0481150101517778, 1.0496352004730487, 1.0483978785517272, 1.0449227646488668, 1.043783421940206, 1.0452157118741203, 1.0465179068962935, 1.0481314775736437, 1.0441807045808533, 1.0432969808121166, 1.0455020678317761, 1.0467331883547557, 1.045536485398212, 1.0430107361367902, 1.0453421261609364, 1.0449684392613219, 1.044227000849936, 1.0446874445966443, 1.0416077768711178, 1.0405147923990283, 1.0409196538236134, 1.0463442156839249, 1.0415745284551245, 1.0402542411366387, 1.0422480687156053, 1.0404323014761785, 1.0382172857861385, 1.0447592425834187, 1.0414546872191417, 1.039639078366482, 1.0393127204512087, 1.0379179062135995, 1.0403749797960071, 1.0433312349612145, 1.0403648005117236, 1.0400401481886958, 1.0379659506061194, 1.0379083357808534, 1.0395014792909403, 1.0385234588399872, 1.0399374563218382, 1.0383428455618642, 1.0402956654501083, 1.0390911330195034, 1.0388091161580342, 1.0381862108817186, 1.0375628889826558, 1.034972264333759, 1.0396456526368476, 1.034646744389668], [1.5264830869024673, 1.2433681347790886, 1.1442264966342761, 1.08553676859802, 1.0448706252952975, 1.0108817975082056, 0.9774886767577637, 0.9524622967328562, 0.9313303649882831, 0.9103208574492608, 0.8915222053180265, 0.8754205786815995, 0.8585325711218598, 0.8502994378661866, 0.8370256989889437, 0.8233938262514446, 0.8190951356116463, 0.8088283617325756, 0.7952348891731418, 0.793094054550466, 0.7804890691166948, 0.7778245481993536, 0.7716113686790247, 0.7585567081599589, 0.7567295964492862, 0.7487960316412284, 0.7432403414679305, 0.7422379400495374, 0.7360997374176674, 0.7305674522810275, 0.7248697480201112, 0.7254234242164875, 0.7153040910011057, 0.7154307539963052, 0.7087326393941479, 0.7078094479754148, 0.7062311277670019, 0.6997956725413842, 0.69814061402055, 0.6967686664722764, 0.6953599281856776, 0.6897856681167013, 0.6899750856182459, 0.6856034231536529, 0.6863846446547057, 0.6829886959336907, 0.6778079468728332, 0.6784676218505405, 0.674472922025739, 0.6726633783648995, 0.6715302152173294, 0.6687977843729737, 0.6684633565452093, 0.6675581095544884, 0.6639084114747889, 0.6624366874280183, 0.6611835197414584, 0.6621126734539676, 0.6578318100527424, 0.6565088098463805, 0.6538112802273782, 0.6542333691854916, 0.6478356630600932, 0.6491684183059141, 0.651223119834195, 0.6525044453799572, 0.6469011900141416, 0.6421441153034835, 0.641606143902025, 0.6397959282788475, 0.6358093048453026, 0.6330998236947047, 0.6292855605063841, 0.6237336400600956, 0.6204156207154169, 0.6160820726009891, 0.6087246214032478, 0.6070054546951333, 0.6079200942764806, 0.6016834574892088, 0.5962264670816528, 0.5980734400012914, 0.5907420504962086, 0.5920675675315625, 0.585393174301328, 0.5852129610679339, 0.5820802120144105, 0.5824328457074397, 0.5843232200883538, 0.5780311059349638, 0.5745940349824593, 0.5763353258371353, 0.5786131686719177, 0.5732624491919642, 0.5705409295037579, 0.5698575534479088, 0.5699150658324551, 0.5620185699685455, 0.5628182369729747, 0.5607591299221034, 0.5595119358862147, 0.5548574549843893, 0.5565357646711951, 0.558329776424886, 0.555354221931199, 0.5525017099459762, 0.5538238183311794, 0.555658675196683, 0.548763701468325, 0.5476490142933853, 0.5501574055694253, 0.551252998468821, 0.545463355853582, 0.5460091745075972, 0.5439478575878436, 0.5432262243822102, 0.5428456606729256, 0.5401974633488509, 0.5403465903209298, 0.5394318281384685, 0.5368300792963608, 0.5382275816119845, 0.5369770315563892, 0.5353511110557925, 0.5359054512875464, 0.5304012647294023, 0.5294901151067156, 0.5298399396069214, 0.5290992048847706, 0.5291288249632892, 0.5323781941438575, 0.5247488739278615, 0.5253312540481158, 0.5278694706461619, 0.5272808505979645, 0.5267093321094123, 0.522936418500093, 0.524592054945886, 0.5207773848720219, 0.5216448125250809, 0.5207117592815853, 0.5243049588273553, 0.5202693690919815, 0.5162342888543673, 0.5184845227910124, 0.5182959482531109, 0.5189496765623007, 0.5157748192663083, 0.5177327210412306, 0.5116292778640756, 0.5145171249804594, 0.5135245823189426, 0.5106274492829047, 0.5167786093890819, 0.5134553068014972, 0.5137421621957703, 0.513501460976003, 0.5124584125245318, 0.5126474085633103, 0.5064932462352011, 0.5122825362721978, 0.5108333342253705, 0.5069299647417824, 0.5082498828849524, 0.5070293740085934, 0.5071378269463854, 0.5063068245339881, 0.5066333867209342, 0.5047390385509452, 0.5006337404022436, 0.5030887318998957, 0.5028442330372608, 0.501861239733446, 0.5061201296003578, 0.5026478125616107]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1) \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "def save_checkpoint(config, state, filename='checkpoint.pth.tar'):\n",
    "    os.makedirs(f'checkpoints/config_{config}', exist_ok=True)\n",
    "    filepath = os.path.join(f'checkpoints/config_{config}', filename)\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "def load_checkpoint(config, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(f'checkpoints/config_{config}', filename)\n",
    "    if os.path.exists(filepath):\n",
    "        return torch.load(filepath)\n",
    "    return None\n",
    "\n",
    "num_epochs = 175\n",
    "\n",
    "hyperparams = [\n",
    "    {'lr': 1.0, 'betas': (0.9, 0.999), 'weight_decay': 0, 'd0': 1e-6},\n",
    "    {'lr': 0.5, 'betas': (0.9, 0.999), 'weight_decay': 0.01, 'd0': 1e-5},\n",
    "    {'lr': 1.0, 'betas': (0.85, 0.999), 'weight_decay': 0.001, 'd0': 1e-7}\n",
    "]\n",
    "\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "all_losses = []\n",
    "\n",
    "for config_id, hparams in enumerate(hyperparams):\n",
    "    print(f\"Starting training for hyperparameter configuration {config_id + 1}/{len(hyperparams)}\")\n",
    "\n",
    "    model = SimpleCNN()\n",
    "    optimizer = DAdaptAdam(\n",
    "        model.parameters(),\n",
    "        lr=hparams['lr'],\n",
    "        betas=hparams['betas'],\n",
    "        weight_decay=hparams['weight_decay'],\n",
    "        d0=hparams['d0']\n",
    "    )\n",
    "\n",
    "    start_epoch = 0\n",
    "    checkpoint = load_checkpoint(config_id)\n",
    "    if checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        train_accuracies = checkpoint['train_accuracies']\n",
    "        losses = checkpoint['losses']\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    else:\n",
    "        train_accuracies = []\n",
    "        losses = []\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        losses.append(running_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            save_checkpoint(config_id, {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'losses': losses,\n",
    "            })\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    all_test_accuracies.append([test_accuracy])\n",
    "    print(f'Accuracy of the network on the 10000 test images: {test_accuracy:.2f}%')\n",
    "\n",
    "    all_train_accuracies.append(train_accuracies)\n",
    "    all_losses.append(losses)\n",
    "\n",
    "    save_checkpoint(config_id, {\n",
    "        'epoch': num_epochs - 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'losses': losses,\n",
    "        'test_accuracy': test_accuracy\n",
    "    })\n",
    "\n",
    "all_train_accuracies = [list(train_accuracies) for train_accuracies in all_train_accuracies]\n",
    "all_test_accuracies = [list(test_accuracies) for test_accuracies in all_test_accuracies]\n",
    "all_losses = [list(losses) for losses in all_losses]\n",
    "\n",
    "print(\"All Training Accuracies over Epochs for each config:\", all_train_accuracies)\n",
    "print(\"All Test Accuracies after each config:\", all_test_accuracies)\n",
    "print(\"All Losses over Epochs for each config:\", all_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35090.721561,
   "end_time": "2024-10-23T23:41:50.339874",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T13:56:59.618313",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
