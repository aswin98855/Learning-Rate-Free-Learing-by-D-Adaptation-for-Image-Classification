{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790792d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T12:21:11.011143Z",
     "iopub.status.busy": "2024-10-23T12:21:11.010762Z",
     "iopub.status.idle": "2024-10-23T19:34:59.792077Z",
     "shell.execute_reply": "2024-10-23T19:34:59.790802Z"
    },
    "papermill": {
     "duration": 26028.862778,
     "end_time": "2024-10-23T19:34:59.870152",
     "exception": false,
     "start_time": "2024-10-23T12:21:11.007374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:10<00:00, 15539210.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Starting run 1/5\n",
      "Run 1, Epoch 1, Loss: 1.945, Training Accuracy: 30.50%\n",
      "Run 1, Epoch 2, Loss: 1.771, Training Accuracy: 36.77%\n",
      "Run 1, Epoch 3, Loss: 1.678, Training Accuracy: 39.56%\n",
      "Run 1, Epoch 4, Loss: 1.621, Training Accuracy: 42.00%\n",
      "Run 1, Epoch 5, Loss: 1.578, Training Accuracy: 43.29%\n",
      "Run 1, Epoch 6, Loss: 1.545, Training Accuracy: 44.47%\n",
      "Run 1, Epoch 7, Loss: 1.509, Training Accuracy: 45.77%\n",
      "Run 1, Epoch 8, Loss: 1.487, Training Accuracy: 46.75%\n",
      "Run 1, Epoch 9, Loss: 1.465, Training Accuracy: 47.57%\n",
      "Run 1, Epoch 10, Loss: 1.447, Training Accuracy: 48.37%\n",
      "Run 1, Epoch 11, Loss: 1.427, Training Accuracy: 49.28%\n",
      "Run 1, Epoch 12, Loss: 1.419, Training Accuracy: 49.64%\n",
      "Run 1, Epoch 13, Loss: 1.415, Training Accuracy: 49.76%\n",
      "Run 1, Epoch 14, Loss: 1.416, Training Accuracy: 49.33%\n",
      "Run 1, Epoch 15, Loss: 1.413, Training Accuracy: 49.82%\n",
      "Run 1, Epoch 16, Loss: 1.409, Training Accuracy: 49.72%\n",
      "Run 1, Epoch 17, Loss: 1.407, Training Accuracy: 49.96%\n",
      "Run 1, Epoch 18, Loss: 1.407, Training Accuracy: 49.99%\n",
      "Run 1, Epoch 19, Loss: 1.401, Training Accuracy: 50.17%\n",
      "Run 1, Epoch 20, Loss: 1.402, Training Accuracy: 50.13%\n",
      "Run 1, Epoch 21, Loss: 1.397, Training Accuracy: 50.30%\n",
      "Run 1, Epoch 22, Loss: 1.396, Training Accuracy: 50.38%\n",
      "Run 1, Epoch 23, Loss: 1.398, Training Accuracy: 50.27%\n",
      "Run 1, Epoch 24, Loss: 1.397, Training Accuracy: 50.36%\n",
      "Run 1, Epoch 25, Loss: 1.399, Training Accuracy: 50.33%\n",
      "Run 1, Epoch 26, Loss: 1.398, Training Accuracy: 50.68%\n",
      "Run 1, Epoch 27, Loss: 1.398, Training Accuracy: 50.33%\n",
      "Run 1, Epoch 28, Loss: 1.396, Training Accuracy: 50.48%\n",
      "Run 1, Epoch 29, Loss: 1.397, Training Accuracy: 50.45%\n",
      "Run 1, Epoch 30, Loss: 1.395, Training Accuracy: 50.63%\n",
      "Run 1, Epoch 31, Loss: 1.396, Training Accuracy: 50.32%\n",
      "Run 1, Epoch 32, Loss: 1.399, Training Accuracy: 50.21%\n",
      "Run 1, Epoch 33, Loss: 1.397, Training Accuracy: 50.21%\n",
      "Run 1, Epoch 34, Loss: 1.392, Training Accuracy: 50.48%\n",
      "Run 1, Epoch 35, Loss: 1.392, Training Accuracy: 50.70%\n",
      "Run 1, Epoch 36, Loss: 1.396, Training Accuracy: 50.28%\n",
      "Run 1, Epoch 37, Loss: 1.394, Training Accuracy: 50.45%\n",
      "Run 1, Epoch 38, Loss: 1.396, Training Accuracy: 50.48%\n",
      "Run 1, Epoch 39, Loss: 1.397, Training Accuracy: 50.23%\n",
      "Run 1, Epoch 40, Loss: 1.398, Training Accuracy: 50.37%\n",
      "Run 1, Epoch 41, Loss: 1.394, Training Accuracy: 50.42%\n",
      "Run 1, Epoch 42, Loss: 1.394, Training Accuracy: 50.73%\n",
      "Run 1, Epoch 43, Loss: 1.392, Training Accuracy: 50.43%\n",
      "Run 1, Epoch 44, Loss: 1.398, Training Accuracy: 50.24%\n",
      "Run 1, Epoch 45, Loss: 1.399, Training Accuracy: 50.47%\n",
      "Run 1, Epoch 46, Loss: 1.396, Training Accuracy: 50.33%\n",
      "Run 1, Epoch 47, Loss: 1.398, Training Accuracy: 50.37%\n",
      "Run 1, Epoch 48, Loss: 1.396, Training Accuracy: 50.33%\n",
      "Run 1, Epoch 49, Loss: 1.395, Training Accuracy: 50.35%\n",
      "Run 1, Epoch 50, Loss: 1.396, Training Accuracy: 50.44%\n",
      "Run 1, Epoch 51, Loss: 1.397, Training Accuracy: 50.28%\n",
      "Run 1, Epoch 52, Loss: 1.391, Training Accuracy: 50.65%\n",
      "Run 1, Epoch 53, Loss: 1.397, Training Accuracy: 50.30%\n",
      "Run 1, Epoch 54, Loss: 1.394, Training Accuracy: 50.43%\n",
      "Run 1, Epoch 55, Loss: 1.395, Training Accuracy: 50.54%\n",
      "Run 1, Epoch 56, Loss: 1.394, Training Accuracy: 50.36%\n",
      "Run 1, Epoch 57, Loss: 1.397, Training Accuracy: 50.65%\n",
      "Run 1, Epoch 58, Loss: 1.396, Training Accuracy: 50.47%\n",
      "Run 1, Epoch 59, Loss: 1.393, Training Accuracy: 50.37%\n",
      "Run 1, Epoch 60, Loss: 1.395, Training Accuracy: 50.36%\n",
      "Run 1, Epoch 61, Loss: 1.397, Training Accuracy: 50.21%\n",
      "Run 1, Epoch 62, Loss: 1.395, Training Accuracy: 50.54%\n",
      "Run 1, Epoch 63, Loss: 1.396, Training Accuracy: 50.30%\n",
      "Run 1, Epoch 64, Loss: 1.393, Training Accuracy: 50.61%\n",
      "Run 1, Epoch 65, Loss: 1.394, Training Accuracy: 50.51%\n",
      "Run 1, Epoch 66, Loss: 1.392, Training Accuracy: 50.61%\n",
      "Run 1, Epoch 67, Loss: 1.394, Training Accuracy: 50.43%\n",
      "Run 1, Epoch 68, Loss: 1.395, Training Accuracy: 50.38%\n",
      "Run 1, Epoch 69, Loss: 1.392, Training Accuracy: 50.53%\n",
      "Run 1, Epoch 70, Loss: 1.397, Training Accuracy: 50.42%\n",
      "Run 1, Epoch 71, Loss: 1.396, Training Accuracy: 50.47%\n",
      "Run 1, Epoch 72, Loss: 1.394, Training Accuracy: 50.30%\n",
      "Run 1, Epoch 73, Loss: 1.396, Training Accuracy: 50.46%\n",
      "Run 1, Epoch 74, Loss: 1.395, Training Accuracy: 50.21%\n",
      "Run 1, Epoch 75, Loss: 1.397, Training Accuracy: 50.28%\n",
      "Run 1, Epoch 76, Loss: 1.397, Training Accuracy: 50.07%\n",
      "Run 1, Epoch 77, Loss: 1.395, Training Accuracy: 50.36%\n",
      "Run 1, Epoch 78, Loss: 1.395, Training Accuracy: 50.44%\n",
      "Run 1, Epoch 79, Loss: 1.392, Training Accuracy: 50.61%\n",
      "Run 1, Epoch 80, Loss: 1.392, Training Accuracy: 50.82%\n",
      "Run 1, Epoch 81, Loss: 1.396, Training Accuracy: 50.46%\n",
      "Run 1, Epoch 82, Loss: 1.394, Training Accuracy: 50.73%\n",
      "Run 1, Epoch 83, Loss: 1.396, Training Accuracy: 50.24%\n",
      "Run 1, Epoch 84, Loss: 1.394, Training Accuracy: 50.39%\n",
      "Run 1, Epoch 85, Loss: 1.397, Training Accuracy: 50.51%\n",
      "Run 1, Epoch 86, Loss: 1.395, Training Accuracy: 50.28%\n",
      "Run 1, Epoch 87, Loss: 1.396, Training Accuracy: 50.38%\n",
      "Run 1, Epoch 88, Loss: 1.397, Training Accuracy: 50.49%\n",
      "Run 1, Epoch 89, Loss: 1.396, Training Accuracy: 50.31%\n",
      "Run 1, Epoch 90, Loss: 1.391, Training Accuracy: 50.71%\n",
      "Run 1, Epoch 91, Loss: 1.395, Training Accuracy: 50.47%\n",
      "Run 1, Epoch 92, Loss: 1.397, Training Accuracy: 50.58%\n",
      "Run 1, Epoch 93, Loss: 1.399, Training Accuracy: 50.30%\n",
      "Run 1, Epoch 94, Loss: 1.396, Training Accuracy: 50.34%\n",
      "Run 1, Epoch 95, Loss: 1.396, Training Accuracy: 50.58%\n",
      "Run 1, Epoch 96, Loss: 1.396, Training Accuracy: 50.33%\n",
      "Run 1, Epoch 97, Loss: 1.397, Training Accuracy: 50.48%\n",
      "Run 1, Epoch 98, Loss: 1.395, Training Accuracy: 50.29%\n",
      "Run 1, Epoch 99, Loss: 1.396, Training Accuracy: 50.52%\n",
      "Run 1, Epoch 100, Loss: 1.398, Training Accuracy: 50.33%\n",
      "Run 1, Epoch 101, Loss: 1.398, Training Accuracy: 50.43%\n",
      "Run 1, Epoch 102, Loss: 1.397, Training Accuracy: 50.51%\n",
      "Run 1, Epoch 103, Loss: 1.397, Training Accuracy: 50.42%\n",
      "Run 1, Epoch 104, Loss: 1.394, Training Accuracy: 50.58%\n",
      "Run 1, Epoch 105, Loss: 1.395, Training Accuracy: 50.33%\n",
      "Run 1, Epoch 106, Loss: 1.397, Training Accuracy: 50.38%\n",
      "Run 1, Epoch 107, Loss: 1.398, Training Accuracy: 50.27%\n",
      "Run 1, Epoch 108, Loss: 1.397, Training Accuracy: 50.32%\n",
      "Run 1, Epoch 109, Loss: 1.393, Training Accuracy: 50.60%\n",
      "Run 1, Epoch 110, Loss: 1.394, Training Accuracy: 50.39%\n",
      "Run 1, Epoch 111, Loss: 1.395, Training Accuracy: 50.55%\n",
      "Run 1, Epoch 112, Loss: 1.397, Training Accuracy: 50.44%\n",
      "Run 1, Epoch 113, Loss: 1.395, Training Accuracy: 50.52%\n",
      "Run 1, Epoch 114, Loss: 1.396, Training Accuracy: 50.64%\n",
      "Run 1, Epoch 115, Loss: 1.396, Training Accuracy: 50.19%\n",
      "Run 1, Epoch 116, Loss: 1.399, Training Accuracy: 50.10%\n",
      "Run 1, Epoch 117, Loss: 1.394, Training Accuracy: 50.39%\n",
      "Run 1, Epoch 118, Loss: 1.396, Training Accuracy: 50.43%\n",
      "Run 1, Epoch 119, Loss: 1.397, Training Accuracy: 50.40%\n",
      "Run 1, Epoch 120, Loss: 1.396, Training Accuracy: 50.16%\n",
      "Run 1, Epoch 121, Loss: 1.395, Training Accuracy: 50.50%\n",
      "Run 1, Epoch 122, Loss: 1.395, Training Accuracy: 50.41%\n",
      "Run 1, Epoch 123, Loss: 1.394, Training Accuracy: 50.54%\n",
      "Run 1, Epoch 124, Loss: 1.395, Training Accuracy: 50.61%\n",
      "Run 1, Epoch 125, Loss: 1.398, Training Accuracy: 50.28%\n",
      "Run 1, Epoch 126, Loss: 1.397, Training Accuracy: 50.33%\n",
      "Run 1, Epoch 127, Loss: 1.397, Training Accuracy: 50.59%\n",
      "Run 1, Epoch 128, Loss: 1.399, Training Accuracy: 50.57%\n",
      "Run 1, Epoch 129, Loss: 1.395, Training Accuracy: 50.24%\n",
      "Run 1, Epoch 130, Loss: 1.394, Training Accuracy: 50.32%\n",
      "Run 1, Epoch 131, Loss: 1.396, Training Accuracy: 50.53%\n",
      "Run 1, Epoch 132, Loss: 1.399, Training Accuracy: 50.41%\n",
      "Run 1, Epoch 133, Loss: 1.396, Training Accuracy: 50.30%\n",
      "Run 1, Epoch 134, Loss: 1.393, Training Accuracy: 50.56%\n",
      "Run 1, Epoch 135, Loss: 1.396, Training Accuracy: 50.45%\n",
      "Run 1, Epoch 136, Loss: 1.395, Training Accuracy: 50.57%\n",
      "Run 1, Epoch 137, Loss: 1.395, Training Accuracy: 50.46%\n",
      "Run 1, Epoch 138, Loss: 1.395, Training Accuracy: 50.68%\n",
      "Run 1, Epoch 139, Loss: 1.397, Training Accuracy: 50.30%\n",
      "Run 1, Epoch 140, Loss: 1.397, Training Accuracy: 50.61%\n",
      "Run 1, Epoch 141, Loss: 1.394, Training Accuracy: 50.27%\n",
      "Run 1, Epoch 142, Loss: 1.396, Training Accuracy: 50.29%\n",
      "Run 1, Epoch 143, Loss: 1.399, Training Accuracy: 50.27%\n",
      "Run 1, Epoch 144, Loss: 1.397, Training Accuracy: 50.58%\n",
      "Run 1, Epoch 145, Loss: 1.396, Training Accuracy: 50.64%\n",
      "Run 1, Epoch 146, Loss: 1.394, Training Accuracy: 50.52%\n",
      "Run 1, Epoch 147, Loss: 1.394, Training Accuracy: 50.47%\n",
      "Run 1, Epoch 148, Loss: 1.395, Training Accuracy: 50.54%\n",
      "Run 1, Epoch 149, Loss: 1.396, Training Accuracy: 50.44%\n",
      "Run 1, Epoch 150, Loss: 1.395, Training Accuracy: 50.39%\n",
      "Run 1, Epoch 151, Loss: 1.397, Training Accuracy: 50.16%\n",
      "Run 1, Epoch 152, Loss: 1.394, Training Accuracy: 50.58%\n",
      "Run 1, Epoch 153, Loss: 1.396, Training Accuracy: 50.53%\n",
      "Run 1, Epoch 154, Loss: 1.396, Training Accuracy: 50.37%\n",
      "Run 1, Epoch 155, Loss: 1.394, Training Accuracy: 50.29%\n",
      "Run 1, Epoch 156, Loss: 1.397, Training Accuracy: 50.38%\n",
      "Run 1, Epoch 157, Loss: 1.397, Training Accuracy: 50.46%\n",
      "Run 1, Epoch 158, Loss: 1.395, Training Accuracy: 50.51%\n",
      "Run 1, Epoch 159, Loss: 1.395, Training Accuracy: 50.20%\n",
      "Run 1, Epoch 160, Loss: 1.396, Training Accuracy: 50.34%\n",
      "Run 1, Epoch 161, Loss: 1.399, Training Accuracy: 50.09%\n",
      "Run 1, Epoch 162, Loss: 1.396, Training Accuracy: 50.35%\n",
      "Run 1, Epoch 163, Loss: 1.398, Training Accuracy: 50.29%\n",
      "Run 1, Epoch 164, Loss: 1.396, Training Accuracy: 50.44%\n",
      "Run 1, Epoch 165, Loss: 1.394, Training Accuracy: 50.61%\n",
      "Run 1, Epoch 166, Loss: 1.397, Training Accuracy: 50.20%\n",
      "Run 1, Epoch 167, Loss: 1.397, Training Accuracy: 50.40%\n",
      "Run 1, Epoch 168, Loss: 1.395, Training Accuracy: 50.49%\n",
      "Run 1, Epoch 169, Loss: 1.391, Training Accuracy: 50.58%\n",
      "Run 1, Epoch 170, Loss: 1.397, Training Accuracy: 50.60%\n",
      "Run 1, Epoch 171, Loss: 1.395, Training Accuracy: 50.49%\n",
      "Run 1, Epoch 172, Loss: 1.402, Training Accuracy: 50.32%\n",
      "Run 1, Epoch 173, Loss: 1.392, Training Accuracy: 50.65%\n",
      "Run 1, Epoch 174, Loss: 1.395, Training Accuracy: 50.51%\n",
      "Run 1, Epoch 175, Loss: 1.397, Training Accuracy: 50.37%\n",
      "Run 1, Final Accuracy on test set: 52.92%\n",
      "Results after run 1:\n",
      "Training Accuracies: [30.502, 36.77, 39.562, 42.004, 43.294, 44.472, 45.768, 46.754, 47.574, 48.374, 49.284, 49.642, 49.756, 49.328, 49.82, 49.724, 49.958, 49.988, 50.168, 50.134, 50.304, 50.378, 50.274, 50.358, 50.328, 50.68, 50.33, 50.48, 50.452, 50.628, 50.322, 50.206, 50.214, 50.484, 50.698, 50.282, 50.452, 50.476, 50.234, 50.372, 50.416, 50.732, 50.434, 50.238, 50.468, 50.334, 50.368, 50.334, 50.348, 50.436, 50.284, 50.648, 50.304, 50.43, 50.544, 50.358, 50.652, 50.47, 50.372, 50.362, 50.208, 50.54, 50.302, 50.608, 50.51, 50.606, 50.434, 50.378, 50.534, 50.422, 50.468, 50.296, 50.456, 50.212, 50.278, 50.072, 50.364, 50.444, 50.614, 50.824, 50.458, 50.726, 50.24, 50.386, 50.508, 50.276, 50.384, 50.488, 50.314, 50.706, 50.47, 50.578, 50.3, 50.338, 50.584, 50.326, 50.482, 50.288, 50.52, 50.334, 50.43, 50.51, 50.418, 50.58, 50.332, 50.38, 50.268, 50.322, 50.596, 50.388, 50.554, 50.44, 50.522, 50.638, 50.188, 50.1, 50.39, 50.43, 50.404, 50.158, 50.502, 50.414, 50.542, 50.606, 50.276, 50.33, 50.592, 50.568, 50.244, 50.318, 50.532, 50.412, 50.3, 50.562, 50.452, 50.568, 50.46, 50.68, 50.304, 50.614, 50.274, 50.288, 50.266, 50.584, 50.638, 50.524, 50.474, 50.538, 50.436, 50.39, 50.162, 50.58, 50.534, 50.374, 50.29, 50.378, 50.46, 50.51, 50.196, 50.336, 50.086, 50.346, 50.29, 50.436, 50.606, 50.198, 50.396, 50.492, 50.576, 50.6, 50.494, 50.322, 50.652, 50.514, 50.372]\n",
      "Test Accuracy: 52.92%\n",
      "Losses: [1.9448428782050873, 1.7714638386845893, 1.6783532901188296, 1.620913436040854, 1.578133629715961, 1.5450509844533622, 1.509306572892172, 1.487175637193958, 1.465194684160335, 1.4473277616988667, 1.4266525903321288, 1.4191582968167942, 1.4149222590429398, 1.4162307047782956, 1.4130828334852252, 1.4090262925838266, 1.4072048307379799, 1.4069810401448204, 1.4008115420256124, 1.4017976266343881, 1.396685718575402, 1.3957604316189467, 1.3976324955215844, 1.3968936884799577, 1.398671301429534, 1.3978467194930366, 1.3982054897586402, 1.3955736870655928, 1.3965677538186387, 1.3949345017942931, 1.396230990929372, 1.3987684469393757, 1.3972520623975397, 1.3924394675227991, 1.3921691322570566, 1.3958863739467338, 1.3941486287299933, 1.395580106074243, 1.3967175886149297, 1.3978866357022843, 1.3938774574748085, 1.3943495793110878, 1.3922581514129249, 1.3982684953743234, 1.3994384690013992, 1.3961558963941492, 1.3975606845772786, 1.3962089344668571, 1.3946228667598246, 1.3955958325539708, 1.3974019825610968, 1.3905645983908184, 1.39660487333527, 1.3944968216864349, 1.3949960311660377, 1.3941475210897147, 1.396625589531706, 1.395671903629742, 1.3928608040675483, 1.3952944379328462, 1.3966357796393392, 1.3952087901742256, 1.3955691223559172, 1.3928727243867371, 1.3944608921285175, 1.3924820920085663, 1.394234459113587, 1.3950818953916544, 1.392057262113332, 1.3971885281145726, 1.396286550385263, 1.3943231368003903, 1.396489603135287, 1.3952439499023321, 1.3965036972709324, 1.3974289549586107, 1.3946854479782416, 1.3953194545053156, 1.392314070325983, 1.3916434699007312, 1.3958906520663015, 1.3936500747490417, 1.3958096421892991, 1.393834596399761, 1.397265869333311, 1.3949570302158365, 1.395582571968703, 1.3970907027154322, 1.3956465632714274, 1.3907990565385355, 1.395446672768849, 1.3973820020475656, 1.3991577125266386, 1.3958763518296848, 1.396429960075242, 1.396318161274161, 1.3973949053098478, 1.3952939266438984, 1.3955966932389436, 1.3981652793372075, 1.397805277343906, 1.3966743766194414, 1.396660123029938, 1.3937594722908782, 1.3945468862343322, 1.3971431148631492, 1.3975202936650541, 1.3970971290412766, 1.3932780280442494, 1.3942748533795253, 1.394514473198015, 1.3973818102760998, 1.3948997676829853, 1.395911961260354, 1.395887202618982, 1.3990265585272514, 1.3938082584639644, 1.39588617546784, 1.3971746925197903, 1.395864125102987, 1.394532492703489, 1.3945748406602902, 1.3937737871618832, 1.3952131021358167, 1.3975748808487602, 1.3969627664522137, 1.3965559524038564, 1.3985960608553094, 1.3950709958210625, 1.3941497836271515, 1.396006299101788, 1.3987857185666213, 1.3955449728709657, 1.3928424156535313, 1.3956048363614875, 1.3945713515781686, 1.395275572986554, 1.3952727006829304, 1.3966600824804867, 1.3973043669215248, 1.394277302200532, 1.3962837924127993, 1.3986318989482986, 1.397004439092963, 1.396416595829722, 1.3936922483127137, 1.3941572307015928, 1.3948288515705587, 1.3964629289134385, 1.3954534545883803, 1.3969390188031794, 1.3944023057932744, 1.3961482499261646, 1.3959837432407662, 1.3939098275226096, 1.3973808340404346, 1.396705375303088, 1.3954854380444188, 1.3947353338646462, 1.396173772299686, 1.3992537583231621, 1.3963261912850773, 1.3979049725910586, 1.396018558450977, 1.393704718641003, 1.397031555395297, 1.3972205094364294, 1.3947003710910182, 1.3909344197539113, 1.396548590391798, 1.3948307915416824, 1.401747357814818, 1.3920824640547222, 1.3951322663470607, 1.397251789527171]\n",
      "Starting run 2/5\n",
      "Run 2, Epoch 1, Loss: 1.938, Training Accuracy: 30.31%\n",
      "Run 2, Epoch 2, Loss: 1.769, Training Accuracy: 36.78%\n",
      "Run 2, Epoch 3, Loss: 1.697, Training Accuracy: 39.37%\n",
      "Run 2, Epoch 4, Loss: 1.651, Training Accuracy: 40.83%\n",
      "Run 2, Epoch 5, Loss: 1.607, Training Accuracy: 42.50%\n",
      "Run 2, Epoch 6, Loss: 1.574, Training Accuracy: 43.48%\n",
      "Run 2, Epoch 7, Loss: 1.541, Training Accuracy: 44.85%\n",
      "Run 2, Epoch 8, Loss: 1.514, Training Accuracy: 45.68%\n",
      "Run 2, Epoch 9, Loss: 1.484, Training Accuracy: 46.91%\n",
      "Run 2, Epoch 10, Loss: 1.465, Training Accuracy: 47.49%\n",
      "Run 2, Epoch 11, Loss: 1.442, Training Accuracy: 48.65%\n",
      "Run 2, Epoch 12, Loss: 1.436, Training Accuracy: 49.00%\n",
      "Run 2, Epoch 13, Loss: 1.431, Training Accuracy: 49.23%\n",
      "Run 2, Epoch 14, Loss: 1.430, Training Accuracy: 49.25%\n",
      "Run 2, Epoch 15, Loss: 1.427, Training Accuracy: 49.17%\n",
      "Run 2, Epoch 16, Loss: 1.425, Training Accuracy: 49.28%\n",
      "Run 2, Epoch 17, Loss: 1.424, Training Accuracy: 49.32%\n",
      "Run 2, Epoch 18, Loss: 1.421, Training Accuracy: 49.50%\n",
      "Run 2, Epoch 19, Loss: 1.417, Training Accuracy: 49.90%\n",
      "Run 2, Epoch 20, Loss: 1.418, Training Accuracy: 49.73%\n",
      "Run 2, Epoch 21, Loss: 1.416, Training Accuracy: 49.84%\n",
      "Run 2, Epoch 22, Loss: 1.412, Training Accuracy: 49.85%\n",
      "Run 2, Epoch 23, Loss: 1.416, Training Accuracy: 49.77%\n",
      "Run 2, Epoch 24, Loss: 1.417, Training Accuracy: 49.44%\n",
      "Run 2, Epoch 25, Loss: 1.415, Training Accuracy: 49.69%\n",
      "Run 2, Epoch 26, Loss: 1.412, Training Accuracy: 49.98%\n",
      "Run 2, Epoch 27, Loss: 1.413, Training Accuracy: 49.83%\n",
      "Run 2, Epoch 28, Loss: 1.411, Training Accuracy: 49.58%\n",
      "Run 2, Epoch 29, Loss: 1.413, Training Accuracy: 49.96%\n",
      "Run 2, Epoch 30, Loss: 1.411, Training Accuracy: 49.91%\n",
      "Run 2, Epoch 31, Loss: 1.411, Training Accuracy: 49.87%\n",
      "Run 2, Epoch 32, Loss: 1.409, Training Accuracy: 49.95%\n",
      "Run 2, Epoch 33, Loss: 1.410, Training Accuracy: 49.75%\n",
      "Run 2, Epoch 34, Loss: 1.411, Training Accuracy: 49.78%\n",
      "Run 2, Epoch 35, Loss: 1.413, Training Accuracy: 49.94%\n",
      "Run 2, Epoch 36, Loss: 1.411, Training Accuracy: 49.82%\n",
      "Run 2, Epoch 37, Loss: 1.409, Training Accuracy: 49.91%\n",
      "Run 2, Epoch 38, Loss: 1.412, Training Accuracy: 49.74%\n",
      "Run 2, Epoch 39, Loss: 1.412, Training Accuracy: 50.01%\n",
      "Run 2, Epoch 40, Loss: 1.409, Training Accuracy: 49.69%\n",
      "Run 2, Epoch 41, Loss: 1.412, Training Accuracy: 49.76%\n",
      "Run 2, Epoch 42, Loss: 1.413, Training Accuracy: 49.72%\n",
      "Run 2, Epoch 43, Loss: 1.410, Training Accuracy: 49.96%\n",
      "Run 2, Epoch 44, Loss: 1.411, Training Accuracy: 49.78%\n",
      "Run 2, Epoch 45, Loss: 1.408, Training Accuracy: 50.10%\n",
      "Run 2, Epoch 46, Loss: 1.411, Training Accuracy: 50.06%\n",
      "Run 2, Epoch 47, Loss: 1.413, Training Accuracy: 49.56%\n",
      "Run 2, Epoch 48, Loss: 1.412, Training Accuracy: 49.91%\n",
      "Run 2, Epoch 49, Loss: 1.409, Training Accuracy: 49.92%\n",
      "Run 2, Epoch 50, Loss: 1.413, Training Accuracy: 49.82%\n",
      "Run 2, Epoch 51, Loss: 1.410, Training Accuracy: 49.88%\n",
      "Run 2, Epoch 52, Loss: 1.411, Training Accuracy: 49.66%\n",
      "Run 2, Epoch 53, Loss: 1.407, Training Accuracy: 49.84%\n",
      "Run 2, Epoch 54, Loss: 1.407, Training Accuracy: 50.03%\n",
      "Run 2, Epoch 55, Loss: 1.412, Training Accuracy: 49.72%\n",
      "Run 2, Epoch 56, Loss: 1.413, Training Accuracy: 49.80%\n",
      "Run 2, Epoch 57, Loss: 1.411, Training Accuracy: 49.95%\n",
      "Run 2, Epoch 58, Loss: 1.410, Training Accuracy: 49.91%\n",
      "Run 2, Epoch 59, Loss: 1.410, Training Accuracy: 49.77%\n",
      "Run 2, Epoch 60, Loss: 1.410, Training Accuracy: 49.96%\n",
      "Run 2, Epoch 61, Loss: 1.409, Training Accuracy: 50.14%\n",
      "Run 2, Epoch 62, Loss: 1.409, Training Accuracy: 49.97%\n",
      "Run 2, Epoch 63, Loss: 1.409, Training Accuracy: 50.14%\n",
      "Run 2, Epoch 64, Loss: 1.414, Training Accuracy: 49.76%\n",
      "Run 2, Epoch 65, Loss: 1.408, Training Accuracy: 49.99%\n",
      "Run 2, Epoch 66, Loss: 1.412, Training Accuracy: 49.86%\n",
      "Run 2, Epoch 67, Loss: 1.409, Training Accuracy: 49.87%\n",
      "Run 2, Epoch 68, Loss: 1.410, Training Accuracy: 49.82%\n",
      "Run 2, Epoch 69, Loss: 1.409, Training Accuracy: 49.84%\n",
      "Run 2, Epoch 70, Loss: 1.411, Training Accuracy: 49.97%\n",
      "Run 2, Epoch 71, Loss: 1.414, Training Accuracy: 49.94%\n",
      "Run 2, Epoch 72, Loss: 1.409, Training Accuracy: 50.05%\n",
      "Run 2, Epoch 73, Loss: 1.412, Training Accuracy: 49.78%\n",
      "Run 2, Epoch 74, Loss: 1.408, Training Accuracy: 50.05%\n",
      "Run 2, Epoch 75, Loss: 1.414, Training Accuracy: 49.85%\n",
      "Run 2, Epoch 76, Loss: 1.409, Training Accuracy: 49.98%\n",
      "Run 2, Epoch 77, Loss: 1.411, Training Accuracy: 49.86%\n",
      "Run 2, Epoch 78, Loss: 1.409, Training Accuracy: 49.88%\n",
      "Run 2, Epoch 79, Loss: 1.410, Training Accuracy: 50.01%\n",
      "Run 2, Epoch 80, Loss: 1.410, Training Accuracy: 50.15%\n",
      "Run 2, Epoch 81, Loss: 1.413, Training Accuracy: 49.78%\n",
      "Run 2, Epoch 82, Loss: 1.411, Training Accuracy: 49.90%\n",
      "Run 2, Epoch 83, Loss: 1.411, Training Accuracy: 49.74%\n",
      "Run 2, Epoch 84, Loss: 1.414, Training Accuracy: 49.67%\n",
      "Run 2, Epoch 85, Loss: 1.412, Training Accuracy: 49.85%\n",
      "Run 2, Epoch 86, Loss: 1.411, Training Accuracy: 49.85%\n",
      "Run 2, Epoch 87, Loss: 1.414, Training Accuracy: 49.79%\n",
      "Run 2, Epoch 88, Loss: 1.410, Training Accuracy: 50.15%\n",
      "Run 2, Epoch 89, Loss: 1.410, Training Accuracy: 49.83%\n",
      "Run 2, Epoch 90, Loss: 1.410, Training Accuracy: 49.93%\n",
      "Run 2, Epoch 91, Loss: 1.413, Training Accuracy: 50.01%\n",
      "Run 2, Epoch 92, Loss: 1.408, Training Accuracy: 50.09%\n",
      "Run 2, Epoch 93, Loss: 1.412, Training Accuracy: 50.00%\n",
      "Run 2, Epoch 94, Loss: 1.411, Training Accuracy: 49.79%\n",
      "Run 2, Epoch 95, Loss: 1.413, Training Accuracy: 49.93%\n",
      "Run 2, Epoch 96, Loss: 1.413, Training Accuracy: 49.85%\n",
      "Run 2, Epoch 97, Loss: 1.411, Training Accuracy: 49.87%\n",
      "Run 2, Epoch 98, Loss: 1.410, Training Accuracy: 49.41%\n",
      "Run 2, Epoch 99, Loss: 1.408, Training Accuracy: 50.19%\n",
      "Run 2, Epoch 100, Loss: 1.410, Training Accuracy: 49.64%\n",
      "Run 2, Epoch 101, Loss: 1.409, Training Accuracy: 50.05%\n",
      "Run 2, Epoch 102, Loss: 1.417, Training Accuracy: 49.73%\n",
      "Run 2, Epoch 103, Loss: 1.414, Training Accuracy: 49.83%\n",
      "Run 2, Epoch 104, Loss: 1.413, Training Accuracy: 49.68%\n",
      "Run 2, Epoch 105, Loss: 1.409, Training Accuracy: 50.02%\n",
      "Run 2, Epoch 106, Loss: 1.413, Training Accuracy: 49.74%\n",
      "Run 2, Epoch 107, Loss: 1.409, Training Accuracy: 49.86%\n",
      "Run 2, Epoch 108, Loss: 1.412, Training Accuracy: 49.76%\n",
      "Run 2, Epoch 109, Loss: 1.412, Training Accuracy: 49.93%\n",
      "Run 2, Epoch 110, Loss: 1.409, Training Accuracy: 49.99%\n",
      "Run 2, Epoch 111, Loss: 1.409, Training Accuracy: 49.72%\n",
      "Run 2, Epoch 112, Loss: 1.411, Training Accuracy: 49.80%\n",
      "Run 2, Epoch 113, Loss: 1.412, Training Accuracy: 49.95%\n",
      "Run 2, Epoch 114, Loss: 1.410, Training Accuracy: 49.96%\n",
      "Run 2, Epoch 115, Loss: 1.411, Training Accuracy: 49.99%\n",
      "Run 2, Epoch 116, Loss: 1.410, Training Accuracy: 50.11%\n",
      "Run 2, Epoch 117, Loss: 1.409, Training Accuracy: 49.85%\n",
      "Run 2, Epoch 118, Loss: 1.412, Training Accuracy: 50.00%\n",
      "Run 2, Epoch 119, Loss: 1.411, Training Accuracy: 49.93%\n",
      "Run 2, Epoch 120, Loss: 1.410, Training Accuracy: 50.05%\n",
      "Run 2, Epoch 121, Loss: 1.414, Training Accuracy: 49.81%\n",
      "Run 2, Epoch 122, Loss: 1.411, Training Accuracy: 49.75%\n",
      "Run 2, Epoch 123, Loss: 1.408, Training Accuracy: 49.74%\n",
      "Run 2, Epoch 124, Loss: 1.410, Training Accuracy: 49.99%\n",
      "Run 2, Epoch 125, Loss: 1.411, Training Accuracy: 50.04%\n",
      "Run 2, Epoch 126, Loss: 1.412, Training Accuracy: 49.80%\n",
      "Run 2, Epoch 127, Loss: 1.411, Training Accuracy: 49.97%\n",
      "Run 2, Epoch 128, Loss: 1.416, Training Accuracy: 49.66%\n",
      "Run 2, Epoch 129, Loss: 1.409, Training Accuracy: 50.00%\n",
      "Run 2, Epoch 130, Loss: 1.410, Training Accuracy: 50.16%\n",
      "Run 2, Epoch 131, Loss: 1.407, Training Accuracy: 49.88%\n",
      "Run 2, Epoch 132, Loss: 1.407, Training Accuracy: 49.99%\n",
      "Run 2, Epoch 133, Loss: 1.411, Training Accuracy: 49.79%\n",
      "Run 2, Epoch 134, Loss: 1.415, Training Accuracy: 49.81%\n",
      "Run 2, Epoch 135, Loss: 1.412, Training Accuracy: 49.88%\n",
      "Run 2, Epoch 136, Loss: 1.411, Training Accuracy: 49.96%\n",
      "Run 2, Epoch 137, Loss: 1.413, Training Accuracy: 49.72%\n",
      "Run 2, Epoch 138, Loss: 1.408, Training Accuracy: 49.94%\n",
      "Run 2, Epoch 139, Loss: 1.409, Training Accuracy: 50.03%\n",
      "Run 2, Epoch 140, Loss: 1.411, Training Accuracy: 49.99%\n",
      "Run 2, Epoch 141, Loss: 1.409, Training Accuracy: 50.19%\n",
      "Run 2, Epoch 142, Loss: 1.413, Training Accuracy: 49.72%\n",
      "Run 2, Epoch 143, Loss: 1.411, Training Accuracy: 50.10%\n",
      "Run 2, Epoch 144, Loss: 1.409, Training Accuracy: 50.05%\n",
      "Run 2, Epoch 145, Loss: 1.412, Training Accuracy: 49.80%\n",
      "Run 2, Epoch 146, Loss: 1.409, Training Accuracy: 49.93%\n",
      "Run 2, Epoch 147, Loss: 1.410, Training Accuracy: 49.88%\n",
      "Run 2, Epoch 148, Loss: 1.412, Training Accuracy: 49.62%\n",
      "Run 2, Epoch 149, Loss: 1.408, Training Accuracy: 50.05%\n",
      "Run 2, Epoch 150, Loss: 1.409, Training Accuracy: 49.86%\n",
      "Run 2, Epoch 151, Loss: 1.413, Training Accuracy: 49.95%\n",
      "Run 2, Epoch 152, Loss: 1.410, Training Accuracy: 49.92%\n",
      "Run 2, Epoch 153, Loss: 1.414, Training Accuracy: 49.72%\n",
      "Run 2, Epoch 154, Loss: 1.408, Training Accuracy: 50.06%\n",
      "Run 2, Epoch 155, Loss: 1.413, Training Accuracy: 49.61%\n",
      "Run 2, Epoch 156, Loss: 1.409, Training Accuracy: 50.23%\n",
      "Run 2, Epoch 157, Loss: 1.414, Training Accuracy: 49.43%\n",
      "Run 2, Epoch 158, Loss: 1.413, Training Accuracy: 49.73%\n",
      "Run 2, Epoch 159, Loss: 1.409, Training Accuracy: 49.79%\n",
      "Run 2, Epoch 160, Loss: 1.411, Training Accuracy: 49.74%\n",
      "Run 2, Epoch 161, Loss: 1.411, Training Accuracy: 50.01%\n",
      "Run 2, Epoch 162, Loss: 1.413, Training Accuracy: 50.07%\n",
      "Run 2, Epoch 163, Loss: 1.411, Training Accuracy: 49.97%\n",
      "Run 2, Epoch 164, Loss: 1.411, Training Accuracy: 49.84%\n",
      "Run 2, Epoch 165, Loss: 1.411, Training Accuracy: 49.72%\n",
      "Run 2, Epoch 166, Loss: 1.412, Training Accuracy: 49.94%\n",
      "Run 2, Epoch 167, Loss: 1.414, Training Accuracy: 49.79%\n",
      "Run 2, Epoch 168, Loss: 1.408, Training Accuracy: 50.00%\n",
      "Run 2, Epoch 169, Loss: 1.413, Training Accuracy: 49.84%\n",
      "Run 2, Epoch 170, Loss: 1.410, Training Accuracy: 49.85%\n",
      "Run 2, Epoch 171, Loss: 1.413, Training Accuracy: 49.90%\n",
      "Run 2, Epoch 172, Loss: 1.411, Training Accuracy: 49.70%\n",
      "Run 2, Epoch 173, Loss: 1.404, Training Accuracy: 50.23%\n",
      "Run 2, Epoch 174, Loss: 1.413, Training Accuracy: 49.89%\n",
      "Run 2, Epoch 175, Loss: 1.409, Training Accuracy: 50.02%\n",
      "Run 2, Final Accuracy on test set: 53.02%\n",
      "Results after run 2:\n",
      "Training Accuracies: [30.308, 36.782, 39.37, 40.834, 42.496, 43.48, 44.846, 45.678, 46.912, 47.486, 48.654, 49.002, 49.226, 49.25, 49.17, 49.278, 49.318, 49.504, 49.896, 49.728, 49.84, 49.85, 49.766, 49.442, 49.688, 49.984, 49.83, 49.58, 49.958, 49.914, 49.87, 49.954, 49.75, 49.78, 49.94, 49.822, 49.914, 49.738, 50.014, 49.688, 49.76, 49.72, 49.964, 49.776, 50.098, 50.062, 49.558, 49.914, 49.918, 49.818, 49.878, 49.656, 49.842, 50.026, 49.72, 49.796, 49.952, 49.914, 49.772, 49.964, 50.136, 49.974, 50.136, 49.76, 49.992, 49.858, 49.874, 49.824, 49.836, 49.97, 49.944, 50.052, 49.78, 50.052, 49.854, 49.976, 49.864, 49.882, 50.012, 50.15, 49.776, 49.896, 49.74, 49.672, 49.854, 49.854, 49.792, 50.152, 49.832, 49.934, 50.008, 50.094, 50.0, 49.792, 49.926, 49.846, 49.874, 49.412, 50.188, 49.638, 50.054, 49.728, 49.834, 49.68, 50.018, 49.744, 49.86, 49.764, 49.934, 49.994, 49.722, 49.802, 49.946, 49.962, 49.99, 50.108, 49.852, 50.0, 49.93, 50.048, 49.81, 49.746, 49.738, 49.992, 50.038, 49.802, 49.968, 49.656, 49.998, 50.158, 49.88, 49.994, 49.79, 49.806, 49.878, 49.958, 49.718, 49.936, 50.032, 49.994, 50.188, 49.724, 50.098, 50.048, 49.804, 49.928, 49.884, 49.618, 50.054, 49.86, 49.954, 49.924, 49.722, 50.056, 49.61, 50.228, 49.43, 49.726, 49.788, 49.74, 50.014, 50.072, 49.968, 49.844, 49.724, 49.938, 49.788, 50.0, 49.838, 49.854, 49.896, 49.7, 50.228, 49.892, 50.016]\n",
      "Test Accuracy: 53.02%\n",
      "Losses: [1.9379417198088469, 1.7694195225415632, 1.6967967827911572, 1.650597888185545, 1.6074482001307067, 1.5736559129432035, 1.5408859707205498, 1.514337325949803, 1.4844223120633293, 1.4648522519699447, 1.442159669173648, 1.4355113146555087, 1.430628629596642, 1.4297422222469165, 1.4270745308502861, 1.4248287802766961, 1.4241503565512654, 1.42090723368213, 1.416657567329114, 1.4179116240547747, 1.4163641569864414, 1.4123395190519445, 1.415569113343573, 1.4165745619922647, 1.4145805436327024, 1.4115980479418468, 1.4125322633996948, 1.4108759856894804, 1.4126568493025993, 1.410819681709075, 1.4112358672539596, 1.4094766661943987, 1.4100707620001205, 1.411014130963084, 1.4125764284597333, 1.4112422438838599, 1.409214918265867, 1.4116338253631007, 1.4122842251492278, 1.4089518986699525, 1.4115301617576033, 1.4125941257037775, 1.4097908168192714, 1.4108014304924499, 1.4076923741708935, 1.4111429026059787, 1.4128120640659576, 1.4124074091996683, 1.4091962771037656, 1.4127228985662046, 1.410119147861705, 1.4111606190576578, 1.4073327032806318, 1.4067957236638764, 1.4115796945893857, 1.4125709262345454, 1.4113539442077012, 1.4100838436190124, 1.4096664016508995, 1.4098807460511737, 1.4092943034208645, 1.4091639960818279, 1.409083392309106, 1.414085222022308, 1.4084869257324493, 1.411881844405933, 1.4093242987342502, 1.4098882358092482, 1.4085327463076853, 1.4111302283108997, 1.4141370609898092, 1.4091199541945592, 1.4119194380157745, 1.4076593745395045, 1.414154235054465, 1.4093189114499884, 1.411218563309106, 1.4089886379973662, 1.4102704491456757, 1.4096300355003923, 1.4127676490017824, 1.410855717976075, 1.411354253968924, 1.4137287624656696, 1.412001798829764, 1.4106624095946017, 1.4143935929783775, 1.409558305045223, 1.4103952774306392, 1.4102919650504657, 1.4133109052467834, 1.4077224764982452, 1.4120685877397543, 1.4106120121143664, 1.4126143440261216, 1.4129753804877592, 1.4112465122471685, 1.4102018004488153, 1.4083411153929923, 1.4103532992970302, 1.4087238503843926, 1.416547900270623, 1.4136652223899235, 1.4134149161141243, 1.4091155294262234, 1.4130015885433578, 1.4093685891012402, 1.4119031551244008, 1.4118197244756363, 1.4093092347654845, 1.4086402133297737, 1.4111221879339584, 1.41162980517463, 1.4096784030689913, 1.410998241066018, 1.4102107935854236, 1.4093826856759504, 1.412389349449626, 1.4107733681378767, 1.4104843752463456, 1.4141954204920308, 1.411404818525095, 1.4075231396633645, 1.410178923241013, 1.4108194826204148, 1.411635043066176, 1.4114236654832846, 1.4158692744077015, 1.4090887094702562, 1.4101946649648953, 1.4074629557407117, 1.4074135939483448, 1.4113078586890568, 1.4147680159420004, 1.412449301661128, 1.411007669880567, 1.4134722121841157, 1.4076663008736223, 1.4091377889408785, 1.4111284953553964, 1.4088638947747858, 1.4129230768784233, 1.4113294587415808, 1.4086928949941455, 1.412006208049062, 1.4089049893571897, 1.4096349944239077, 1.4119105756740131, 1.408059007371478, 1.4088822767862579, 1.4127940021817336, 1.4100215364904964, 1.414046342415578, 1.4080254766337401, 1.4131190755483134, 1.4093338226723244, 1.4142153592365783, 1.4133293823817807, 1.4088498570425125, 1.4105515653824867, 1.4109114716424966, 1.4127076769728795, 1.410948069809038, 1.4105017139478717, 1.410864484889428, 1.4123343003680333, 1.4137947806311995, 1.4079076280374356, 1.4125366238376977, 1.4102137738176623, 1.4127377384458966, 1.4114166888434563, 1.4040484464991734, 1.4126372803812441, 1.4088148735368344]\n",
      "Starting run 3/5\n",
      "Run 3, Epoch 1, Loss: 1.917, Training Accuracy: 31.11%\n",
      "Run 3, Epoch 2, Loss: 1.737, Training Accuracy: 38.01%\n",
      "Run 3, Epoch 3, Loss: 1.660, Training Accuracy: 40.47%\n",
      "Run 3, Epoch 4, Loss: 1.607, Training Accuracy: 42.26%\n",
      "Run 3, Epoch 5, Loss: 1.560, Training Accuracy: 44.09%\n",
      "Run 3, Epoch 6, Loss: 1.526, Training Accuracy: 45.21%\n",
      "Run 3, Epoch 7, Loss: 1.500, Training Accuracy: 46.30%\n",
      "Run 3, Epoch 8, Loss: 1.471, Training Accuracy: 47.09%\n",
      "Run 3, Epoch 9, Loss: 1.453, Training Accuracy: 47.94%\n",
      "Run 3, Epoch 10, Loss: 1.428, Training Accuracy: 49.09%\n",
      "Run 3, Epoch 11, Loss: 1.405, Training Accuracy: 50.16%\n",
      "Run 3, Epoch 12, Loss: 1.399, Training Accuracy: 50.26%\n",
      "Run 3, Epoch 13, Loss: 1.398, Training Accuracy: 50.32%\n",
      "Run 3, Epoch 14, Loss: 1.395, Training Accuracy: 50.22%\n",
      "Run 3, Epoch 15, Loss: 1.391, Training Accuracy: 50.37%\n",
      "Run 3, Epoch 16, Loss: 1.389, Training Accuracy: 50.93%\n",
      "Run 3, Epoch 17, Loss: 1.386, Training Accuracy: 50.85%\n",
      "Run 3, Epoch 18, Loss: 1.383, Training Accuracy: 51.20%\n",
      "Run 3, Epoch 19, Loss: 1.381, Training Accuracy: 51.07%\n",
      "Run 3, Epoch 20, Loss: 1.377, Training Accuracy: 51.03%\n",
      "Run 3, Epoch 21, Loss: 1.376, Training Accuracy: 51.19%\n",
      "Run 3, Epoch 22, Loss: 1.374, Training Accuracy: 51.35%\n",
      "Run 3, Epoch 23, Loss: 1.380, Training Accuracy: 51.06%\n",
      "Run 3, Epoch 24, Loss: 1.373, Training Accuracy: 51.33%\n",
      "Run 3, Epoch 25, Loss: 1.375, Training Accuracy: 51.25%\n",
      "Run 3, Epoch 26, Loss: 1.374, Training Accuracy: 51.47%\n",
      "Run 3, Epoch 27, Loss: 1.373, Training Accuracy: 51.34%\n",
      "Run 3, Epoch 28, Loss: 1.377, Training Accuracy: 51.02%\n",
      "Run 3, Epoch 29, Loss: 1.373, Training Accuracy: 51.28%\n",
      "Run 3, Epoch 30, Loss: 1.374, Training Accuracy: 51.31%\n",
      "Run 3, Epoch 31, Loss: 1.377, Training Accuracy: 50.93%\n",
      "Run 3, Epoch 32, Loss: 1.373, Training Accuracy: 51.33%\n",
      "Run 3, Epoch 33, Loss: 1.371, Training Accuracy: 51.44%\n",
      "Run 3, Epoch 34, Loss: 1.375, Training Accuracy: 51.09%\n",
      "Run 3, Epoch 35, Loss: 1.374, Training Accuracy: 51.14%\n",
      "Run 3, Epoch 36, Loss: 1.374, Training Accuracy: 51.12%\n",
      "Run 3, Epoch 37, Loss: 1.377, Training Accuracy: 51.22%\n",
      "Run 3, Epoch 38, Loss: 1.375, Training Accuracy: 51.37%\n",
      "Run 3, Epoch 39, Loss: 1.375, Training Accuracy: 51.22%\n",
      "Run 3, Epoch 40, Loss: 1.371, Training Accuracy: 51.21%\n",
      "Run 3, Epoch 41, Loss: 1.373, Training Accuracy: 51.39%\n",
      "Run 3, Epoch 42, Loss: 1.373, Training Accuracy: 51.16%\n",
      "Run 3, Epoch 43, Loss: 1.374, Training Accuracy: 51.19%\n",
      "Run 3, Epoch 44, Loss: 1.371, Training Accuracy: 51.41%\n",
      "Run 3, Epoch 45, Loss: 1.373, Training Accuracy: 51.09%\n",
      "Run 3, Epoch 46, Loss: 1.372, Training Accuracy: 51.26%\n",
      "Run 3, Epoch 47, Loss: 1.374, Training Accuracy: 51.32%\n",
      "Run 3, Epoch 48, Loss: 1.373, Training Accuracy: 51.34%\n",
      "Run 3, Epoch 49, Loss: 1.375, Training Accuracy: 51.28%\n",
      "Run 3, Epoch 50, Loss: 1.374, Training Accuracy: 51.23%\n",
      "Run 3, Epoch 51, Loss: 1.372, Training Accuracy: 51.43%\n",
      "Run 3, Epoch 52, Loss: 1.375, Training Accuracy: 51.09%\n",
      "Run 3, Epoch 53, Loss: 1.374, Training Accuracy: 51.27%\n",
      "Run 3, Epoch 54, Loss: 1.374, Training Accuracy: 51.11%\n",
      "Run 3, Epoch 55, Loss: 1.374, Training Accuracy: 51.25%\n",
      "Run 3, Epoch 56, Loss: 1.377, Training Accuracy: 51.22%\n",
      "Run 3, Epoch 57, Loss: 1.376, Training Accuracy: 51.32%\n",
      "Run 3, Epoch 58, Loss: 1.374, Training Accuracy: 51.14%\n",
      "Run 3, Epoch 59, Loss: 1.373, Training Accuracy: 51.36%\n",
      "Run 3, Epoch 60, Loss: 1.376, Training Accuracy: 51.42%\n",
      "Run 3, Epoch 61, Loss: 1.371, Training Accuracy: 51.42%\n",
      "Run 3, Epoch 62, Loss: 1.372, Training Accuracy: 51.45%\n",
      "Run 3, Epoch 63, Loss: 1.373, Training Accuracy: 51.29%\n",
      "Run 3, Epoch 64, Loss: 1.372, Training Accuracy: 51.40%\n",
      "Run 3, Epoch 65, Loss: 1.377, Training Accuracy: 51.23%\n",
      "Run 3, Epoch 66, Loss: 1.374, Training Accuracy: 51.24%\n",
      "Run 3, Epoch 67, Loss: 1.370, Training Accuracy: 51.41%\n",
      "Run 3, Epoch 68, Loss: 1.375, Training Accuracy: 51.14%\n",
      "Run 3, Epoch 69, Loss: 1.370, Training Accuracy: 51.48%\n",
      "Run 3, Epoch 70, Loss: 1.374, Training Accuracy: 51.11%\n",
      "Run 3, Epoch 71, Loss: 1.376, Training Accuracy: 51.35%\n",
      "Run 3, Epoch 72, Loss: 1.374, Training Accuracy: 51.39%\n",
      "Run 3, Epoch 73, Loss: 1.371, Training Accuracy: 51.48%\n",
      "Run 3, Epoch 74, Loss: 1.375, Training Accuracy: 51.41%\n",
      "Run 3, Epoch 75, Loss: 1.376, Training Accuracy: 51.22%\n",
      "Run 3, Epoch 76, Loss: 1.371, Training Accuracy: 51.29%\n",
      "Run 3, Epoch 77, Loss: 1.374, Training Accuracy: 51.31%\n",
      "Run 3, Epoch 78, Loss: 1.372, Training Accuracy: 51.30%\n",
      "Run 3, Epoch 79, Loss: 1.378, Training Accuracy: 51.13%\n",
      "Run 3, Epoch 80, Loss: 1.368, Training Accuracy: 51.40%\n",
      "Run 3, Epoch 81, Loss: 1.371, Training Accuracy: 51.43%\n",
      "Run 3, Epoch 82, Loss: 1.372, Training Accuracy: 51.28%\n",
      "Run 3, Epoch 83, Loss: 1.373, Training Accuracy: 51.46%\n",
      "Run 3, Epoch 84, Loss: 1.375, Training Accuracy: 51.51%\n",
      "Run 3, Epoch 85, Loss: 1.374, Training Accuracy: 51.16%\n",
      "Run 3, Epoch 86, Loss: 1.371, Training Accuracy: 51.27%\n",
      "Run 3, Epoch 87, Loss: 1.375, Training Accuracy: 51.26%\n",
      "Run 3, Epoch 88, Loss: 1.377, Training Accuracy: 51.22%\n",
      "Run 3, Epoch 89, Loss: 1.372, Training Accuracy: 51.53%\n",
      "Run 3, Epoch 90, Loss: 1.374, Training Accuracy: 51.33%\n",
      "Run 3, Epoch 91, Loss: 1.375, Training Accuracy: 51.09%\n",
      "Run 3, Epoch 92, Loss: 1.372, Training Accuracy: 51.16%\n",
      "Run 3, Epoch 93, Loss: 1.375, Training Accuracy: 51.17%\n",
      "Run 3, Epoch 94, Loss: 1.373, Training Accuracy: 51.31%\n",
      "Run 3, Epoch 95, Loss: 1.373, Training Accuracy: 51.07%\n",
      "Run 3, Epoch 96, Loss: 1.375, Training Accuracy: 51.24%\n",
      "Run 3, Epoch 97, Loss: 1.372, Training Accuracy: 51.40%\n",
      "Run 3, Epoch 98, Loss: 1.372, Training Accuracy: 51.26%\n",
      "Run 3, Epoch 99, Loss: 1.375, Training Accuracy: 51.14%\n",
      "Run 3, Epoch 100, Loss: 1.376, Training Accuracy: 51.41%\n",
      "Run 3, Epoch 101, Loss: 1.373, Training Accuracy: 51.40%\n",
      "Run 3, Epoch 102, Loss: 1.370, Training Accuracy: 51.35%\n",
      "Run 3, Epoch 103, Loss: 1.374, Training Accuracy: 51.25%\n",
      "Run 3, Epoch 104, Loss: 1.372, Training Accuracy: 51.31%\n",
      "Run 3, Epoch 105, Loss: 1.376, Training Accuracy: 51.21%\n",
      "Run 3, Epoch 106, Loss: 1.371, Training Accuracy: 51.37%\n",
      "Run 3, Epoch 107, Loss: 1.376, Training Accuracy: 51.32%\n",
      "Run 3, Epoch 108, Loss: 1.373, Training Accuracy: 51.27%\n",
      "Run 3, Epoch 109, Loss: 1.371, Training Accuracy: 51.62%\n",
      "Run 3, Epoch 110, Loss: 1.371, Training Accuracy: 51.39%\n",
      "Run 3, Epoch 111, Loss: 1.375, Training Accuracy: 51.17%\n",
      "Run 3, Epoch 112, Loss: 1.377, Training Accuracy: 51.21%\n",
      "Run 3, Epoch 113, Loss: 1.375, Training Accuracy: 51.19%\n",
      "Run 3, Epoch 114, Loss: 1.377, Training Accuracy: 51.21%\n",
      "Run 3, Epoch 115, Loss: 1.376, Training Accuracy: 51.19%\n",
      "Run 3, Epoch 116, Loss: 1.375, Training Accuracy: 51.34%\n",
      "Run 3, Epoch 117, Loss: 1.374, Training Accuracy: 51.50%\n",
      "Run 3, Epoch 118, Loss: 1.371, Training Accuracy: 51.32%\n",
      "Run 3, Epoch 119, Loss: 1.372, Training Accuracy: 51.27%\n",
      "Run 3, Epoch 120, Loss: 1.372, Training Accuracy: 51.23%\n",
      "Run 3, Epoch 121, Loss: 1.375, Training Accuracy: 51.32%\n",
      "Run 3, Epoch 122, Loss: 1.373, Training Accuracy: 51.31%\n",
      "Run 3, Epoch 123, Loss: 1.373, Training Accuracy: 51.41%\n",
      "Run 3, Epoch 124, Loss: 1.372, Training Accuracy: 51.26%\n",
      "Run 3, Epoch 125, Loss: 1.372, Training Accuracy: 51.44%\n",
      "Run 3, Epoch 126, Loss: 1.374, Training Accuracy: 51.13%\n",
      "Run 3, Epoch 127, Loss: 1.373, Training Accuracy: 51.32%\n",
      "Run 3, Epoch 128, Loss: 1.375, Training Accuracy: 51.29%\n",
      "Run 3, Epoch 129, Loss: 1.374, Training Accuracy: 51.42%\n",
      "Run 3, Epoch 130, Loss: 1.375, Training Accuracy: 51.15%\n",
      "Run 3, Epoch 131, Loss: 1.373, Training Accuracy: 51.24%\n",
      "Run 3, Epoch 132, Loss: 1.372, Training Accuracy: 51.38%\n",
      "Run 3, Epoch 133, Loss: 1.379, Training Accuracy: 50.96%\n",
      "Run 3, Epoch 134, Loss: 1.377, Training Accuracy: 51.17%\n",
      "Run 3, Epoch 135, Loss: 1.373, Training Accuracy: 51.28%\n",
      "Run 3, Epoch 136, Loss: 1.373, Training Accuracy: 51.31%\n",
      "Run 3, Epoch 137, Loss: 1.372, Training Accuracy: 51.55%\n",
      "Run 3, Epoch 138, Loss: 1.372, Training Accuracy: 51.37%\n",
      "Run 3, Epoch 139, Loss: 1.377, Training Accuracy: 51.36%\n",
      "Run 3, Epoch 140, Loss: 1.375, Training Accuracy: 51.18%\n",
      "Run 3, Epoch 141, Loss: 1.375, Training Accuracy: 51.22%\n",
      "Run 3, Epoch 142, Loss: 1.371, Training Accuracy: 51.24%\n",
      "Run 3, Epoch 143, Loss: 1.373, Training Accuracy: 51.32%\n",
      "Run 3, Epoch 144, Loss: 1.375, Training Accuracy: 51.16%\n",
      "Run 3, Epoch 145, Loss: 1.374, Training Accuracy: 51.27%\n",
      "Run 3, Epoch 146, Loss: 1.374, Training Accuracy: 51.26%\n",
      "Run 3, Epoch 147, Loss: 1.375, Training Accuracy: 51.36%\n",
      "Run 3, Epoch 148, Loss: 1.373, Training Accuracy: 51.23%\n",
      "Run 3, Epoch 149, Loss: 1.375, Training Accuracy: 51.04%\n",
      "Run 3, Epoch 150, Loss: 1.374, Training Accuracy: 51.29%\n",
      "Run 3, Epoch 151, Loss: 1.376, Training Accuracy: 51.09%\n",
      "Run 3, Epoch 152, Loss: 1.373, Training Accuracy: 51.16%\n",
      "Run 3, Epoch 153, Loss: 1.373, Training Accuracy: 51.30%\n",
      "Run 3, Epoch 154, Loss: 1.376, Training Accuracy: 51.18%\n",
      "Run 3, Epoch 155, Loss: 1.373, Training Accuracy: 51.32%\n",
      "Run 3, Epoch 156, Loss: 1.373, Training Accuracy: 51.34%\n",
      "Run 3, Epoch 157, Loss: 1.371, Training Accuracy: 51.33%\n",
      "Run 3, Epoch 158, Loss: 1.374, Training Accuracy: 51.03%\n",
      "Run 3, Epoch 159, Loss: 1.376, Training Accuracy: 51.10%\n",
      "Run 3, Epoch 160, Loss: 1.372, Training Accuracy: 51.50%\n",
      "Run 3, Epoch 161, Loss: 1.372, Training Accuracy: 51.38%\n",
      "Run 3, Epoch 162, Loss: 1.371, Training Accuracy: 51.53%\n",
      "Run 3, Epoch 163, Loss: 1.374, Training Accuracy: 51.28%\n",
      "Run 3, Epoch 164, Loss: 1.374, Training Accuracy: 51.21%\n",
      "Run 3, Epoch 165, Loss: 1.374, Training Accuracy: 51.41%\n",
      "Run 3, Epoch 166, Loss: 1.374, Training Accuracy: 51.44%\n",
      "Run 3, Epoch 167, Loss: 1.374, Training Accuracy: 51.13%\n",
      "Run 3, Epoch 168, Loss: 1.372, Training Accuracy: 51.40%\n",
      "Run 3, Epoch 169, Loss: 1.375, Training Accuracy: 51.51%\n",
      "Run 3, Epoch 170, Loss: 1.373, Training Accuracy: 51.41%\n",
      "Run 3, Epoch 171, Loss: 1.375, Training Accuracy: 51.20%\n",
      "Run 3, Epoch 172, Loss: 1.375, Training Accuracy: 51.49%\n",
      "Run 3, Epoch 173, Loss: 1.376, Training Accuracy: 51.21%\n",
      "Run 3, Epoch 174, Loss: 1.374, Training Accuracy: 51.31%\n",
      "Run 3, Epoch 175, Loss: 1.373, Training Accuracy: 51.16%\n",
      "Run 3, Final Accuracy on test set: 54.98%\n",
      "Results after run 3:\n",
      "Training Accuracies: [31.106, 38.01, 40.474, 42.258, 44.092, 45.212, 46.3, 47.094, 47.944, 49.09, 50.162, 50.256, 50.32, 50.22, 50.366, 50.93, 50.846, 51.2, 51.068, 51.032, 51.194, 51.348, 51.062, 51.332, 51.248, 51.474, 51.338, 51.02, 51.28, 51.312, 50.926, 51.326, 51.444, 51.086, 51.138, 51.122, 51.22, 51.374, 51.222, 51.212, 51.39, 51.156, 51.194, 51.406, 51.088, 51.258, 51.324, 51.34, 51.28, 51.23, 51.432, 51.09, 51.266, 51.112, 51.248, 51.222, 51.322, 51.136, 51.364, 51.42, 51.422, 51.446, 51.294, 51.402, 51.226, 51.244, 51.406, 51.142, 51.478, 51.114, 51.352, 51.386, 51.48, 51.41, 51.224, 51.29, 51.31, 51.298, 51.134, 51.402, 51.43, 51.282, 51.456, 51.51, 51.16, 51.274, 51.258, 51.22, 51.526, 51.334, 51.088, 51.162, 51.172, 51.314, 51.074, 51.24, 51.396, 51.256, 51.142, 51.406, 51.4, 51.352, 51.254, 51.306, 51.208, 51.366, 51.322, 51.268, 51.622, 51.388, 51.17, 51.21, 51.194, 51.21, 51.192, 51.336, 51.502, 51.318, 51.268, 51.226, 51.324, 51.306, 51.41, 51.258, 51.436, 51.128, 51.316, 51.294, 51.422, 51.152, 51.238, 51.376, 50.962, 51.166, 51.28, 51.31, 51.546, 51.372, 51.36, 51.184, 51.22, 51.242, 51.316, 51.158, 51.272, 51.26, 51.362, 51.226, 51.042, 51.288, 51.094, 51.16, 51.298, 51.176, 51.316, 51.342, 51.334, 51.026, 51.104, 51.498, 51.378, 51.532, 51.276, 51.206, 51.408, 51.436, 51.134, 51.396, 51.51, 51.408, 51.196, 51.488, 51.208, 51.312, 51.156]\n",
      "Test Accuracy: 54.98%\n",
      "Losses: [1.9165282209815881, 1.7374821654366106, 1.6604049583835065, 1.6069805872104967, 1.5603542901061076, 1.5255152545011867, 1.5000124624013291, 1.470877580935388, 1.4529578883934509, 1.4276455659085832, 1.4050421364167158, 1.3989282901329763, 1.397612139392082, 1.394797983071993, 1.3914052241903436, 1.3889675747098216, 1.386155184882376, 1.3825874188367058, 1.3808657403492257, 1.3770143473544694, 1.3762555719946352, 1.3742286116265885, 1.3800837396050962, 1.3726450881689711, 1.3752865242531231, 1.3735312467340923, 1.3729494621076852, 1.376997803788051, 1.3729720515058474, 1.373737015382713, 1.3767339012506978, 1.372773724138889, 1.3708926503310728, 1.3753463014617295, 1.3742328321232515, 1.3744710336255905, 1.3772215492585127, 1.3751175900554413, 1.3749254612666566, 1.371373724144743, 1.3729294511058447, 1.372995633908245, 1.3739017336569783, 1.3707741267235993, 1.3734661069367549, 1.3717040241221943, 1.373511934829185, 1.3727978056349108, 1.3746996922871035, 1.3744085313718948, 1.3719061847842868, 1.3748368652885223, 1.373965337453291, 1.3737691643902712, 1.3736562692295864, 1.3767474895852911, 1.3756137150327872, 1.3738804009869277, 1.3728246954091066, 1.3761978807961543, 1.3706919787180087, 1.3724291294127169, 1.373200434553044, 1.3721039148852647, 1.3765176513310893, 1.3743237933844252, 1.3704496905626848, 1.3747786384104463, 1.3702667805239976, 1.3736065495044678, 1.3755759616642047, 1.373954454346386, 1.370641077570903, 1.3749250557721424, 1.3757555573187825, 1.3714631972715372, 1.373561890838701, 1.3721145181094898, 1.3775920956336019, 1.368048766689837, 1.3713012436771637, 1.3719233558001116, 1.372981965389398, 1.374552053563735, 1.373843541230692, 1.3710695935027373, 1.374646916413856, 1.37679588672755, 1.371716652075043, 1.3735128223438702, 1.3751198309461783, 1.3724844352058743, 1.3751855491067442, 1.3728142787733346, 1.3733402896110358, 1.3746271066348572, 1.3723331619711483, 1.3716865674309109, 1.3747151477257613, 1.3760512228816977, 1.372512137493514, 1.3700007877081557, 1.3735944252184895, 1.3715792708384715, 1.3762195235323114, 1.37099392883613, 1.375574136329124, 1.3729219973239752, 1.3712189374372477, 1.3706096243065642, 1.375047268160164, 1.3769250766700492, 1.3745670772879326, 1.3768147760644898, 1.375877792268153, 1.3748719515398031, 1.374442148391548, 1.371495892934482, 1.3716424179199103, 1.371980028384177, 1.3752220085514781, 1.372839286199311, 1.3729512590886381, 1.3720212516272465, 1.371917734060751, 1.3743041677548147, 1.3734748525082912, 1.3753881844718132, 1.3743029381612988, 1.3753731165395673, 1.3731546752593096, 1.3718942580625528, 1.3791266719398596, 1.3767314079167592, 1.3732430654413559, 1.373003657516616, 1.372197809426681, 1.3720144649295856, 1.3768845638045875, 1.374682107544921, 1.375455089237379, 1.371311867328556, 1.3733078006588284, 1.3747768841131265, 1.373528479920019, 1.3741627245607888, 1.3748530125069192, 1.3730538165782724, 1.3750177734648175, 1.373676850972578, 1.3761370895463791, 1.373094638900074, 1.3731884483791068, 1.3761966972399855, 1.3728301101328466, 1.372812750394387, 1.3706878143198349, 1.3744755583955808, 1.376165085131555, 1.3718441148548175, 1.3717767488011314, 1.3713577543683064, 1.3739406998505068, 1.3742814283541707, 1.3738030844637195, 1.373643088218806, 1.3743635577618922, 1.372124926513418, 1.3753249544621733, 1.3725179430773802, 1.374755409672437, 1.37526098297685, 1.375771905150255, 1.3736412540421157, 1.3729845818961064]\n",
      "Starting run 4/5\n",
      "Run 4, Epoch 1, Loss: 1.942, Training Accuracy: 30.13%\n",
      "Run 4, Epoch 2, Loss: 1.750, Training Accuracy: 37.21%\n",
      "Run 4, Epoch 3, Loss: 1.658, Training Accuracy: 40.29%\n",
      "Run 4, Epoch 4, Loss: 1.600, Training Accuracy: 42.27%\n",
      "Run 4, Epoch 5, Loss: 1.559, Training Accuracy: 43.83%\n",
      "Run 4, Epoch 6, Loss: 1.530, Training Accuracy: 45.22%\n",
      "Run 4, Epoch 7, Loss: 1.506, Training Accuracy: 46.13%\n",
      "Run 4, Epoch 8, Loss: 1.485, Training Accuracy: 46.49%\n",
      "Run 4, Epoch 9, Loss: 1.466, Training Accuracy: 47.42%\n",
      "Run 4, Epoch 10, Loss: 1.453, Training Accuracy: 48.20%\n",
      "Run 4, Epoch 11, Loss: 1.431, Training Accuracy: 49.13%\n",
      "Run 4, Epoch 12, Loss: 1.426, Training Accuracy: 49.39%\n",
      "Run 4, Epoch 13, Loss: 1.423, Training Accuracy: 49.36%\n",
      "Run 4, Epoch 14, Loss: 1.418, Training Accuracy: 49.49%\n",
      "Run 4, Epoch 15, Loss: 1.418, Training Accuracy: 49.52%\n",
      "Run 4, Epoch 16, Loss: 1.414, Training Accuracy: 49.61%\n",
      "Run 4, Epoch 17, Loss: 1.415, Training Accuracy: 49.51%\n",
      "Run 4, Epoch 18, Loss: 1.411, Training Accuracy: 49.89%\n",
      "Run 4, Epoch 19, Loss: 1.408, Training Accuracy: 49.78%\n",
      "Run 4, Epoch 20, Loss: 1.408, Training Accuracy: 49.71%\n",
      "Run 4, Epoch 21, Loss: 1.406, Training Accuracy: 49.93%\n",
      "Run 4, Epoch 22, Loss: 1.405, Training Accuracy: 49.88%\n",
      "Run 4, Epoch 23, Loss: 1.405, Training Accuracy: 49.98%\n",
      "Run 4, Epoch 24, Loss: 1.407, Training Accuracy: 50.04%\n",
      "Run 4, Epoch 25, Loss: 1.408, Training Accuracy: 50.28%\n",
      "Run 4, Epoch 26, Loss: 1.404, Training Accuracy: 50.20%\n",
      "Run 4, Epoch 27, Loss: 1.407, Training Accuracy: 49.98%\n",
      "Run 4, Epoch 28, Loss: 1.403, Training Accuracy: 50.11%\n",
      "Run 4, Epoch 29, Loss: 1.406, Training Accuracy: 49.99%\n",
      "Run 4, Epoch 30, Loss: 1.402, Training Accuracy: 50.21%\n",
      "Run 4, Epoch 31, Loss: 1.401, Training Accuracy: 50.28%\n",
      "Run 4, Epoch 32, Loss: 1.404, Training Accuracy: 50.00%\n",
      "Run 4, Epoch 33, Loss: 1.398, Training Accuracy: 50.41%\n",
      "Run 4, Epoch 34, Loss: 1.402, Training Accuracy: 50.22%\n",
      "Run 4, Epoch 35, Loss: 1.404, Training Accuracy: 50.11%\n",
      "Run 4, Epoch 36, Loss: 1.401, Training Accuracy: 50.33%\n",
      "Run 4, Epoch 37, Loss: 1.401, Training Accuracy: 50.24%\n",
      "Run 4, Epoch 38, Loss: 1.401, Training Accuracy: 50.38%\n",
      "Run 4, Epoch 39, Loss: 1.401, Training Accuracy: 50.17%\n",
      "Run 4, Epoch 40, Loss: 1.400, Training Accuracy: 50.19%\n",
      "Run 4, Epoch 41, Loss: 1.401, Training Accuracy: 50.25%\n",
      "Run 4, Epoch 42, Loss: 1.403, Training Accuracy: 50.20%\n",
      "Run 4, Epoch 43, Loss: 1.399, Training Accuracy: 50.11%\n",
      "Run 4, Epoch 44, Loss: 1.402, Training Accuracy: 50.26%\n",
      "Run 4, Epoch 45, Loss: 1.398, Training Accuracy: 50.39%\n",
      "Run 4, Epoch 46, Loss: 1.403, Training Accuracy: 50.16%\n",
      "Run 4, Epoch 47, Loss: 1.403, Training Accuracy: 50.21%\n",
      "Run 4, Epoch 48, Loss: 1.400, Training Accuracy: 50.20%\n",
      "Run 4, Epoch 49, Loss: 1.401, Training Accuracy: 50.28%\n",
      "Run 4, Epoch 50, Loss: 1.403, Training Accuracy: 50.19%\n",
      "Run 4, Epoch 51, Loss: 1.403, Training Accuracy: 50.09%\n",
      "Run 4, Epoch 52, Loss: 1.401, Training Accuracy: 50.13%\n",
      "Run 4, Epoch 53, Loss: 1.403, Training Accuracy: 50.20%\n",
      "Run 4, Epoch 54, Loss: 1.403, Training Accuracy: 50.02%\n",
      "Run 4, Epoch 55, Loss: 1.402, Training Accuracy: 50.16%\n",
      "Run 4, Epoch 56, Loss: 1.406, Training Accuracy: 50.08%\n",
      "Run 4, Epoch 57, Loss: 1.403, Training Accuracy: 50.31%\n",
      "Run 4, Epoch 58, Loss: 1.397, Training Accuracy: 50.19%\n",
      "Run 4, Epoch 59, Loss: 1.404, Training Accuracy: 49.85%\n",
      "Run 4, Epoch 60, Loss: 1.401, Training Accuracy: 50.28%\n",
      "Run 4, Epoch 61, Loss: 1.401, Training Accuracy: 50.27%\n",
      "Run 4, Epoch 62, Loss: 1.404, Training Accuracy: 49.86%\n",
      "Run 4, Epoch 63, Loss: 1.402, Training Accuracy: 50.18%\n",
      "Run 4, Epoch 64, Loss: 1.403, Training Accuracy: 49.99%\n",
      "Run 4, Epoch 65, Loss: 1.400, Training Accuracy: 50.12%\n",
      "Run 4, Epoch 66, Loss: 1.402, Training Accuracy: 50.28%\n",
      "Run 4, Epoch 67, Loss: 1.401, Training Accuracy: 50.29%\n",
      "Run 4, Epoch 68, Loss: 1.398, Training Accuracy: 50.33%\n",
      "Run 4, Epoch 69, Loss: 1.402, Training Accuracy: 49.89%\n",
      "Run 4, Epoch 70, Loss: 1.402, Training Accuracy: 50.24%\n",
      "Run 4, Epoch 71, Loss: 1.399, Training Accuracy: 50.39%\n",
      "Run 4, Epoch 72, Loss: 1.401, Training Accuracy: 50.00%\n",
      "Run 4, Epoch 73, Loss: 1.403, Training Accuracy: 50.26%\n",
      "Run 4, Epoch 74, Loss: 1.402, Training Accuracy: 50.04%\n",
      "Run 4, Epoch 75, Loss: 1.401, Training Accuracy: 50.23%\n",
      "Run 4, Epoch 76, Loss: 1.400, Training Accuracy: 50.17%\n",
      "Run 4, Epoch 77, Loss: 1.403, Training Accuracy: 50.10%\n",
      "Run 4, Epoch 78, Loss: 1.405, Training Accuracy: 49.98%\n",
      "Run 4, Epoch 79, Loss: 1.405, Training Accuracy: 50.03%\n",
      "Run 4, Epoch 80, Loss: 1.404, Training Accuracy: 50.03%\n",
      "Run 4, Epoch 81, Loss: 1.401, Training Accuracy: 50.11%\n",
      "Run 4, Epoch 82, Loss: 1.402, Training Accuracy: 50.16%\n",
      "Run 4, Epoch 83, Loss: 1.402, Training Accuracy: 50.31%\n",
      "Run 4, Epoch 84, Loss: 1.404, Training Accuracy: 50.17%\n",
      "Run 4, Epoch 85, Loss: 1.401, Training Accuracy: 50.20%\n",
      "Run 4, Epoch 86, Loss: 1.399, Training Accuracy: 49.98%\n",
      "Run 4, Epoch 87, Loss: 1.403, Training Accuracy: 50.22%\n",
      "Run 4, Epoch 88, Loss: 1.399, Training Accuracy: 50.37%\n",
      "Run 4, Epoch 89, Loss: 1.401, Training Accuracy: 50.11%\n",
      "Run 4, Epoch 90, Loss: 1.399, Training Accuracy: 50.26%\n",
      "Run 4, Epoch 91, Loss: 1.402, Training Accuracy: 50.10%\n",
      "Run 4, Epoch 92, Loss: 1.403, Training Accuracy: 50.16%\n",
      "Run 4, Epoch 93, Loss: 1.402, Training Accuracy: 49.95%\n",
      "Run 4, Epoch 94, Loss: 1.401, Training Accuracy: 50.12%\n",
      "Run 4, Epoch 95, Loss: 1.397, Training Accuracy: 50.31%\n",
      "Run 4, Epoch 96, Loss: 1.400, Training Accuracy: 50.09%\n",
      "Run 4, Epoch 97, Loss: 1.400, Training Accuracy: 50.17%\n",
      "Run 4, Epoch 98, Loss: 1.403, Training Accuracy: 50.09%\n",
      "Run 4, Epoch 99, Loss: 1.400, Training Accuracy: 50.46%\n",
      "Run 4, Epoch 100, Loss: 1.403, Training Accuracy: 49.96%\n",
      "Run 4, Epoch 101, Loss: 1.400, Training Accuracy: 50.36%\n",
      "Run 4, Epoch 102, Loss: 1.402, Training Accuracy: 50.31%\n",
      "Run 4, Epoch 103, Loss: 1.402, Training Accuracy: 50.48%\n",
      "Run 4, Epoch 104, Loss: 1.404, Training Accuracy: 50.17%\n",
      "Run 4, Epoch 105, Loss: 1.402, Training Accuracy: 50.25%\n",
      "Run 4, Epoch 106, Loss: 1.405, Training Accuracy: 50.17%\n",
      "Run 4, Epoch 107, Loss: 1.404, Training Accuracy: 50.12%\n",
      "Run 4, Epoch 108, Loss: 1.404, Training Accuracy: 50.30%\n",
      "Run 4, Epoch 109, Loss: 1.399, Training Accuracy: 50.33%\n",
      "Run 4, Epoch 110, Loss: 1.402, Training Accuracy: 50.11%\n",
      "Run 4, Epoch 111, Loss: 1.404, Training Accuracy: 50.06%\n",
      "Run 4, Epoch 112, Loss: 1.400, Training Accuracy: 50.11%\n",
      "Run 4, Epoch 113, Loss: 1.400, Training Accuracy: 50.53%\n",
      "Run 4, Epoch 114, Loss: 1.400, Training Accuracy: 50.37%\n",
      "Run 4, Epoch 115, Loss: 1.401, Training Accuracy: 50.15%\n",
      "Run 4, Epoch 116, Loss: 1.401, Training Accuracy: 50.21%\n",
      "Run 4, Epoch 117, Loss: 1.402, Training Accuracy: 50.23%\n",
      "Run 4, Epoch 118, Loss: 1.403, Training Accuracy: 50.11%\n",
      "Run 4, Epoch 119, Loss: 1.402, Training Accuracy: 50.19%\n",
      "Run 4, Epoch 120, Loss: 1.403, Training Accuracy: 50.15%\n",
      "Run 4, Epoch 121, Loss: 1.400, Training Accuracy: 50.49%\n",
      "Run 4, Epoch 122, Loss: 1.404, Training Accuracy: 50.31%\n",
      "Run 4, Epoch 123, Loss: 1.402, Training Accuracy: 50.35%\n",
      "Run 4, Epoch 124, Loss: 1.402, Training Accuracy: 50.23%\n",
      "Run 4, Epoch 125, Loss: 1.403, Training Accuracy: 50.33%\n",
      "Run 4, Epoch 126, Loss: 1.397, Training Accuracy: 50.32%\n",
      "Run 4, Epoch 127, Loss: 1.403, Training Accuracy: 50.18%\n",
      "Run 4, Epoch 128, Loss: 1.398, Training Accuracy: 50.33%\n",
      "Run 4, Epoch 129, Loss: 1.404, Training Accuracy: 49.91%\n",
      "Run 4, Epoch 130, Loss: 1.402, Training Accuracy: 50.12%\n",
      "Run 4, Epoch 131, Loss: 1.402, Training Accuracy: 50.18%\n",
      "Run 4, Epoch 132, Loss: 1.402, Training Accuracy: 50.12%\n",
      "Run 4, Epoch 133, Loss: 1.404, Training Accuracy: 49.90%\n",
      "Run 4, Epoch 134, Loss: 1.399, Training Accuracy: 50.26%\n",
      "Run 4, Epoch 135, Loss: 1.403, Training Accuracy: 50.16%\n",
      "Run 4, Epoch 136, Loss: 1.404, Training Accuracy: 50.20%\n",
      "Run 4, Epoch 137, Loss: 1.399, Training Accuracy: 50.31%\n",
      "Run 4, Epoch 138, Loss: 1.403, Training Accuracy: 50.07%\n",
      "Run 4, Epoch 139, Loss: 1.403, Training Accuracy: 50.35%\n",
      "Run 4, Epoch 140, Loss: 1.401, Training Accuracy: 50.26%\n",
      "Run 4, Epoch 141, Loss: 1.402, Training Accuracy: 50.00%\n",
      "Run 4, Epoch 142, Loss: 1.400, Training Accuracy: 50.60%\n",
      "Run 4, Epoch 143, Loss: 1.398, Training Accuracy: 50.38%\n",
      "Run 4, Epoch 144, Loss: 1.405, Training Accuracy: 50.02%\n",
      "Run 4, Epoch 145, Loss: 1.400, Training Accuracy: 50.37%\n",
      "Run 4, Epoch 146, Loss: 1.399, Training Accuracy: 50.52%\n",
      "Run 4, Epoch 147, Loss: 1.399, Training Accuracy: 50.21%\n",
      "Run 4, Epoch 148, Loss: 1.402, Training Accuracy: 50.44%\n",
      "Run 4, Epoch 149, Loss: 1.400, Training Accuracy: 50.21%\n",
      "Run 4, Epoch 150, Loss: 1.402, Training Accuracy: 50.13%\n",
      "Run 4, Epoch 151, Loss: 1.403, Training Accuracy: 49.93%\n",
      "Run 4, Epoch 152, Loss: 1.401, Training Accuracy: 50.12%\n",
      "Run 4, Epoch 153, Loss: 1.404, Training Accuracy: 49.99%\n",
      "Run 4, Epoch 154, Loss: 1.401, Training Accuracy: 50.41%\n",
      "Run 4, Epoch 155, Loss: 1.400, Training Accuracy: 50.01%\n",
      "Run 4, Epoch 156, Loss: 1.401, Training Accuracy: 50.25%\n",
      "Run 4, Epoch 157, Loss: 1.400, Training Accuracy: 50.45%\n",
      "Run 4, Epoch 158, Loss: 1.400, Training Accuracy: 50.29%\n",
      "Run 4, Epoch 159, Loss: 1.399, Training Accuracy: 50.34%\n",
      "Run 4, Epoch 160, Loss: 1.402, Training Accuracy: 50.07%\n",
      "Run 4, Epoch 161, Loss: 1.400, Training Accuracy: 50.15%\n",
      "Run 4, Epoch 162, Loss: 1.403, Training Accuracy: 50.07%\n",
      "Run 4, Epoch 163, Loss: 1.404, Training Accuracy: 50.14%\n",
      "Run 4, Epoch 164, Loss: 1.401, Training Accuracy: 50.26%\n",
      "Run 4, Epoch 165, Loss: 1.403, Training Accuracy: 49.80%\n",
      "Run 4, Epoch 166, Loss: 1.403, Training Accuracy: 50.03%\n",
      "Run 4, Epoch 167, Loss: 1.403, Training Accuracy: 50.18%\n",
      "Run 4, Epoch 168, Loss: 1.403, Training Accuracy: 50.15%\n",
      "Run 4, Epoch 169, Loss: 1.403, Training Accuracy: 50.20%\n",
      "Run 4, Epoch 170, Loss: 1.401, Training Accuracy: 50.26%\n",
      "Run 4, Epoch 171, Loss: 1.401, Training Accuracy: 50.10%\n",
      "Run 4, Epoch 172, Loss: 1.404, Training Accuracy: 49.97%\n",
      "Run 4, Epoch 173, Loss: 1.403, Training Accuracy: 50.02%\n",
      "Run 4, Epoch 174, Loss: 1.401, Training Accuracy: 50.02%\n",
      "Run 4, Epoch 175, Loss: 1.401, Training Accuracy: 50.21%\n",
      "Run 4, Final Accuracy on test set: 53.45%\n",
      "Results after run 4:\n",
      "Training Accuracies: [30.134, 37.206, 40.292, 42.274, 43.834, 45.22, 46.132, 46.494, 47.416, 48.2, 49.134, 49.392, 49.358, 49.494, 49.522, 49.608, 49.506, 49.894, 49.778, 49.71, 49.93, 49.878, 49.978, 50.036, 50.276, 50.198, 49.98, 50.108, 49.986, 50.208, 50.276, 49.996, 50.41, 50.218, 50.106, 50.334, 50.238, 50.376, 50.17, 50.186, 50.246, 50.196, 50.114, 50.262, 50.388, 50.158, 50.21, 50.204, 50.282, 50.186, 50.086, 50.126, 50.204, 50.02, 50.16, 50.082, 50.31, 50.192, 49.852, 50.282, 50.27, 49.864, 50.178, 49.988, 50.116, 50.282, 50.286, 50.332, 49.888, 50.244, 50.394, 50.004, 50.258, 50.044, 50.228, 50.172, 50.096, 49.98, 50.028, 50.032, 50.114, 50.156, 50.314, 50.172, 50.204, 49.984, 50.22, 50.366, 50.106, 50.26, 50.098, 50.158, 49.948, 50.116, 50.308, 50.09, 50.17, 50.088, 50.458, 49.964, 50.36, 50.314, 50.48, 50.166, 50.246, 50.172, 50.118, 50.3, 50.326, 50.112, 50.056, 50.108, 50.53, 50.368, 50.154, 50.21, 50.228, 50.112, 50.194, 50.152, 50.486, 50.314, 50.346, 50.234, 50.326, 50.322, 50.178, 50.326, 49.906, 50.116, 50.18, 50.124, 49.902, 50.264, 50.16, 50.202, 50.314, 50.066, 50.35, 50.26, 49.996, 50.602, 50.382, 50.016, 50.368, 50.518, 50.208, 50.438, 50.214, 50.132, 49.928, 50.122, 49.994, 50.414, 50.006, 50.252, 50.45, 50.292, 50.34, 50.072, 50.154, 50.072, 50.136, 50.26, 49.798, 50.03, 50.182, 50.15, 50.202, 50.262, 50.096, 49.968, 50.016, 50.016, 50.21]\n",
      "Test Accuracy: 53.45%\n",
      "Losses: [1.941708608356583, 1.7499450222610513, 1.6575708916729979, 1.5999618015630777, 1.5594161626932872, 1.5297828251138672, 1.5061105846444054, 1.4851274612309682, 1.4660121168931732, 1.453443215631158, 1.4311857311926839, 1.4255450756653496, 1.4229178346331468, 1.4181477108879772, 1.4180127214592742, 1.413874257556008, 1.4150915828812154, 1.410595856664126, 1.4079765428972366, 1.4076753467550058, 1.4061384557763024, 1.4051599804397739, 1.4050407623086134, 1.4065419563551997, 1.4076312216346527, 1.4036353719813743, 1.4068446820959106, 1.4032347583404892, 1.4057433943614326, 1.4018111478947008, 1.4014178362039045, 1.403599363763619, 1.39844910110659, 1.4024252922028837, 1.4041224004667434, 1.4009303638087514, 1.400929931789408, 1.400514293204793, 1.4005284894762746, 1.4001443422663853, 1.4012929371860632, 1.4029895356853905, 1.3992312856952247, 1.4018906361001837, 1.3977357935722527, 1.4032551955688946, 1.4025038450270357, 1.4001555064755022, 1.4009471238421662, 1.4026153703479816, 1.4032835856727932, 1.4006977221545052, 1.4027055617793442, 1.4033144136219073, 1.4020655505797441, 1.406037201966776, 1.402571195226801, 1.3967725242800115, 1.40419754073443, 1.4014130733202181, 1.4008977593058516, 1.4043087038542608, 1.4015363628602089, 1.4028467653352585, 1.3998254552826552, 1.401914058134074, 1.4006703406038796, 1.3977783869599443, 1.4018164113964267, 1.4019884200352233, 1.3990815893158584, 1.4007253841975766, 1.4032158156490082, 1.4023036154944573, 1.4006273139773122, 1.399874767683961, 1.4033248610508717, 1.40493652155942, 1.4054183066653474, 1.40440469599136, 1.4005920905286393, 1.402315072086461, 1.4015193421517491, 1.4041541586141757, 1.4013080215819962, 1.3985644514908266, 1.403047409203961, 1.3993554706768612, 1.4013367866920998, 1.399031932091774, 1.401656659057988, 1.40314761756936, 1.402425662026076, 1.401010950203137, 1.3970732518169275, 1.4004744995585487, 1.400180130053664, 1.4033731432522045, 1.3999302170770553, 1.4033928353463292, 1.400272686463183, 1.4021539663719704, 1.4021303970795458, 1.4044804621840377, 1.402458400372654, 1.4052774598226523, 1.4036994962131275, 1.4038815053222735, 1.3989493868235128, 1.4016148815374545, 1.4037228667217752, 1.4004301058361903, 1.4003214796485803, 1.4002981963364973, 1.4012484099249096, 1.4011153504061882, 1.402083161846756, 1.4027070935120058, 1.402256120806155, 1.402763459383679, 1.3999781590288558, 1.403922937715145, 1.4016420896095998, 1.401612296738588, 1.4029451803783017, 1.3970602482481076, 1.4033824897483183, 1.3975918454587306, 1.4044711730059456, 1.4016912675574613, 1.4024497941326912, 1.4023132467513804, 1.404074227413558, 1.398806095123291, 1.403044561290985, 1.4039058569447158, 1.3985192507429196, 1.4029192979378469, 1.4029732787090798, 1.4014598835459755, 1.402249063067424, 1.3999740034722916, 1.398416127695147, 1.4050797803322677, 1.4000893130021936, 1.3993596744049541, 1.3989113288767197, 1.402314004995634, 1.4003028238520903, 1.4017317788985075, 1.4027023833730947, 1.4009183084263521, 1.4040946579345353, 1.40091463306066, 1.3996020643912312, 1.400742518017664, 1.3998843513791213, 1.3999134674096656, 1.3987260465426823, 1.402069447290562, 1.3998342667089398, 1.4029766275449786, 1.4036638849531597, 1.4009177693930428, 1.4034479606486951, 1.402591252570872, 1.4027139149663392, 1.4030845262815275, 1.4030110226262866, 1.4005245205081638, 1.4014086583081413, 1.4043344811100484, 1.4033404436257795, 1.4008561006897247, 1.4006544205233873]\n",
      "Starting run 5/5\n",
      "Run 5, Epoch 1, Loss: 1.912, Training Accuracy: 31.41%\n",
      "Run 5, Epoch 2, Loss: 1.737, Training Accuracy: 37.39%\n",
      "Run 5, Epoch 3, Loss: 1.665, Training Accuracy: 39.71%\n",
      "Run 5, Epoch 4, Loss: 1.615, Training Accuracy: 41.73%\n",
      "Run 5, Epoch 5, Loss: 1.572, Training Accuracy: 43.29%\n",
      "Run 5, Epoch 6, Loss: 1.540, Training Accuracy: 44.42%\n",
      "Run 5, Epoch 7, Loss: 1.510, Training Accuracy: 45.49%\n",
      "Run 5, Epoch 8, Loss: 1.492, Training Accuracy: 46.27%\n",
      "Run 5, Epoch 9, Loss: 1.468, Training Accuracy: 47.29%\n",
      "Run 5, Epoch 10, Loss: 1.448, Training Accuracy: 48.15%\n",
      "Run 5, Epoch 11, Loss: 1.429, Training Accuracy: 49.05%\n",
      "Run 5, Epoch 12, Loss: 1.421, Training Accuracy: 49.30%\n",
      "Run 5, Epoch 13, Loss: 1.419, Training Accuracy: 49.28%\n",
      "Run 5, Epoch 14, Loss: 1.416, Training Accuracy: 49.45%\n",
      "Run 5, Epoch 15, Loss: 1.415, Training Accuracy: 49.49%\n",
      "Run 5, Epoch 16, Loss: 1.413, Training Accuracy: 49.38%\n",
      "Run 5, Epoch 17, Loss: 1.413, Training Accuracy: 49.25%\n",
      "Run 5, Epoch 18, Loss: 1.408, Training Accuracy: 49.78%\n",
      "Run 5, Epoch 19, Loss: 1.410, Training Accuracy: 49.67%\n",
      "Run 5, Epoch 20, Loss: 1.404, Training Accuracy: 49.87%\n",
      "Run 5, Epoch 21, Loss: 1.402, Training Accuracy: 49.88%\n",
      "Run 5, Epoch 22, Loss: 1.399, Training Accuracy: 50.13%\n",
      "Run 5, Epoch 23, Loss: 1.403, Training Accuracy: 49.79%\n",
      "Run 5, Epoch 24, Loss: 1.406, Training Accuracy: 49.93%\n",
      "Run 5, Epoch 25, Loss: 1.407, Training Accuracy: 49.77%\n",
      "Run 5, Epoch 26, Loss: 1.400, Training Accuracy: 50.01%\n",
      "Run 5, Epoch 27, Loss: 1.405, Training Accuracy: 49.99%\n",
      "Run 5, Epoch 28, Loss: 1.404, Training Accuracy: 49.85%\n",
      "Run 5, Epoch 29, Loss: 1.403, Training Accuracy: 49.71%\n",
      "Run 5, Epoch 30, Loss: 1.402, Training Accuracy: 49.84%\n",
      "Run 5, Epoch 31, Loss: 1.404, Training Accuracy: 49.57%\n",
      "Run 5, Epoch 32, Loss: 1.404, Training Accuracy: 49.95%\n",
      "Run 5, Epoch 33, Loss: 1.398, Training Accuracy: 50.25%\n",
      "Run 5, Epoch 34, Loss: 1.404, Training Accuracy: 49.95%\n",
      "Run 5, Epoch 35, Loss: 1.403, Training Accuracy: 50.06%\n",
      "Run 5, Epoch 36, Loss: 1.399, Training Accuracy: 50.32%\n",
      "Run 5, Epoch 37, Loss: 1.404, Training Accuracy: 50.03%\n",
      "Run 5, Epoch 38, Loss: 1.402, Training Accuracy: 50.12%\n",
      "Run 5, Epoch 39, Loss: 1.401, Training Accuracy: 49.90%\n",
      "Run 5, Epoch 40, Loss: 1.403, Training Accuracy: 50.21%\n",
      "Run 5, Epoch 41, Loss: 1.401, Training Accuracy: 49.81%\n",
      "Run 5, Epoch 42, Loss: 1.402, Training Accuracy: 49.77%\n",
      "Run 5, Epoch 43, Loss: 1.403, Training Accuracy: 50.08%\n",
      "Run 5, Epoch 44, Loss: 1.403, Training Accuracy: 49.95%\n",
      "Run 5, Epoch 45, Loss: 1.402, Training Accuracy: 49.88%\n",
      "Run 5, Epoch 46, Loss: 1.401, Training Accuracy: 49.85%\n",
      "Run 5, Epoch 47, Loss: 1.405, Training Accuracy: 50.01%\n",
      "Run 5, Epoch 48, Loss: 1.401, Training Accuracy: 49.91%\n",
      "Run 5, Epoch 49, Loss: 1.404, Training Accuracy: 50.20%\n",
      "Run 5, Epoch 50, Loss: 1.401, Training Accuracy: 50.11%\n",
      "Run 5, Epoch 51, Loss: 1.404, Training Accuracy: 50.05%\n",
      "Run 5, Epoch 52, Loss: 1.402, Training Accuracy: 50.05%\n",
      "Run 5, Epoch 53, Loss: 1.400, Training Accuracy: 50.08%\n",
      "Run 5, Epoch 54, Loss: 1.400, Training Accuracy: 50.21%\n",
      "Run 5, Epoch 55, Loss: 1.402, Training Accuracy: 49.88%\n",
      "Run 5, Epoch 56, Loss: 1.398, Training Accuracy: 49.98%\n",
      "Run 5, Epoch 57, Loss: 1.401, Training Accuracy: 50.00%\n",
      "Run 5, Epoch 58, Loss: 1.403, Training Accuracy: 50.10%\n",
      "Run 5, Epoch 59, Loss: 1.403, Training Accuracy: 49.78%\n",
      "Run 5, Epoch 60, Loss: 1.399, Training Accuracy: 50.06%\n",
      "Run 5, Epoch 61, Loss: 1.399, Training Accuracy: 50.07%\n",
      "Run 5, Epoch 62, Loss: 1.400, Training Accuracy: 50.16%\n",
      "Run 5, Epoch 63, Loss: 1.403, Training Accuracy: 49.87%\n",
      "Run 5, Epoch 64, Loss: 1.403, Training Accuracy: 49.97%\n",
      "Run 5, Epoch 65, Loss: 1.398, Training Accuracy: 50.12%\n",
      "Run 5, Epoch 66, Loss: 1.400, Training Accuracy: 50.11%\n",
      "Run 5, Epoch 67, Loss: 1.399, Training Accuracy: 50.15%\n",
      "Run 5, Epoch 68, Loss: 1.400, Training Accuracy: 50.06%\n",
      "Run 5, Epoch 69, Loss: 1.401, Training Accuracy: 50.14%\n",
      "Run 5, Epoch 70, Loss: 1.398, Training Accuracy: 50.08%\n",
      "Run 5, Epoch 71, Loss: 1.401, Training Accuracy: 49.91%\n",
      "Run 5, Epoch 72, Loss: 1.407, Training Accuracy: 49.66%\n",
      "Run 5, Epoch 73, Loss: 1.401, Training Accuracy: 49.93%\n",
      "Run 5, Epoch 74, Loss: 1.402, Training Accuracy: 50.19%\n",
      "Run 5, Epoch 75, Loss: 1.400, Training Accuracy: 50.23%\n",
      "Run 5, Epoch 76, Loss: 1.406, Training Accuracy: 49.75%\n",
      "Run 5, Epoch 77, Loss: 1.402, Training Accuracy: 50.09%\n",
      "Run 5, Epoch 78, Loss: 1.402, Training Accuracy: 49.90%\n",
      "Run 5, Epoch 79, Loss: 1.398, Training Accuracy: 49.97%\n",
      "Run 5, Epoch 80, Loss: 1.404, Training Accuracy: 49.62%\n",
      "Run 5, Epoch 81, Loss: 1.398, Training Accuracy: 50.06%\n",
      "Run 5, Epoch 82, Loss: 1.403, Training Accuracy: 49.89%\n",
      "Run 5, Epoch 83, Loss: 1.403, Training Accuracy: 49.93%\n",
      "Run 5, Epoch 84, Loss: 1.405, Training Accuracy: 49.72%\n",
      "Run 5, Epoch 85, Loss: 1.403, Training Accuracy: 49.98%\n",
      "Run 5, Epoch 86, Loss: 1.401, Training Accuracy: 50.30%\n",
      "Run 5, Epoch 87, Loss: 1.399, Training Accuracy: 50.19%\n",
      "Run 5, Epoch 88, Loss: 1.402, Training Accuracy: 49.94%\n",
      "Run 5, Epoch 89, Loss: 1.402, Training Accuracy: 50.01%\n",
      "Run 5, Epoch 90, Loss: 1.402, Training Accuracy: 50.01%\n",
      "Run 5, Epoch 91, Loss: 1.401, Training Accuracy: 50.12%\n",
      "Run 5, Epoch 92, Loss: 1.405, Training Accuracy: 49.96%\n",
      "Run 5, Epoch 93, Loss: 1.403, Training Accuracy: 50.02%\n",
      "Run 5, Epoch 94, Loss: 1.402, Training Accuracy: 49.68%\n",
      "Run 5, Epoch 95, Loss: 1.403, Training Accuracy: 50.06%\n",
      "Run 5, Epoch 96, Loss: 1.400, Training Accuracy: 50.31%\n",
      "Run 5, Epoch 97, Loss: 1.402, Training Accuracy: 50.18%\n",
      "Run 5, Epoch 98, Loss: 1.396, Training Accuracy: 50.09%\n",
      "Run 5, Epoch 99, Loss: 1.401, Training Accuracy: 49.97%\n",
      "Run 5, Epoch 100, Loss: 1.397, Training Accuracy: 50.25%\n",
      "Run 5, Epoch 101, Loss: 1.401, Training Accuracy: 49.78%\n",
      "Run 5, Epoch 102, Loss: 1.401, Training Accuracy: 50.13%\n",
      "Run 5, Epoch 103, Loss: 1.400, Training Accuracy: 49.97%\n",
      "Run 5, Epoch 104, Loss: 1.402, Training Accuracy: 49.83%\n",
      "Run 5, Epoch 105, Loss: 1.401, Training Accuracy: 49.99%\n",
      "Run 5, Epoch 106, Loss: 1.401, Training Accuracy: 49.97%\n",
      "Run 5, Epoch 107, Loss: 1.405, Training Accuracy: 49.99%\n",
      "Run 5, Epoch 108, Loss: 1.400, Training Accuracy: 49.96%\n",
      "Run 5, Epoch 109, Loss: 1.405, Training Accuracy: 50.02%\n",
      "Run 5, Epoch 110, Loss: 1.403, Training Accuracy: 50.08%\n",
      "Run 5, Epoch 111, Loss: 1.401, Training Accuracy: 50.05%\n",
      "Run 5, Epoch 112, Loss: 1.400, Training Accuracy: 49.73%\n",
      "Run 5, Epoch 113, Loss: 1.400, Training Accuracy: 49.86%\n",
      "Run 5, Epoch 114, Loss: 1.401, Training Accuracy: 50.02%\n",
      "Run 5, Epoch 115, Loss: 1.399, Training Accuracy: 50.20%\n",
      "Run 5, Epoch 116, Loss: 1.399, Training Accuracy: 50.08%\n",
      "Run 5, Epoch 117, Loss: 1.401, Training Accuracy: 50.21%\n",
      "Run 5, Epoch 118, Loss: 1.402, Training Accuracy: 50.09%\n",
      "Run 5, Epoch 119, Loss: 1.398, Training Accuracy: 50.27%\n",
      "Run 5, Epoch 120, Loss: 1.404, Training Accuracy: 49.74%\n",
      "Run 5, Epoch 121, Loss: 1.400, Training Accuracy: 49.95%\n",
      "Run 5, Epoch 122, Loss: 1.405, Training Accuracy: 49.86%\n",
      "Run 5, Epoch 123, Loss: 1.401, Training Accuracy: 50.19%\n",
      "Run 5, Epoch 124, Loss: 1.403, Training Accuracy: 50.01%\n",
      "Run 5, Epoch 125, Loss: 1.400, Training Accuracy: 50.05%\n",
      "Run 5, Epoch 126, Loss: 1.400, Training Accuracy: 50.03%\n",
      "Run 5, Epoch 127, Loss: 1.403, Training Accuracy: 50.06%\n",
      "Run 5, Epoch 128, Loss: 1.403, Training Accuracy: 49.78%\n",
      "Run 5, Epoch 129, Loss: 1.402, Training Accuracy: 49.90%\n",
      "Run 5, Epoch 130, Loss: 1.401, Training Accuracy: 50.11%\n",
      "Run 5, Epoch 131, Loss: 1.400, Training Accuracy: 50.16%\n",
      "Run 5, Epoch 132, Loss: 1.402, Training Accuracy: 50.09%\n",
      "Run 5, Epoch 133, Loss: 1.397, Training Accuracy: 50.09%\n",
      "Run 5, Epoch 134, Loss: 1.401, Training Accuracy: 49.84%\n",
      "Run 5, Epoch 135, Loss: 1.404, Training Accuracy: 50.04%\n",
      "Run 5, Epoch 136, Loss: 1.401, Training Accuracy: 50.08%\n",
      "Run 5, Epoch 137, Loss: 1.406, Training Accuracy: 50.10%\n",
      "Run 5, Epoch 138, Loss: 1.405, Training Accuracy: 50.08%\n",
      "Run 5, Epoch 139, Loss: 1.401, Training Accuracy: 49.90%\n",
      "Run 5, Epoch 140, Loss: 1.404, Training Accuracy: 49.81%\n",
      "Run 5, Epoch 141, Loss: 1.401, Training Accuracy: 50.03%\n",
      "Run 5, Epoch 142, Loss: 1.404, Training Accuracy: 50.03%\n",
      "Run 5, Epoch 143, Loss: 1.398, Training Accuracy: 50.29%\n",
      "Run 5, Epoch 144, Loss: 1.399, Training Accuracy: 49.83%\n",
      "Run 5, Epoch 145, Loss: 1.400, Training Accuracy: 50.13%\n",
      "Run 5, Epoch 146, Loss: 1.402, Training Accuracy: 49.94%\n",
      "Run 5, Epoch 147, Loss: 1.402, Training Accuracy: 49.96%\n",
      "Run 5, Epoch 148, Loss: 1.402, Training Accuracy: 49.98%\n",
      "Run 5, Epoch 149, Loss: 1.400, Training Accuracy: 50.07%\n",
      "Run 5, Epoch 150, Loss: 1.402, Training Accuracy: 49.78%\n",
      "Run 5, Epoch 151, Loss: 1.402, Training Accuracy: 49.92%\n",
      "Run 5, Epoch 152, Loss: 1.400, Training Accuracy: 50.02%\n",
      "Run 5, Epoch 153, Loss: 1.399, Training Accuracy: 49.87%\n",
      "Run 5, Epoch 154, Loss: 1.398, Training Accuracy: 50.23%\n",
      "Run 5, Epoch 155, Loss: 1.404, Training Accuracy: 49.90%\n",
      "Run 5, Epoch 156, Loss: 1.402, Training Accuracy: 50.17%\n",
      "Run 5, Epoch 157, Loss: 1.404, Training Accuracy: 49.82%\n",
      "Run 5, Epoch 158, Loss: 1.402, Training Accuracy: 50.27%\n",
      "Run 5, Epoch 159, Loss: 1.400, Training Accuracy: 50.14%\n",
      "Run 5, Epoch 160, Loss: 1.401, Training Accuracy: 50.05%\n",
      "Run 5, Epoch 161, Loss: 1.403, Training Accuracy: 49.91%\n",
      "Run 5, Epoch 162, Loss: 1.402, Training Accuracy: 50.08%\n",
      "Run 5, Epoch 163, Loss: 1.401, Training Accuracy: 50.17%\n",
      "Run 5, Epoch 164, Loss: 1.403, Training Accuracy: 49.99%\n",
      "Run 5, Epoch 165, Loss: 1.403, Training Accuracy: 49.88%\n",
      "Run 5, Epoch 166, Loss: 1.406, Training Accuracy: 49.74%\n",
      "Run 5, Epoch 167, Loss: 1.404, Training Accuracy: 49.98%\n",
      "Run 5, Epoch 168, Loss: 1.402, Training Accuracy: 49.81%\n",
      "Run 5, Epoch 169, Loss: 1.403, Training Accuracy: 50.15%\n",
      "Run 5, Epoch 170, Loss: 1.401, Training Accuracy: 49.91%\n",
      "Run 5, Epoch 171, Loss: 1.402, Training Accuracy: 49.90%\n",
      "Run 5, Epoch 172, Loss: 1.406, Training Accuracy: 49.95%\n",
      "Run 5, Epoch 173, Loss: 1.404, Training Accuracy: 49.96%\n",
      "Run 5, Epoch 174, Loss: 1.397, Training Accuracy: 50.05%\n",
      "Run 5, Epoch 175, Loss: 1.405, Training Accuracy: 50.02%\n",
      "Run 5, Final Accuracy on test set: 53.62%\n",
      "Results after run 5:\n",
      "Training Accuracies: [31.41, 37.386, 39.714, 41.728, 43.288, 44.418, 45.492, 46.274, 47.294, 48.15, 49.046, 49.302, 49.278, 49.446, 49.488, 49.376, 49.25, 49.782, 49.674, 49.874, 49.88, 50.126, 49.794, 49.928, 49.766, 50.008, 49.992, 49.854, 49.71, 49.84, 49.574, 49.954, 50.248, 49.954, 50.064, 50.32, 50.028, 50.116, 49.898, 50.206, 49.808, 49.766, 50.076, 49.954, 49.876, 49.848, 50.01, 49.908, 50.2, 50.106, 50.05, 50.046, 50.082, 50.206, 49.878, 49.976, 50.004, 50.1, 49.782, 50.058, 50.068, 50.16, 49.866, 49.968, 50.124, 50.112, 50.146, 50.06, 50.142, 50.084, 49.914, 49.662, 49.934, 50.192, 50.226, 49.752, 50.094, 49.896, 49.968, 49.618, 50.06, 49.892, 49.93, 49.72, 49.984, 50.296, 50.192, 49.944, 50.01, 50.014, 50.124, 49.96, 50.024, 49.682, 50.062, 50.306, 50.176, 50.09, 49.972, 50.252, 49.784, 50.13, 49.97, 49.834, 49.994, 49.97, 49.988, 49.962, 50.016, 50.08, 50.052, 49.734, 49.86, 50.022, 50.196, 50.082, 50.208, 50.094, 50.268, 49.744, 49.954, 49.862, 50.19, 50.006, 50.052, 50.026, 50.062, 49.776, 49.904, 50.114, 50.164, 50.09, 50.09, 49.838, 50.036, 50.076, 50.1, 50.082, 49.904, 49.812, 50.03, 50.032, 50.288, 49.832, 50.126, 49.94, 49.962, 49.982, 50.068, 49.78, 49.916, 50.024, 49.868, 50.234, 49.902, 50.174, 49.816, 50.266, 50.138, 50.046, 49.906, 50.08, 50.17, 49.99, 49.88, 49.742, 49.984, 49.806, 50.152, 49.906, 49.904, 49.946, 49.96, 50.048, 50.02]\n",
      "Test Accuracy: 53.62%\n",
      "Losses: [1.9115015914677964, 1.7370388300522515, 1.6654405993268924, 1.614874797708848, 1.5724130128045826, 1.5404839841910944, 1.509836036530907, 1.4921114530099933, 1.4678260816637512, 1.4478325871250513, 1.428500741948862, 1.4211907716053527, 1.4192701928755815, 1.4159106430800066, 1.4145901815970536, 1.4129847214959772, 1.413031242387679, 1.4083080547849844, 1.4104219705552397, 1.4039266213126804, 1.4018745346142507, 1.399473075976457, 1.4032459847457575, 1.4058141433979239, 1.4067705006855529, 1.3999629377404137, 1.4053450295382448, 1.404070718209152, 1.403450164343695, 1.4023769542079447, 1.4037162413072708, 1.4036654032709655, 1.398283698979546, 1.4044785691649102, 1.4027000040654332, 1.3989223939988313, 1.4037724358346455, 1.402430352042703, 1.400617603755668, 1.402534402544846, 1.4006966660394693, 1.4019933140186398, 1.4028239475796596, 1.4029806224281525, 1.4022209046746763, 1.4010716920618511, 1.4045330648836882, 1.4014762588169263, 1.4036167006358466, 1.4013589181558554, 1.4035371691369645, 1.4022591879300754, 1.399958512667195, 1.4001561747792433, 1.4019265507188294, 1.3981905821949014, 1.4005616848426097, 1.4027698799167447, 1.4029390878994445, 1.3989799763540478, 1.3990713614027213, 1.40035202527595, 1.4034135487988173, 1.403210880506374, 1.3979472418880219, 1.400341379977858, 1.3994089666839755, 1.3999283996689351, 1.4007620466944506, 1.3978072901820893, 1.401301700440819, 1.4066634065354877, 1.401180892649209, 1.4016235801569945, 1.399996248047675, 1.4056256716818456, 1.402046355749945, 1.401762161108539, 1.3984030253442048, 1.4039483045982888, 1.3978879235284714, 1.4033445558889444, 1.4033772296003064, 1.404681232884107, 1.4034955141794345, 1.4008904643680737, 1.3986343898431723, 1.401950524591119, 1.4019698002149381, 1.4020022663009135, 1.4011151988792907, 1.4045719990644918, 1.4033814738778507, 1.4021128108129477, 1.4028849467597044, 1.3995362050697933, 1.4015788486241685, 1.3958903099874707, 1.4011420273719846, 1.3966255913610044, 1.4014765307726458, 1.401340331872711, 1.4000821592252883, 1.4020231367681948, 1.40140679150896, 1.4005455796980797, 1.4047855035118435, 1.4003365448368785, 1.405373391592899, 1.4029475511492366, 1.4013753419032182, 1.4004464899487508, 1.4000585890182144, 1.400791214860004, 1.3992634569592488, 1.39864326956327, 1.4006329473022305, 1.4022377299530733, 1.3982373945548405, 1.4037820994091765, 1.400465397578676, 1.4054650373166175, 1.4012155395639523, 1.4025548318462908, 1.4002368816024506, 1.4000747770909459, 1.4030620844467827, 1.4033137623916196, 1.402412084362391, 1.4008675700868183, 1.4002890266725778, 1.4020320167931755, 1.3969594545071693, 1.4005284163043321, 1.4044287000470759, 1.4012096440395736, 1.4060232718582348, 1.4046963133165598, 1.4008261459257902, 1.403746648517716, 1.4010526371733916, 1.4035124934237937, 1.3984033662035031, 1.3993598125169955, 1.399611208749854, 1.4023092777832695, 1.4023075615963363, 1.4015041501320842, 1.3997212258141365, 1.4019462696426666, 1.4024532371774658, 1.3998887599886531, 1.3994400461616419, 1.3976687016084677, 1.403524363132389, 1.4015404915870608, 1.4037828000305255, 1.401838296819526, 1.400413335131867, 1.400748428176431, 1.403466533821867, 1.4018818338203918, 1.4007089958166528, 1.4033112300326451, 1.4031125218667033, 1.4063591685746333, 1.4043703317032445, 1.4021388956957765, 1.4029669917148093, 1.401101987685084, 1.402482665713181, 1.4057842611961657, 1.403634007019765, 1.3965768375055259, 1.404506900731255]\n",
      "All Training Accuracies over Epochs for each run: [[30.502 36.77  39.562 42.004 43.294 44.472 45.768 46.754 47.574 48.374\n",
      "  49.284 49.642 49.756 49.328 49.82  49.724 49.958 49.988 50.168 50.134\n",
      "  50.304 50.378 50.274 50.358 50.328 50.68  50.33  50.48  50.452 50.628\n",
      "  50.322 50.206 50.214 50.484 50.698 50.282 50.452 50.476 50.234 50.372\n",
      "  50.416 50.732 50.434 50.238 50.468 50.334 50.368 50.334 50.348 50.436\n",
      "  50.284 50.648 50.304 50.43  50.544 50.358 50.652 50.47  50.372 50.362\n",
      "  50.208 50.54  50.302 50.608 50.51  50.606 50.434 50.378 50.534 50.422\n",
      "  50.468 50.296 50.456 50.212 50.278 50.072 50.364 50.444 50.614 50.824\n",
      "  50.458 50.726 50.24  50.386 50.508 50.276 50.384 50.488 50.314 50.706\n",
      "  50.47  50.578 50.3   50.338 50.584 50.326 50.482 50.288 50.52  50.334\n",
      "  50.43  50.51  50.418 50.58  50.332 50.38  50.268 50.322 50.596 50.388\n",
      "  50.554 50.44  50.522 50.638 50.188 50.1   50.39  50.43  50.404 50.158\n",
      "  50.502 50.414 50.542 50.606 50.276 50.33  50.592 50.568 50.244 50.318\n",
      "  50.532 50.412 50.3   50.562 50.452 50.568 50.46  50.68  50.304 50.614\n",
      "  50.274 50.288 50.266 50.584 50.638 50.524 50.474 50.538 50.436 50.39\n",
      "  50.162 50.58  50.534 50.374 50.29  50.378 50.46  50.51  50.196 50.336\n",
      "  50.086 50.346 50.29  50.436 50.606 50.198 50.396 50.492 50.576 50.6\n",
      "  50.494 50.322 50.652 50.514 50.372]\n",
      " [30.308 36.782 39.37  40.834 42.496 43.48  44.846 45.678 46.912 47.486\n",
      "  48.654 49.002 49.226 49.25  49.17  49.278 49.318 49.504 49.896 49.728\n",
      "  49.84  49.85  49.766 49.442 49.688 49.984 49.83  49.58  49.958 49.914\n",
      "  49.87  49.954 49.75  49.78  49.94  49.822 49.914 49.738 50.014 49.688\n",
      "  49.76  49.72  49.964 49.776 50.098 50.062 49.558 49.914 49.918 49.818\n",
      "  49.878 49.656 49.842 50.026 49.72  49.796 49.952 49.914 49.772 49.964\n",
      "  50.136 49.974 50.136 49.76  49.992 49.858 49.874 49.824 49.836 49.97\n",
      "  49.944 50.052 49.78  50.052 49.854 49.976 49.864 49.882 50.012 50.15\n",
      "  49.776 49.896 49.74  49.672 49.854 49.854 49.792 50.152 49.832 49.934\n",
      "  50.008 50.094 50.    49.792 49.926 49.846 49.874 49.412 50.188 49.638\n",
      "  50.054 49.728 49.834 49.68  50.018 49.744 49.86  49.764 49.934 49.994\n",
      "  49.722 49.802 49.946 49.962 49.99  50.108 49.852 50.    49.93  50.048\n",
      "  49.81  49.746 49.738 49.992 50.038 49.802 49.968 49.656 49.998 50.158\n",
      "  49.88  49.994 49.79  49.806 49.878 49.958 49.718 49.936 50.032 49.994\n",
      "  50.188 49.724 50.098 50.048 49.804 49.928 49.884 49.618 50.054 49.86\n",
      "  49.954 49.924 49.722 50.056 49.61  50.228 49.43  49.726 49.788 49.74\n",
      "  50.014 50.072 49.968 49.844 49.724 49.938 49.788 50.    49.838 49.854\n",
      "  49.896 49.7   50.228 49.892 50.016]\n",
      " [31.106 38.01  40.474 42.258 44.092 45.212 46.3   47.094 47.944 49.09\n",
      "  50.162 50.256 50.32  50.22  50.366 50.93  50.846 51.2   51.068 51.032\n",
      "  51.194 51.348 51.062 51.332 51.248 51.474 51.338 51.02  51.28  51.312\n",
      "  50.926 51.326 51.444 51.086 51.138 51.122 51.22  51.374 51.222 51.212\n",
      "  51.39  51.156 51.194 51.406 51.088 51.258 51.324 51.34  51.28  51.23\n",
      "  51.432 51.09  51.266 51.112 51.248 51.222 51.322 51.136 51.364 51.42\n",
      "  51.422 51.446 51.294 51.402 51.226 51.244 51.406 51.142 51.478 51.114\n",
      "  51.352 51.386 51.48  51.41  51.224 51.29  51.31  51.298 51.134 51.402\n",
      "  51.43  51.282 51.456 51.51  51.16  51.274 51.258 51.22  51.526 51.334\n",
      "  51.088 51.162 51.172 51.314 51.074 51.24  51.396 51.256 51.142 51.406\n",
      "  51.4   51.352 51.254 51.306 51.208 51.366 51.322 51.268 51.622 51.388\n",
      "  51.17  51.21  51.194 51.21  51.192 51.336 51.502 51.318 51.268 51.226\n",
      "  51.324 51.306 51.41  51.258 51.436 51.128 51.316 51.294 51.422 51.152\n",
      "  51.238 51.376 50.962 51.166 51.28  51.31  51.546 51.372 51.36  51.184\n",
      "  51.22  51.242 51.316 51.158 51.272 51.26  51.362 51.226 51.042 51.288\n",
      "  51.094 51.16  51.298 51.176 51.316 51.342 51.334 51.026 51.104 51.498\n",
      "  51.378 51.532 51.276 51.206 51.408 51.436 51.134 51.396 51.51  51.408\n",
      "  51.196 51.488 51.208 51.312 51.156]\n",
      " [30.134 37.206 40.292 42.274 43.834 45.22  46.132 46.494 47.416 48.2\n",
      "  49.134 49.392 49.358 49.494 49.522 49.608 49.506 49.894 49.778 49.71\n",
      "  49.93  49.878 49.978 50.036 50.276 50.198 49.98  50.108 49.986 50.208\n",
      "  50.276 49.996 50.41  50.218 50.106 50.334 50.238 50.376 50.17  50.186\n",
      "  50.246 50.196 50.114 50.262 50.388 50.158 50.21  50.204 50.282 50.186\n",
      "  50.086 50.126 50.204 50.02  50.16  50.082 50.31  50.192 49.852 50.282\n",
      "  50.27  49.864 50.178 49.988 50.116 50.282 50.286 50.332 49.888 50.244\n",
      "  50.394 50.004 50.258 50.044 50.228 50.172 50.096 49.98  50.028 50.032\n",
      "  50.114 50.156 50.314 50.172 50.204 49.984 50.22  50.366 50.106 50.26\n",
      "  50.098 50.158 49.948 50.116 50.308 50.09  50.17  50.088 50.458 49.964\n",
      "  50.36  50.314 50.48  50.166 50.246 50.172 50.118 50.3   50.326 50.112\n",
      "  50.056 50.108 50.53  50.368 50.154 50.21  50.228 50.112 50.194 50.152\n",
      "  50.486 50.314 50.346 50.234 50.326 50.322 50.178 50.326 49.906 50.116\n",
      "  50.18  50.124 49.902 50.264 50.16  50.202 50.314 50.066 50.35  50.26\n",
      "  49.996 50.602 50.382 50.016 50.368 50.518 50.208 50.438 50.214 50.132\n",
      "  49.928 50.122 49.994 50.414 50.006 50.252 50.45  50.292 50.34  50.072\n",
      "  50.154 50.072 50.136 50.26  49.798 50.03  50.182 50.15  50.202 50.262\n",
      "  50.096 49.968 50.016 50.016 50.21 ]\n",
      " [31.41  37.386 39.714 41.728 43.288 44.418 45.492 46.274 47.294 48.15\n",
      "  49.046 49.302 49.278 49.446 49.488 49.376 49.25  49.782 49.674 49.874\n",
      "  49.88  50.126 49.794 49.928 49.766 50.008 49.992 49.854 49.71  49.84\n",
      "  49.574 49.954 50.248 49.954 50.064 50.32  50.028 50.116 49.898 50.206\n",
      "  49.808 49.766 50.076 49.954 49.876 49.848 50.01  49.908 50.2   50.106\n",
      "  50.05  50.046 50.082 50.206 49.878 49.976 50.004 50.1   49.782 50.058\n",
      "  50.068 50.16  49.866 49.968 50.124 50.112 50.146 50.06  50.142 50.084\n",
      "  49.914 49.662 49.934 50.192 50.226 49.752 50.094 49.896 49.968 49.618\n",
      "  50.06  49.892 49.93  49.72  49.984 50.296 50.192 49.944 50.01  50.014\n",
      "  50.124 49.96  50.024 49.682 50.062 50.306 50.176 50.09  49.972 50.252\n",
      "  49.784 50.13  49.97  49.834 49.994 49.97  49.988 49.962 50.016 50.08\n",
      "  50.052 49.734 49.86  50.022 50.196 50.082 50.208 50.094 50.268 49.744\n",
      "  49.954 49.862 50.19  50.006 50.052 50.026 50.062 49.776 49.904 50.114\n",
      "  50.164 50.09  50.09  49.838 50.036 50.076 50.1   50.082 49.904 49.812\n",
      "  50.03  50.032 50.288 49.832 50.126 49.94  49.962 49.982 50.068 49.78\n",
      "  49.916 50.024 49.868 50.234 49.902 50.174 49.816 50.266 50.138 50.046\n",
      "  49.906 50.08  50.17  49.99  49.88  49.742 49.984 49.806 50.152 49.906\n",
      "  49.904 49.946 49.96  50.048 50.02 ]]\n",
      "All Test Accuracies after each run: [[52.92]\n",
      " [53.02]\n",
      " [54.98]\n",
      " [53.45]\n",
      " [53.62]]\n",
      "All Losses over Epochs for each run: [[1.94484288 1.77146384 1.67835329 1.62091344 1.57813363 1.54505098\n",
      "  1.50930657 1.48717564 1.46519468 1.44732776 1.42665259 1.4191583\n",
      "  1.41492226 1.4162307  1.41308283 1.40902629 1.40720483 1.40698104\n",
      "  1.40081154 1.40179763 1.39668572 1.39576043 1.3976325  1.39689369\n",
      "  1.3986713  1.39784672 1.39820549 1.39557369 1.39656775 1.3949345\n",
      "  1.39623099 1.39876845 1.39725206 1.39243947 1.39216913 1.39588637\n",
      "  1.39414863 1.39558011 1.39671759 1.39788664 1.39387746 1.39434958\n",
      "  1.39225815 1.3982685  1.39943847 1.3961559  1.39756068 1.39620893\n",
      "  1.39462287 1.39559583 1.39740198 1.3905646  1.39660487 1.39449682\n",
      "  1.39499603 1.39414752 1.39662559 1.3956719  1.3928608  1.39529444\n",
      "  1.39663578 1.39520879 1.39556912 1.39287272 1.39446089 1.39248209\n",
      "  1.39423446 1.3950819  1.39205726 1.39718853 1.39628655 1.39432314\n",
      "  1.3964896  1.39524395 1.3965037  1.39742895 1.39468545 1.39531945\n",
      "  1.39231407 1.39164347 1.39589065 1.39365007 1.39580964 1.3938346\n",
      "  1.39726587 1.39495703 1.39558257 1.3970907  1.39564656 1.39079906\n",
      "  1.39544667 1.397382   1.39915771 1.39587635 1.39642996 1.39631816\n",
      "  1.39739491 1.39529393 1.39559669 1.39816528 1.39780528 1.39667438\n",
      "  1.39666012 1.39375947 1.39454689 1.39714311 1.39752029 1.39709713\n",
      "  1.39327803 1.39427485 1.39451447 1.39738181 1.39489977 1.39591196\n",
      "  1.3958872  1.39902656 1.39380826 1.39588618 1.39717469 1.39586413\n",
      "  1.39453249 1.39457484 1.39377379 1.3952131  1.39757488 1.39696277\n",
      "  1.39655595 1.39859606 1.395071   1.39414978 1.3960063  1.39878572\n",
      "  1.39554497 1.39284242 1.39560484 1.39457135 1.39527557 1.3952727\n",
      "  1.39666008 1.39730437 1.3942773  1.39628379 1.3986319  1.39700444\n",
      "  1.3964166  1.39369225 1.39415723 1.39482885 1.39646293 1.39545345\n",
      "  1.39693902 1.39440231 1.39614825 1.39598374 1.39390983 1.39738083\n",
      "  1.39670538 1.39548544 1.39473533 1.39617377 1.39925376 1.39632619\n",
      "  1.39790497 1.39601856 1.39370472 1.39703156 1.39722051 1.39470037\n",
      "  1.39093442 1.39654859 1.39483079 1.40174736 1.39208246 1.39513227\n",
      "  1.39725179]\n",
      " [1.93794172 1.76941952 1.69679678 1.65059789 1.6074482  1.57365591\n",
      "  1.54088597 1.51433733 1.48442231 1.46485225 1.44215967 1.43551131\n",
      "  1.43062863 1.42974222 1.42707453 1.42482878 1.42415036 1.42090723\n",
      "  1.41665757 1.41791162 1.41636416 1.41233952 1.41556911 1.41657456\n",
      "  1.41458054 1.41159805 1.41253226 1.41087599 1.41265685 1.41081968\n",
      "  1.41123587 1.40947667 1.41007076 1.41101413 1.41257643 1.41124224\n",
      "  1.40921492 1.41163383 1.41228423 1.4089519  1.41153016 1.41259413\n",
      "  1.40979082 1.41080143 1.40769237 1.4111429  1.41281206 1.41240741\n",
      "  1.40919628 1.4127229  1.41011915 1.41116062 1.4073327  1.40679572\n",
      "  1.41157969 1.41257093 1.41135394 1.41008384 1.4096664  1.40988075\n",
      "  1.4092943  1.409164   1.40908339 1.41408522 1.40848693 1.41188184\n",
      "  1.4093243  1.40988824 1.40853275 1.41113023 1.41413706 1.40911995\n",
      "  1.41191944 1.40765937 1.41415424 1.40931891 1.41121856 1.40898864\n",
      "  1.41027045 1.40963004 1.41276765 1.41085572 1.41135425 1.41372876\n",
      "  1.4120018  1.41066241 1.41439359 1.40955831 1.41039528 1.41029197\n",
      "  1.41331091 1.40772248 1.41206859 1.41061201 1.41261434 1.41297538\n",
      "  1.41124651 1.4102018  1.40834112 1.4103533  1.40872385 1.4165479\n",
      "  1.41366522 1.41341492 1.40911553 1.41300159 1.40936859 1.41190316\n",
      "  1.41181972 1.40930923 1.40864021 1.41112219 1.41162981 1.4096784\n",
      "  1.41099824 1.41021079 1.40938269 1.41238935 1.41077337 1.41048438\n",
      "  1.41419542 1.41140482 1.40752314 1.41017892 1.41081948 1.41163504\n",
      "  1.41142367 1.41586927 1.40908871 1.41019466 1.40746296 1.40741359\n",
      "  1.41130786 1.41476802 1.4124493  1.41100767 1.41347221 1.4076663\n",
      "  1.40913779 1.4111285  1.40886389 1.41292308 1.41132946 1.40869289\n",
      "  1.41200621 1.40890499 1.40963499 1.41191058 1.40805901 1.40888228\n",
      "  1.412794   1.41002154 1.41404634 1.40802548 1.41311908 1.40933382\n",
      "  1.41421536 1.41332938 1.40884986 1.41055157 1.41091147 1.41270768\n",
      "  1.41094807 1.41050171 1.41086448 1.4123343  1.41379478 1.40790763\n",
      "  1.41253662 1.41021377 1.41273774 1.41141669 1.40404845 1.41263728\n",
      "  1.40881487]\n",
      " [1.91652822 1.73748217 1.66040496 1.60698059 1.56035429 1.52551525\n",
      "  1.50001246 1.47087758 1.45295789 1.42764557 1.40504214 1.39892829\n",
      "  1.39761214 1.39479798 1.39140522 1.38896757 1.38615518 1.38258742\n",
      "  1.38086574 1.37701435 1.37625557 1.37422861 1.38008374 1.37264509\n",
      "  1.37528652 1.37353125 1.37294946 1.3769978  1.37297205 1.37373702\n",
      "  1.3767339  1.37277372 1.37089265 1.3753463  1.37423283 1.37447103\n",
      "  1.37722155 1.37511759 1.37492546 1.37137372 1.37292945 1.37299563\n",
      "  1.37390173 1.37077413 1.37346611 1.37170402 1.37351193 1.37279781\n",
      "  1.37469969 1.37440853 1.37190618 1.37483687 1.37396534 1.37376916\n",
      "  1.37365627 1.37674749 1.37561372 1.3738804  1.3728247  1.37619788\n",
      "  1.37069198 1.37242913 1.37320043 1.37210391 1.37651765 1.37432379\n",
      "  1.37044969 1.37477864 1.37026678 1.37360655 1.37557596 1.37395445\n",
      "  1.37064108 1.37492506 1.37575556 1.3714632  1.37356189 1.37211452\n",
      "  1.3775921  1.36804877 1.37130124 1.37192336 1.37298197 1.37455205\n",
      "  1.37384354 1.37106959 1.37464692 1.37679589 1.37171665 1.37351282\n",
      "  1.37511983 1.37248444 1.37518555 1.37281428 1.37334029 1.37462711\n",
      "  1.37233316 1.37168657 1.37471515 1.37605122 1.37251214 1.37000079\n",
      "  1.37359443 1.37157927 1.37621952 1.37099393 1.37557414 1.372922\n",
      "  1.37121894 1.37060962 1.37504727 1.37692508 1.37456708 1.37681478\n",
      "  1.37587779 1.37487195 1.37444215 1.37149589 1.37164242 1.37198003\n",
      "  1.37522201 1.37283929 1.37295126 1.37202125 1.37191773 1.37430417\n",
      "  1.37347485 1.37538818 1.37430294 1.37537312 1.37315468 1.37189426\n",
      "  1.37912667 1.37673141 1.37324307 1.37300366 1.37219781 1.37201446\n",
      "  1.37688456 1.37468211 1.37545509 1.37131187 1.3733078  1.37477688\n",
      "  1.37352848 1.37416272 1.37485301 1.37305382 1.37501777 1.37367685\n",
      "  1.37613709 1.37309464 1.37318845 1.3761967  1.37283011 1.37281275\n",
      "  1.37068781 1.37447556 1.37616509 1.37184411 1.37177675 1.37135775\n",
      "  1.3739407  1.37428143 1.37380308 1.37364309 1.37436356 1.37212493\n",
      "  1.37532495 1.37251794 1.37475541 1.37526098 1.37577191 1.37364125\n",
      "  1.37298458]\n",
      " [1.94170861 1.74994502 1.65757089 1.5999618  1.55941616 1.52978283\n",
      "  1.50611058 1.48512746 1.46601212 1.45344322 1.43118573 1.42554508\n",
      "  1.42291783 1.41814771 1.41801272 1.41387426 1.41509158 1.41059586\n",
      "  1.40797654 1.40767535 1.40613846 1.40515998 1.40504076 1.40654196\n",
      "  1.40763122 1.40363537 1.40684468 1.40323476 1.40574339 1.40181115\n",
      "  1.40141784 1.40359936 1.3984491  1.40242529 1.4041224  1.40093036\n",
      "  1.40092993 1.40051429 1.40052849 1.40014434 1.40129294 1.40298954\n",
      "  1.39923129 1.40189064 1.39773579 1.4032552  1.40250385 1.40015551\n",
      "  1.40094712 1.40261537 1.40328359 1.40069772 1.40270556 1.40331441\n",
      "  1.40206555 1.4060372  1.4025712  1.39677252 1.40419754 1.40141307\n",
      "  1.40089776 1.4043087  1.40153636 1.40284677 1.39982546 1.40191406\n",
      "  1.40067034 1.39777839 1.40181641 1.40198842 1.39908159 1.40072538\n",
      "  1.40321582 1.40230362 1.40062731 1.39987477 1.40332486 1.40493652\n",
      "  1.40541831 1.4044047  1.40059209 1.40231507 1.40151934 1.40415416\n",
      "  1.40130802 1.39856445 1.40304741 1.39935547 1.40133679 1.39903193\n",
      "  1.40165666 1.40314762 1.40242566 1.40101095 1.39707325 1.4004745\n",
      "  1.40018013 1.40337314 1.39993022 1.40339284 1.40027269 1.40215397\n",
      "  1.4021304  1.40448046 1.4024584  1.40527746 1.4036995  1.40388151\n",
      "  1.39894939 1.40161488 1.40372287 1.40043011 1.40032148 1.4002982\n",
      "  1.40124841 1.40111535 1.40208316 1.40270709 1.40225612 1.40276346\n",
      "  1.39997816 1.40392294 1.40164209 1.4016123  1.40294518 1.39706025\n",
      "  1.40338249 1.39759185 1.40447117 1.40169127 1.40244979 1.40231325\n",
      "  1.40407423 1.3988061  1.40304456 1.40390586 1.39851925 1.4029193\n",
      "  1.40297328 1.40145988 1.40224906 1.399974   1.39841613 1.40507978\n",
      "  1.40008931 1.39935967 1.39891133 1.402314   1.40030282 1.40173178\n",
      "  1.40270238 1.40091831 1.40409466 1.40091463 1.39960206 1.40074252\n",
      "  1.39988435 1.39991347 1.39872605 1.40206945 1.39983427 1.40297663\n",
      "  1.40366388 1.40091777 1.40344796 1.40259125 1.40271391 1.40308453\n",
      "  1.40301102 1.40052452 1.40140866 1.40433448 1.40334044 1.4008561\n",
      "  1.40065442]\n",
      " [1.91150159 1.73703883 1.6654406  1.6148748  1.57241301 1.54048398\n",
      "  1.50983604 1.49211145 1.46782608 1.44783259 1.42850074 1.42119077\n",
      "  1.41927019 1.41591064 1.41459018 1.41298472 1.41303124 1.40830805\n",
      "  1.41042197 1.40392662 1.40187453 1.39947308 1.40324598 1.40581414\n",
      "  1.4067705  1.39996294 1.40534503 1.40407072 1.40345016 1.40237695\n",
      "  1.40371624 1.4036654  1.3982837  1.40447857 1.4027     1.39892239\n",
      "  1.40377244 1.40243035 1.4006176  1.4025344  1.40069667 1.40199331\n",
      "  1.40282395 1.40298062 1.4022209  1.40107169 1.40453306 1.40147626\n",
      "  1.4036167  1.40135892 1.40353717 1.40225919 1.39995851 1.40015617\n",
      "  1.40192655 1.39819058 1.40056168 1.40276988 1.40293909 1.39897998\n",
      "  1.39907136 1.40035203 1.40341355 1.40321088 1.39794724 1.40034138\n",
      "  1.39940897 1.3999284  1.40076205 1.39780729 1.4013017  1.40666341\n",
      "  1.40118089 1.40162358 1.39999625 1.40562567 1.40204636 1.40176216\n",
      "  1.39840303 1.4039483  1.39788792 1.40334456 1.40337723 1.40468123\n",
      "  1.40349551 1.40089046 1.39863439 1.40195052 1.4019698  1.40200227\n",
      "  1.4011152  1.404572   1.40338147 1.40211281 1.40288495 1.39953621\n",
      "  1.40157885 1.39589031 1.40114203 1.39662559 1.40147653 1.40134033\n",
      "  1.40008216 1.40202314 1.40140679 1.40054558 1.4047855  1.40033654\n",
      "  1.40537339 1.40294755 1.40137534 1.40044649 1.40005859 1.40079121\n",
      "  1.39926346 1.39864327 1.40063295 1.40223773 1.39823739 1.4037821\n",
      "  1.4004654  1.40546504 1.40121554 1.40255483 1.40023688 1.40007478\n",
      "  1.40306208 1.40331376 1.40241208 1.40086757 1.40028903 1.40203202\n",
      "  1.39695945 1.40052842 1.4044287  1.40120964 1.40602327 1.40469631\n",
      "  1.40082615 1.40374665 1.40105264 1.40351249 1.39840337 1.39935981\n",
      "  1.39961121 1.40230928 1.40230756 1.40150415 1.39972123 1.40194627\n",
      "  1.40245324 1.39988876 1.39944005 1.3976687  1.40352436 1.40154049\n",
      "  1.4037828  1.4018383  1.40041334 1.40074843 1.40346653 1.40188183\n",
      "  1.400709   1.40331123 1.40311252 1.40635917 1.40437033 1.4021389\n",
      "  1.40296699 1.40110199 1.40248267 1.40578426 1.40363401 1.39657684\n",
      "  1.4045069 ]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                    download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "num_epochs = 175\n",
    "num_runs = 5\n",
    "\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "all_losses = []\n",
    "\n",
    "def save_checkpoint(run, model, optimizer, scheduler, train_accuracies, losses):\n",
    "    checkpoint = {\n",
    "        'run': run,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'losses': losses\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth'))\n",
    "\n",
    "def load_checkpoint(run):\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth'))\n",
    "    return checkpoint\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Starting run {run + 1}/{num_runs}\")\n",
    "\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    if os.path.exists(os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth')):\n",
    "        print(f\"Loading checkpoint for run {run + 1}\")\n",
    "        checkpoint = load_checkpoint(run)\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        train_accuracies = checkpoint['train_accuracies']\n",
    "        losses = checkpoint['losses']\n",
    "    else:\n",
    "        train_accuracies = []\n",
    "        losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        losses.append(running_loss / len(trainloader))\n",
    "        print(f\"Run {run+1}, Epoch {epoch+1}, Loss: {running_loss / len(trainloader):.3f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        scheduler.step()\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f\"Run {run+1}, Final Accuracy on test set: {test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    all_train_accuracies.append(train_accuracies)\n",
    "    all_test_accuracies.append(test_accuracies)\n",
    "    all_losses.append(losses)\n",
    "\n",
    "    save_checkpoint(run, net, optimizer, scheduler, train_accuracies, losses)\n",
    "\n",
    "    print(f\"Results after run {run + 1}:\")\n",
    "    print(f\"Training Accuracies: {train_accuracies}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"Losses: {losses}\")\n",
    "\n",
    "all_train_accuracies = np.array(all_train_accuracies)\n",
    "all_test_accuracies = np.array(all_test_accuracies)\n",
    "all_losses = np.array(all_losses)\n",
    "\n",
    "np.savetxt('train_accuracies.txt', all_train_accuracies)\n",
    "np.savetxt('test_accuracies.txt', all_test_accuracies)\n",
    "np.savetxt('losses.txt', all_losses)\n",
    "\n",
    "print(\"All Training Accuracies over Epochs for each run:\", all_train_accuracies)\n",
    "print(\"All Test Accuracies after each run:\", all_test_accuracies)\n",
    "print(\"All Losses over Epochs for each run:\", all_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26033.243295,
   "end_time": "2024-10-23T19:35:01.565358",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T12:21:08.322063",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
