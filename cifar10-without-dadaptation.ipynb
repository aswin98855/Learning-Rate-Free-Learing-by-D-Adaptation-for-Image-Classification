{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70f118a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T15:39:48.221412Z",
     "iopub.status.busy": "2024-08-29T15:39:48.221069Z",
     "iopub.status.idle": "2024-08-29T22:59:53.933731Z",
     "shell.execute_reply": "2024-08-29T22:59:53.932423Z"
    },
    "papermill": {
     "duration": 26405.719196,
     "end_time": "2024-08-29T22:59:53.936364",
     "exception": false,
     "start_time": "2024-08-29T15:39:48.217168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:11<00:00, 14291825.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Starting run 1/5\n",
      "Run 1, Epoch 1, Loss: 1.682, Training Accuracy: 38.39%\n",
      "Run 1, Epoch 2, Loss: 1.421, Training Accuracy: 48.54%\n",
      "Run 1, Epoch 3, Loss: 1.316, Training Accuracy: 52.71%\n",
      "Run 1, Epoch 4, Loss: 1.224, Training Accuracy: 56.30%\n",
      "Run 1, Epoch 5, Loss: 1.167, Training Accuracy: 58.29%\n",
      "Run 1, Epoch 6, Loss: 1.113, Training Accuracy: 60.35%\n",
      "Run 1, Epoch 7, Loss: 1.073, Training Accuracy: 62.02%\n",
      "Run 1, Epoch 8, Loss: 1.030, Training Accuracy: 63.19%\n",
      "Run 1, Epoch 9, Loss: 0.997, Training Accuracy: 64.46%\n",
      "Run 1, Epoch 10, Loss: 0.979, Training Accuracy: 65.27%\n",
      "Run 1, Epoch 11, Loss: 0.913, Training Accuracy: 67.69%\n",
      "Run 1, Epoch 12, Loss: 0.902, Training Accuracy: 68.23%\n",
      "Run 1, Epoch 13, Loss: 0.891, Training Accuracy: 68.81%\n",
      "Run 1, Epoch 14, Loss: 0.889, Training Accuracy: 68.79%\n",
      "Run 1, Epoch 15, Loss: 0.880, Training Accuracy: 68.84%\n",
      "Run 1, Epoch 16, Loss: 0.881, Training Accuracy: 69.01%\n",
      "Run 1, Epoch 17, Loss: 0.878, Training Accuracy: 68.99%\n",
      "Run 1, Epoch 18, Loss: 0.873, Training Accuracy: 69.16%\n",
      "Run 1, Epoch 19, Loss: 0.870, Training Accuracy: 69.38%\n",
      "Run 1, Epoch 20, Loss: 0.863, Training Accuracy: 69.68%\n",
      "Run 1, Epoch 21, Loss: 0.858, Training Accuracy: 69.75%\n",
      "Run 1, Epoch 22, Loss: 0.857, Training Accuracy: 69.84%\n",
      "Run 1, Epoch 23, Loss: 0.857, Training Accuracy: 69.77%\n",
      "Run 1, Epoch 24, Loss: 0.853, Training Accuracy: 70.19%\n",
      "Run 1, Epoch 25, Loss: 0.855, Training Accuracy: 70.12%\n",
      "Run 1, Epoch 26, Loss: 0.851, Training Accuracy: 70.07%\n",
      "Run 1, Epoch 27, Loss: 0.854, Training Accuracy: 69.97%\n",
      "Run 1, Epoch 28, Loss: 0.852, Training Accuracy: 69.92%\n",
      "Run 1, Epoch 29, Loss: 0.852, Training Accuracy: 69.87%\n",
      "Run 1, Epoch 30, Loss: 0.851, Training Accuracy: 70.04%\n",
      "Run 1, Epoch 31, Loss: 0.850, Training Accuracy: 70.15%\n",
      "Run 1, Epoch 32, Loss: 0.851, Training Accuracy: 70.04%\n",
      "Run 1, Epoch 33, Loss: 0.857, Training Accuracy: 69.80%\n",
      "Run 1, Epoch 34, Loss: 0.850, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 35, Loss: 0.852, Training Accuracy: 69.77%\n",
      "Run 1, Epoch 36, Loss: 0.853, Training Accuracy: 70.10%\n",
      "Run 1, Epoch 37, Loss: 0.845, Training Accuracy: 70.24%\n",
      "Run 1, Epoch 38, Loss: 0.856, Training Accuracy: 69.89%\n",
      "Run 1, Epoch 39, Loss: 0.848, Training Accuracy: 70.27%\n",
      "Run 1, Epoch 40, Loss: 0.847, Training Accuracy: 70.34%\n",
      "Run 1, Epoch 41, Loss: 0.858, Training Accuracy: 69.81%\n",
      "Run 1, Epoch 42, Loss: 0.848, Training Accuracy: 70.07%\n",
      "Run 1, Epoch 43, Loss: 0.854, Training Accuracy: 69.98%\n",
      "Run 1, Epoch 44, Loss: 0.851, Training Accuracy: 70.08%\n",
      "Run 1, Epoch 45, Loss: 0.850, Training Accuracy: 70.12%\n",
      "Run 1, Epoch 46, Loss: 0.851, Training Accuracy: 69.90%\n",
      "Run 1, Epoch 47, Loss: 0.851, Training Accuracy: 70.05%\n",
      "Run 1, Epoch 48, Loss: 0.853, Training Accuracy: 69.82%\n",
      "Run 1, Epoch 49, Loss: 0.853, Training Accuracy: 69.85%\n",
      "Run 1, Epoch 50, Loss: 0.848, Training Accuracy: 70.25%\n",
      "Run 1, Epoch 51, Loss: 0.852, Training Accuracy: 70.17%\n",
      "Run 1, Epoch 52, Loss: 0.850, Training Accuracy: 69.96%\n",
      "Run 1, Epoch 53, Loss: 0.854, Training Accuracy: 70.04%\n",
      "Run 1, Epoch 54, Loss: 0.848, Training Accuracy: 70.08%\n",
      "Run 1, Epoch 55, Loss: 0.850, Training Accuracy: 69.83%\n",
      "Run 1, Epoch 56, Loss: 0.847, Training Accuracy: 70.09%\n",
      "Run 1, Epoch 57, Loss: 0.852, Training Accuracy: 69.96%\n",
      "Run 1, Epoch 58, Loss: 0.849, Training Accuracy: 70.08%\n",
      "Run 1, Epoch 59, Loss: 0.851, Training Accuracy: 69.84%\n",
      "Run 1, Epoch 60, Loss: 0.850, Training Accuracy: 70.23%\n",
      "Run 1, Epoch 61, Loss: 0.849, Training Accuracy: 69.98%\n",
      "Run 1, Epoch 62, Loss: 0.853, Training Accuracy: 70.07%\n",
      "Run 1, Epoch 63, Loss: 0.852, Training Accuracy: 69.85%\n",
      "Run 1, Epoch 64, Loss: 0.849, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 65, Loss: 0.855, Training Accuracy: 70.00%\n",
      "Run 1, Epoch 66, Loss: 0.847, Training Accuracy: 70.22%\n",
      "Run 1, Epoch 67, Loss: 0.856, Training Accuracy: 69.66%\n",
      "Run 1, Epoch 68, Loss: 0.855, Training Accuracy: 69.71%\n",
      "Run 1, Epoch 69, Loss: 0.851, Training Accuracy: 69.91%\n",
      "Run 1, Epoch 70, Loss: 0.851, Training Accuracy: 69.95%\n",
      "Run 1, Epoch 71, Loss: 0.852, Training Accuracy: 70.19%\n",
      "Run 1, Epoch 72, Loss: 0.850, Training Accuracy: 70.19%\n",
      "Run 1, Epoch 73, Loss: 0.851, Training Accuracy: 70.14%\n",
      "Run 1, Epoch 74, Loss: 0.848, Training Accuracy: 70.39%\n",
      "Run 1, Epoch 75, Loss: 0.852, Training Accuracy: 70.11%\n",
      "Run 1, Epoch 76, Loss: 0.855, Training Accuracy: 69.90%\n",
      "Run 1, Epoch 77, Loss: 0.852, Training Accuracy: 69.95%\n",
      "Run 1, Epoch 78, Loss: 0.851, Training Accuracy: 69.79%\n",
      "Run 1, Epoch 79, Loss: 0.852, Training Accuracy: 69.98%\n",
      "Run 1, Epoch 80, Loss: 0.853, Training Accuracy: 69.93%\n",
      "Run 1, Epoch 81, Loss: 0.849, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 82, Loss: 0.855, Training Accuracy: 69.74%\n",
      "Run 1, Epoch 83, Loss: 0.853, Training Accuracy: 69.99%\n",
      "Run 1, Epoch 84, Loss: 0.849, Training Accuracy: 70.05%\n",
      "Run 1, Epoch 85, Loss: 0.856, Training Accuracy: 69.94%\n",
      "Run 1, Epoch 86, Loss: 0.852, Training Accuracy: 69.91%\n",
      "Run 1, Epoch 87, Loss: 0.851, Training Accuracy: 69.86%\n",
      "Run 1, Epoch 88, Loss: 0.850, Training Accuracy: 69.84%\n",
      "Run 1, Epoch 89, Loss: 0.853, Training Accuracy: 70.04%\n",
      "Run 1, Epoch 90, Loss: 0.852, Training Accuracy: 70.25%\n",
      "Run 1, Epoch 91, Loss: 0.850, Training Accuracy: 70.10%\n",
      "Run 1, Epoch 92, Loss: 0.852, Training Accuracy: 70.17%\n",
      "Run 1, Epoch 93, Loss: 0.851, Training Accuracy: 70.13%\n",
      "Run 1, Epoch 94, Loss: 0.852, Training Accuracy: 70.18%\n",
      "Run 1, Epoch 95, Loss: 0.851, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 96, Loss: 0.848, Training Accuracy: 70.22%\n",
      "Run 1, Epoch 97, Loss: 0.848, Training Accuracy: 70.15%\n",
      "Run 1, Epoch 98, Loss: 0.850, Training Accuracy: 70.03%\n",
      "Run 1, Epoch 99, Loss: 0.849, Training Accuracy: 70.13%\n",
      "Run 1, Epoch 100, Loss: 0.849, Training Accuracy: 70.27%\n",
      "Run 1, Epoch 101, Loss: 0.851, Training Accuracy: 69.94%\n",
      "Run 1, Epoch 102, Loss: 0.850, Training Accuracy: 70.06%\n",
      "Run 1, Epoch 103, Loss: 0.851, Training Accuracy: 70.06%\n",
      "Run 1, Epoch 104, Loss: 0.853, Training Accuracy: 70.09%\n",
      "Run 1, Epoch 105, Loss: 0.848, Training Accuracy: 70.09%\n",
      "Run 1, Epoch 106, Loss: 0.847, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 107, Loss: 0.850, Training Accuracy: 70.13%\n",
      "Run 1, Epoch 108, Loss: 0.850, Training Accuracy: 70.13%\n",
      "Run 1, Epoch 109, Loss: 0.845, Training Accuracy: 70.04%\n",
      "Run 1, Epoch 110, Loss: 0.850, Training Accuracy: 70.12%\n",
      "Run 1, Epoch 111, Loss: 0.849, Training Accuracy: 70.06%\n",
      "Run 1, Epoch 112, Loss: 0.848, Training Accuracy: 70.13%\n",
      "Run 1, Epoch 113, Loss: 0.853, Training Accuracy: 70.08%\n",
      "Run 1, Epoch 114, Loss: 0.848, Training Accuracy: 70.14%\n",
      "Run 1, Epoch 115, Loss: 0.854, Training Accuracy: 69.91%\n",
      "Run 1, Epoch 116, Loss: 0.856, Training Accuracy: 69.91%\n",
      "Run 1, Epoch 117, Loss: 0.851, Training Accuracy: 70.23%\n",
      "Run 1, Epoch 118, Loss: 0.852, Training Accuracy: 70.07%\n",
      "Run 1, Epoch 119, Loss: 0.852, Training Accuracy: 70.08%\n",
      "Run 1, Epoch 120, Loss: 0.855, Training Accuracy: 69.81%\n",
      "Run 1, Epoch 121, Loss: 0.848, Training Accuracy: 70.41%\n",
      "Run 1, Epoch 122, Loss: 0.848, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 123, Loss: 0.853, Training Accuracy: 69.84%\n",
      "Run 1, Epoch 124, Loss: 0.849, Training Accuracy: 69.99%\n",
      "Run 1, Epoch 125, Loss: 0.852, Training Accuracy: 70.08%\n",
      "Run 1, Epoch 126, Loss: 0.851, Training Accuracy: 69.96%\n",
      "Run 1, Epoch 127, Loss: 0.849, Training Accuracy: 70.13%\n",
      "Run 1, Epoch 128, Loss: 0.852, Training Accuracy: 69.98%\n",
      "Run 1, Epoch 129, Loss: 0.850, Training Accuracy: 70.19%\n",
      "Run 1, Epoch 130, Loss: 0.851, Training Accuracy: 70.35%\n",
      "Run 1, Epoch 131, Loss: 0.850, Training Accuracy: 70.11%\n",
      "Run 1, Epoch 132, Loss: 0.848, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 133, Loss: 0.852, Training Accuracy: 70.09%\n",
      "Run 1, Epoch 134, Loss: 0.851, Training Accuracy: 69.98%\n",
      "Run 1, Epoch 135, Loss: 0.850, Training Accuracy: 70.10%\n",
      "Run 1, Epoch 136, Loss: 0.852, Training Accuracy: 70.01%\n",
      "Run 1, Epoch 137, Loss: 0.848, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 138, Loss: 0.849, Training Accuracy: 69.97%\n",
      "Run 1, Epoch 139, Loss: 0.850, Training Accuracy: 70.01%\n",
      "Run 1, Epoch 140, Loss: 0.852, Training Accuracy: 70.05%\n",
      "Run 1, Epoch 141, Loss: 0.852, Training Accuracy: 69.80%\n",
      "Run 1, Epoch 142, Loss: 0.848, Training Accuracy: 69.99%\n",
      "Run 1, Epoch 143, Loss: 0.853, Training Accuracy: 69.91%\n",
      "Run 1, Epoch 144, Loss: 0.854, Training Accuracy: 69.95%\n",
      "Run 1, Epoch 145, Loss: 0.854, Training Accuracy: 69.94%\n",
      "Run 1, Epoch 146, Loss: 0.853, Training Accuracy: 69.97%\n",
      "Run 1, Epoch 147, Loss: 0.847, Training Accuracy: 70.29%\n",
      "Run 1, Epoch 148, Loss: 0.851, Training Accuracy: 69.90%\n",
      "Run 1, Epoch 149, Loss: 0.851, Training Accuracy: 69.87%\n",
      "Run 1, Epoch 150, Loss: 0.851, Training Accuracy: 69.89%\n",
      "Run 1, Epoch 151, Loss: 0.851, Training Accuracy: 70.13%\n",
      "Run 1, Epoch 152, Loss: 0.850, Training Accuracy: 70.15%\n",
      "Run 1, Epoch 153, Loss: 0.855, Training Accuracy: 69.83%\n",
      "Run 1, Epoch 154, Loss: 0.849, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 155, Loss: 0.848, Training Accuracy: 70.29%\n",
      "Run 1, Epoch 156, Loss: 0.851, Training Accuracy: 69.92%\n",
      "Run 1, Epoch 157, Loss: 0.852, Training Accuracy: 69.97%\n",
      "Run 1, Epoch 158, Loss: 0.849, Training Accuracy: 70.09%\n",
      "Run 1, Epoch 159, Loss: 0.852, Training Accuracy: 69.94%\n",
      "Run 1, Epoch 160, Loss: 0.853, Training Accuracy: 70.26%\n",
      "Run 1, Epoch 161, Loss: 0.851, Training Accuracy: 70.02%\n",
      "Run 1, Epoch 162, Loss: 0.849, Training Accuracy: 70.07%\n",
      "Run 1, Epoch 163, Loss: 0.851, Training Accuracy: 70.11%\n",
      "Run 1, Epoch 164, Loss: 0.851, Training Accuracy: 70.09%\n",
      "Run 1, Epoch 165, Loss: 0.850, Training Accuracy: 70.05%\n",
      "Run 1, Epoch 166, Loss: 0.845, Training Accuracy: 69.99%\n",
      "Run 1, Epoch 167, Loss: 0.851, Training Accuracy: 70.01%\n",
      "Run 1, Epoch 168, Loss: 0.851, Training Accuracy: 70.01%\n",
      "Run 1, Epoch 169, Loss: 0.852, Training Accuracy: 69.90%\n",
      "Run 1, Epoch 170, Loss: 0.847, Training Accuracy: 70.01%\n",
      "Run 1, Epoch 171, Loss: 0.850, Training Accuracy: 70.29%\n",
      "Run 1, Epoch 172, Loss: 0.852, Training Accuracy: 69.88%\n",
      "Run 1, Epoch 173, Loss: 0.849, Training Accuracy: 70.01%\n",
      "Run 1, Epoch 174, Loss: 0.852, Training Accuracy: 69.95%\n",
      "Run 1, Epoch 175, Loss: 0.851, Training Accuracy: 69.86%\n",
      "Run 1, Final Accuracy on test set: 71.72%\n",
      "Results after run 1:\n",
      "Training Accuracies: [38.392, 48.536, 52.714, 56.304, 58.292, 60.35, 62.018, 63.188, 64.464, 65.266, 67.692, 68.23, 68.808, 68.792, 68.842, 69.012, 68.986, 69.16, 69.376, 69.684, 69.752, 69.842, 69.772, 70.188, 70.116, 70.074, 69.974, 69.918, 69.872, 70.036, 70.154, 70.04, 69.802, 70.024, 69.766, 70.1, 70.244, 69.892, 70.27, 70.336, 69.812, 70.072, 69.978, 70.078, 70.122, 69.896, 70.052, 69.822, 69.852, 70.248, 70.17, 69.96, 70.044, 70.078, 69.83, 70.092, 69.964, 70.076, 69.842, 70.23, 69.976, 70.068, 69.852, 70.018, 70.002, 70.224, 69.656, 69.712, 69.914, 69.95, 70.19, 70.186, 70.14, 70.39, 70.114, 69.9, 69.948, 69.794, 69.976, 69.926, 70.02, 69.74, 69.988, 70.048, 69.938, 69.906, 69.864, 69.836, 70.038, 70.248, 70.096, 70.17, 70.134, 70.182, 70.016, 70.222, 70.152, 70.028, 70.128, 70.268, 69.938, 70.064, 70.058, 70.09, 70.088, 70.022, 70.13, 70.128, 70.038, 70.124, 70.062, 70.132, 70.08, 70.142, 69.908, 69.908, 70.226, 70.072, 70.076, 69.806, 70.406, 70.024, 69.842, 69.988, 70.082, 69.956, 70.128, 69.982, 70.19, 70.348, 70.106, 70.016, 70.092, 69.982, 70.1, 70.014, 70.024, 69.968, 70.014, 70.046, 69.796, 69.994, 69.912, 69.954, 69.936, 69.966, 70.288, 69.898, 69.87, 69.888, 70.132, 70.146, 69.834, 70.016, 70.29, 69.922, 69.968, 70.088, 69.944, 70.264, 70.024, 70.068, 70.114, 70.092, 70.052, 69.994, 70.012, 70.012, 69.902, 70.014, 70.29, 69.876, 70.014, 69.952, 69.864]\n",
      "Test Accuracy: 71.72%\n",
      "Losses: [1.6818486238684496, 1.4213017490513795, 1.3161959815818025, 1.2243249020003297, 1.166952788982245, 1.1134957793118703, 1.0726107285760553, 1.0300370194113162, 0.9972512093956208, 0.9787916048713352, 0.9130463042222631, 0.901643440241704, 0.8909033277760381, 0.8890321083995693, 0.8801965108308036, 0.8806850161698773, 0.8781484536197789, 0.8726143977221321, 0.8701814765210651, 0.8632961229595078, 0.8575108909545957, 0.8574961236370798, 0.8566066775175617, 0.8528647319130276, 0.8551274019738903, 0.8514327455664534, 0.8543358513766237, 0.8517828234626205, 0.8523909268171891, 0.8508129161032264, 0.850139879204733, 0.8507156013832677, 0.856814261135238, 0.8500788877992069, 0.8520630954781456, 0.8526343174297791, 0.8452837329996211, 0.8557110114780533, 0.8477497853891319, 0.8474113961010028, 0.8582238937582811, 0.848043714185505, 0.8538454075908417, 0.8511844245369172, 0.8499782253104402, 0.8507838848301822, 0.8514848642641931, 0.8529521947931451, 0.8527458069269614, 0.8478961083895106, 0.8518120077869776, 0.8503829667635281, 0.8538449687116286, 0.848477253523629, 0.8504125163378313, 0.8473548607143295, 0.8518580894945832, 0.848693488808849, 0.8506568949240858, 0.8498308987873594, 0.8492647906398529, 0.8530130134823987, 0.8515511876176995, 0.8488316574060094, 0.8548576959868526, 0.8465228378010528, 0.8557830335539015, 0.855380140759451, 0.8506692253110354, 0.8506488161318747, 0.8524960021838508, 0.8495325496434556, 0.850545787292978, 0.8477435958050096, 0.8515093992738163, 0.8549689025525242, 0.8518358720537952, 0.8511519479324751, 0.8517346504094351, 0.8525133457635065, 0.8494888650791724, 0.8548136943441522, 0.8533756357934469, 0.848940635123826, 0.8556664520517334, 0.8523156432544484, 0.8514664167028558, 0.8497881686596005, 0.8531462528821453, 0.8515784170316614, 0.8497778091894086, 0.8521636239707927, 0.8505611872429129, 0.8517992804422403, 0.8508015360368792, 0.8484415672624203, 0.8484730254048887, 0.849786391343607, 0.8494954449136544, 0.8486530950002353, 0.8512684147985999, 0.8499798863135335, 0.8510744480220863, 0.8528194611944506, 0.8478998970192717, 0.8467629112855858, 0.8500095012852603, 0.8502576933492481, 0.8454124619588828, 0.8498389641647144, 0.8490616397174728, 0.8475315939739841, 0.8530946152899271, 0.8484693318986527, 0.8543639126641062, 0.8559424996071154, 0.8506416968067588, 0.852068182605002, 0.852316415218441, 0.8554944696328829, 0.8481021183530998, 0.8478889715336168, 0.8530417503908162, 0.8493074610105256, 0.8518652274175678, 0.8509763846616916, 0.8493926095230805, 0.8517166048364566, 0.8495936806854385, 0.8510996935617589, 0.850204919305299, 0.8476851686187412, 0.8518476634074355, 0.8511692532493026, 0.8503954716960488, 0.8515826436259862, 0.8476784075312602, 0.8485722182047032, 0.8504059197347792, 0.852490256052188, 0.8516282126726702, 0.8478028634015251, 0.853397093465566, 0.8536700520978864, 0.8540944978404228, 0.852754869272032, 0.8470264770795622, 0.8510698432202839, 0.8508879060635481, 0.8513502265181383, 0.8511635619965966, 0.8499705631409764, 0.8547960789612187, 0.8488314102982621, 0.8482686683649907, 0.8509820035046629, 0.8523938055233578, 0.8492313438974073, 0.8520880471105161, 0.8525557452455506, 0.8510751837049909, 0.8489590569225418, 0.850687728208654, 0.8509874752415415, 0.8495295267275838, 0.8450540232536433, 0.8513100549692998, 0.8511144627085732, 0.8522930854116865, 0.847105439971475, 0.8504339061734622, 0.8515688116898013, 0.8492339646724789, 0.8521193060118829, 0.8505776396492863]\n",
      "Starting run 2/5\n",
      "Run 2, Epoch 1, Loss: 1.724, Training Accuracy: 37.18%\n",
      "Run 2, Epoch 2, Loss: 1.441, Training Accuracy: 47.83%\n",
      "Run 2, Epoch 3, Loss: 1.319, Training Accuracy: 52.72%\n",
      "Run 2, Epoch 4, Loss: 1.238, Training Accuracy: 55.71%\n",
      "Run 2, Epoch 5, Loss: 1.179, Training Accuracy: 58.01%\n",
      "Run 2, Epoch 6, Loss: 1.128, Training Accuracy: 60.07%\n",
      "Run 2, Epoch 7, Loss: 1.084, Training Accuracy: 61.34%\n",
      "Run 2, Epoch 8, Loss: 1.051, Training Accuracy: 62.79%\n",
      "Run 2, Epoch 9, Loss: 1.024, Training Accuracy: 63.86%\n",
      "Run 2, Epoch 10, Loss: 1.003, Training Accuracy: 64.70%\n",
      "Run 2, Epoch 11, Loss: 0.942, Training Accuracy: 66.87%\n",
      "Run 2, Epoch 12, Loss: 0.927, Training Accuracy: 67.41%\n",
      "Run 2, Epoch 13, Loss: 0.920, Training Accuracy: 67.62%\n",
      "Run 2, Epoch 14, Loss: 0.913, Training Accuracy: 68.10%\n",
      "Run 2, Epoch 15, Loss: 0.909, Training Accuracy: 67.74%\n",
      "Run 2, Epoch 16, Loss: 0.905, Training Accuracy: 68.19%\n",
      "Run 2, Epoch 17, Loss: 0.903, Training Accuracy: 68.18%\n",
      "Run 2, Epoch 18, Loss: 0.899, Training Accuracy: 68.40%\n",
      "Run 2, Epoch 19, Loss: 0.896, Training Accuracy: 68.47%\n",
      "Run 2, Epoch 20, Loss: 0.891, Training Accuracy: 68.83%\n",
      "Run 2, Epoch 21, Loss: 0.880, Training Accuracy: 69.02%\n",
      "Run 2, Epoch 22, Loss: 0.878, Training Accuracy: 69.13%\n",
      "Run 2, Epoch 23, Loss: 0.886, Training Accuracy: 68.84%\n",
      "Run 2, Epoch 24, Loss: 0.877, Training Accuracy: 69.33%\n",
      "Run 2, Epoch 25, Loss: 0.878, Training Accuracy: 69.01%\n",
      "Run 2, Epoch 26, Loss: 0.881, Training Accuracy: 69.19%\n",
      "Run 2, Epoch 27, Loss: 0.881, Training Accuracy: 69.08%\n",
      "Run 2, Epoch 28, Loss: 0.884, Training Accuracy: 68.64%\n",
      "Run 2, Epoch 29, Loss: 0.878, Training Accuracy: 69.07%\n",
      "Run 2, Epoch 30, Loss: 0.878, Training Accuracy: 69.40%\n",
      "Run 2, Epoch 31, Loss: 0.877, Training Accuracy: 69.32%\n",
      "Run 2, Epoch 32, Loss: 0.877, Training Accuracy: 69.33%\n",
      "Run 2, Epoch 33, Loss: 0.880, Training Accuracy: 69.06%\n",
      "Run 2, Epoch 34, Loss: 0.878, Training Accuracy: 69.13%\n",
      "Run 2, Epoch 35, Loss: 0.877, Training Accuracy: 69.36%\n",
      "Run 2, Epoch 36, Loss: 0.876, Training Accuracy: 69.31%\n",
      "Run 2, Epoch 37, Loss: 0.877, Training Accuracy: 69.47%\n",
      "Run 2, Epoch 38, Loss: 0.879, Training Accuracy: 68.92%\n",
      "Run 2, Epoch 39, Loss: 0.880, Training Accuracy: 69.16%\n",
      "Run 2, Epoch 40, Loss: 0.876, Training Accuracy: 69.09%\n",
      "Run 2, Epoch 41, Loss: 0.877, Training Accuracy: 69.12%\n",
      "Run 2, Epoch 42, Loss: 0.879, Training Accuracy: 69.12%\n",
      "Run 2, Epoch 43, Loss: 0.875, Training Accuracy: 69.24%\n",
      "Run 2, Epoch 44, Loss: 0.879, Training Accuracy: 69.13%\n",
      "Run 2, Epoch 45, Loss: 0.875, Training Accuracy: 69.17%\n",
      "Run 2, Epoch 46, Loss: 0.876, Training Accuracy: 69.11%\n",
      "Run 2, Epoch 47, Loss: 0.876, Training Accuracy: 69.32%\n",
      "Run 2, Epoch 48, Loss: 0.879, Training Accuracy: 69.20%\n",
      "Run 2, Epoch 49, Loss: 0.875, Training Accuracy: 69.32%\n",
      "Run 2, Epoch 50, Loss: 0.877, Training Accuracy: 69.23%\n",
      "Run 2, Epoch 51, Loss: 0.878, Training Accuracy: 69.15%\n",
      "Run 2, Epoch 52, Loss: 0.877, Training Accuracy: 69.24%\n",
      "Run 2, Epoch 53, Loss: 0.875, Training Accuracy: 69.17%\n",
      "Run 2, Epoch 54, Loss: 0.878, Training Accuracy: 69.23%\n",
      "Run 2, Epoch 55, Loss: 0.878, Training Accuracy: 69.31%\n",
      "Run 2, Epoch 56, Loss: 0.880, Training Accuracy: 68.93%\n",
      "Run 2, Epoch 57, Loss: 0.880, Training Accuracy: 69.22%\n",
      "Run 2, Epoch 58, Loss: 0.877, Training Accuracy: 69.15%\n",
      "Run 2, Epoch 59, Loss: 0.874, Training Accuracy: 69.21%\n",
      "Run 2, Epoch 60, Loss: 0.875, Training Accuracy: 69.30%\n",
      "Run 2, Epoch 61, Loss: 0.880, Training Accuracy: 69.17%\n",
      "Run 2, Epoch 62, Loss: 0.876, Training Accuracy: 69.38%\n",
      "Run 2, Epoch 63, Loss: 0.879, Training Accuracy: 69.20%\n",
      "Run 2, Epoch 64, Loss: 0.875, Training Accuracy: 69.40%\n",
      "Run 2, Epoch 65, Loss: 0.880, Training Accuracy: 68.93%\n",
      "Run 2, Epoch 66, Loss: 0.874, Training Accuracy: 69.50%\n",
      "Run 2, Epoch 67, Loss: 0.877, Training Accuracy: 69.27%\n",
      "Run 2, Epoch 68, Loss: 0.879, Training Accuracy: 68.95%\n",
      "Run 2, Epoch 69, Loss: 0.875, Training Accuracy: 69.07%\n",
      "Run 2, Epoch 70, Loss: 0.874, Training Accuracy: 69.10%\n",
      "Run 2, Epoch 71, Loss: 0.880, Training Accuracy: 69.03%\n",
      "Run 2, Epoch 72, Loss: 0.875, Training Accuracy: 69.42%\n",
      "Run 2, Epoch 73, Loss: 0.884, Training Accuracy: 68.85%\n",
      "Run 2, Epoch 74, Loss: 0.878, Training Accuracy: 69.24%\n",
      "Run 2, Epoch 75, Loss: 0.876, Training Accuracy: 69.07%\n",
      "Run 2, Epoch 76, Loss: 0.876, Training Accuracy: 69.14%\n",
      "Run 2, Epoch 77, Loss: 0.878, Training Accuracy: 69.00%\n",
      "Run 2, Epoch 78, Loss: 0.876, Training Accuracy: 69.09%\n",
      "Run 2, Epoch 79, Loss: 0.876, Training Accuracy: 69.24%\n",
      "Run 2, Epoch 80, Loss: 0.873, Training Accuracy: 69.42%\n",
      "Run 2, Epoch 81, Loss: 0.875, Training Accuracy: 69.20%\n",
      "Run 2, Epoch 82, Loss: 0.877, Training Accuracy: 69.11%\n",
      "Run 2, Epoch 83, Loss: 0.875, Training Accuracy: 69.34%\n",
      "Run 2, Epoch 84, Loss: 0.882, Training Accuracy: 68.67%\n",
      "Run 2, Epoch 85, Loss: 0.879, Training Accuracy: 69.14%\n",
      "Run 2, Epoch 86, Loss: 0.878, Training Accuracy: 69.33%\n",
      "Run 2, Epoch 87, Loss: 0.882, Training Accuracy: 68.95%\n",
      "Run 2, Epoch 88, Loss: 0.874, Training Accuracy: 69.14%\n",
      "Run 2, Epoch 89, Loss: 0.877, Training Accuracy: 69.12%\n",
      "Run 2, Epoch 90, Loss: 0.878, Training Accuracy: 69.14%\n",
      "Run 2, Epoch 91, Loss: 0.882, Training Accuracy: 68.91%\n",
      "Run 2, Epoch 92, Loss: 0.877, Training Accuracy: 69.21%\n",
      "Run 2, Epoch 93, Loss: 0.879, Training Accuracy: 69.07%\n",
      "Run 2, Epoch 94, Loss: 0.878, Training Accuracy: 69.07%\n",
      "Run 2, Epoch 95, Loss: 0.876, Training Accuracy: 69.15%\n",
      "Run 2, Epoch 96, Loss: 0.877, Training Accuracy: 69.20%\n",
      "Run 2, Epoch 97, Loss: 0.877, Training Accuracy: 69.02%\n",
      "Run 2, Epoch 98, Loss: 0.877, Training Accuracy: 69.06%\n",
      "Run 2, Epoch 99, Loss: 0.875, Training Accuracy: 69.19%\n",
      "Run 2, Epoch 100, Loss: 0.878, Training Accuracy: 69.08%\n",
      "Run 2, Epoch 101, Loss: 0.878, Training Accuracy: 69.10%\n",
      "Run 2, Epoch 102, Loss: 0.877, Training Accuracy: 69.33%\n",
      "Run 2, Epoch 103, Loss: 0.879, Training Accuracy: 69.32%\n",
      "Run 2, Epoch 104, Loss: 0.877, Training Accuracy: 69.11%\n",
      "Run 2, Epoch 105, Loss: 0.875, Training Accuracy: 69.18%\n",
      "Run 2, Epoch 106, Loss: 0.875, Training Accuracy: 69.28%\n",
      "Run 2, Epoch 107, Loss: 0.875, Training Accuracy: 69.27%\n",
      "Run 2, Epoch 108, Loss: 0.874, Training Accuracy: 69.18%\n",
      "Run 2, Epoch 109, Loss: 0.881, Training Accuracy: 68.95%\n",
      "Run 2, Epoch 110, Loss: 0.872, Training Accuracy: 69.31%\n",
      "Run 2, Epoch 111, Loss: 0.877, Training Accuracy: 69.13%\n",
      "Run 2, Epoch 112, Loss: 0.878, Training Accuracy: 69.11%\n",
      "Run 2, Epoch 113, Loss: 0.882, Training Accuracy: 69.17%\n",
      "Run 2, Epoch 114, Loss: 0.880, Training Accuracy: 69.01%\n",
      "Run 2, Epoch 115, Loss: 0.880, Training Accuracy: 69.03%\n",
      "Run 2, Epoch 116, Loss: 0.878, Training Accuracy: 69.11%\n",
      "Run 2, Epoch 117, Loss: 0.876, Training Accuracy: 69.41%\n",
      "Run 2, Epoch 118, Loss: 0.876, Training Accuracy: 69.17%\n",
      "Run 2, Epoch 119, Loss: 0.874, Training Accuracy: 69.32%\n",
      "Run 2, Epoch 120, Loss: 0.875, Training Accuracy: 69.13%\n",
      "Run 2, Epoch 121, Loss: 0.878, Training Accuracy: 68.97%\n",
      "Run 2, Epoch 122, Loss: 0.878, Training Accuracy: 68.88%\n",
      "Run 2, Epoch 123, Loss: 0.879, Training Accuracy: 69.00%\n",
      "Run 2, Epoch 124, Loss: 0.875, Training Accuracy: 69.14%\n",
      "Run 2, Epoch 125, Loss: 0.878, Training Accuracy: 69.33%\n",
      "Run 2, Epoch 126, Loss: 0.878, Training Accuracy: 69.01%\n",
      "Run 2, Epoch 127, Loss: 0.874, Training Accuracy: 69.28%\n",
      "Run 2, Epoch 128, Loss: 0.879, Training Accuracy: 69.03%\n",
      "Run 2, Epoch 129, Loss: 0.876, Training Accuracy: 69.02%\n",
      "Run 2, Epoch 130, Loss: 0.878, Training Accuracy: 69.13%\n",
      "Run 2, Epoch 131, Loss: 0.877, Training Accuracy: 69.32%\n",
      "Run 2, Epoch 132, Loss: 0.876, Training Accuracy: 69.23%\n",
      "Run 2, Epoch 133, Loss: 0.873, Training Accuracy: 69.22%\n",
      "Run 2, Epoch 134, Loss: 0.876, Training Accuracy: 69.17%\n",
      "Run 2, Epoch 135, Loss: 0.878, Training Accuracy: 68.89%\n",
      "Run 2, Epoch 136, Loss: 0.876, Training Accuracy: 69.35%\n",
      "Run 2, Epoch 137, Loss: 0.877, Training Accuracy: 69.20%\n",
      "Run 2, Epoch 138, Loss: 0.877, Training Accuracy: 69.22%\n",
      "Run 2, Epoch 139, Loss: 0.878, Training Accuracy: 69.00%\n",
      "Run 2, Epoch 140, Loss: 0.879, Training Accuracy: 69.22%\n",
      "Run 2, Epoch 141, Loss: 0.877, Training Accuracy: 69.21%\n",
      "Run 2, Epoch 142, Loss: 0.882, Training Accuracy: 68.92%\n",
      "Run 2, Epoch 143, Loss: 0.874, Training Accuracy: 69.44%\n",
      "Run 2, Epoch 144, Loss: 0.872, Training Accuracy: 69.19%\n",
      "Run 2, Epoch 145, Loss: 0.880, Training Accuracy: 69.11%\n",
      "Run 2, Epoch 146, Loss: 0.880, Training Accuracy: 69.05%\n",
      "Run 2, Epoch 147, Loss: 0.876, Training Accuracy: 69.37%\n",
      "Run 2, Epoch 148, Loss: 0.878, Training Accuracy: 69.04%\n",
      "Run 2, Epoch 149, Loss: 0.878, Training Accuracy: 69.23%\n",
      "Run 2, Epoch 150, Loss: 0.877, Training Accuracy: 69.16%\n",
      "Run 2, Epoch 151, Loss: 0.876, Training Accuracy: 69.03%\n",
      "Run 2, Epoch 152, Loss: 0.878, Training Accuracy: 69.15%\n",
      "Run 2, Epoch 153, Loss: 0.870, Training Accuracy: 69.38%\n",
      "Run 2, Epoch 154, Loss: 0.875, Training Accuracy: 69.36%\n",
      "Run 2, Epoch 155, Loss: 0.878, Training Accuracy: 69.22%\n",
      "Run 2, Epoch 156, Loss: 0.880, Training Accuracy: 69.12%\n",
      "Run 2, Epoch 157, Loss: 0.878, Training Accuracy: 69.15%\n",
      "Run 2, Epoch 158, Loss: 0.873, Training Accuracy: 69.16%\n",
      "Run 2, Epoch 159, Loss: 0.878, Training Accuracy: 69.28%\n",
      "Run 2, Epoch 160, Loss: 0.878, Training Accuracy: 69.29%\n",
      "Run 2, Epoch 161, Loss: 0.879, Training Accuracy: 69.18%\n",
      "Run 2, Epoch 162, Loss: 0.875, Training Accuracy: 69.09%\n",
      "Run 2, Epoch 163, Loss: 0.881, Training Accuracy: 68.90%\n",
      "Run 2, Epoch 164, Loss: 0.876, Training Accuracy: 69.20%\n",
      "Run 2, Epoch 165, Loss: 0.877, Training Accuracy: 69.07%\n",
      "Run 2, Epoch 166, Loss: 0.878, Training Accuracy: 69.13%\n",
      "Run 2, Epoch 167, Loss: 0.877, Training Accuracy: 69.18%\n",
      "Run 2, Epoch 168, Loss: 0.877, Training Accuracy: 69.05%\n",
      "Run 2, Epoch 169, Loss: 0.876, Training Accuracy: 69.21%\n",
      "Run 2, Epoch 170, Loss: 0.877, Training Accuracy: 69.28%\n",
      "Run 2, Epoch 171, Loss: 0.878, Training Accuracy: 69.19%\n",
      "Run 2, Epoch 172, Loss: 0.877, Training Accuracy: 69.26%\n",
      "Run 2, Epoch 173, Loss: 0.875, Training Accuracy: 69.17%\n",
      "Run 2, Epoch 174, Loss: 0.878, Training Accuracy: 69.00%\n",
      "Run 2, Epoch 175, Loss: 0.876, Training Accuracy: 69.45%\n",
      "Run 2, Final Accuracy on test set: 70.41%\n",
      "Results after run 2:\n",
      "Training Accuracies: [37.178, 47.826, 52.72, 55.712, 58.006, 60.072, 61.344, 62.788, 63.864, 64.702, 66.87, 67.412, 67.618, 68.098, 67.736, 68.188, 68.182, 68.396, 68.47, 68.834, 69.024, 69.134, 68.836, 69.33, 69.014, 69.19, 69.08, 68.642, 69.072, 69.396, 69.32, 69.334, 69.06, 69.132, 69.358, 69.308, 69.472, 68.916, 69.156, 69.094, 69.118, 69.116, 69.24, 69.132, 69.172, 69.112, 69.318, 69.198, 69.318, 69.234, 69.148, 69.244, 69.172, 69.232, 69.312, 68.926, 69.222, 69.152, 69.214, 69.298, 69.168, 69.38, 69.198, 69.4, 68.926, 69.504, 69.274, 68.95, 69.074, 69.104, 69.026, 69.422, 68.85, 69.236, 69.072, 69.14, 69.0, 69.094, 69.238, 69.416, 69.196, 69.112, 69.342, 68.672, 69.136, 69.33, 68.946, 69.138, 69.122, 69.142, 68.906, 69.206, 69.068, 69.068, 69.15, 69.198, 69.02, 69.058, 69.194, 69.076, 69.096, 69.33, 69.318, 69.112, 69.178, 69.28, 69.27, 69.182, 68.95, 69.306, 69.132, 69.108, 69.172, 69.014, 69.032, 69.112, 69.412, 69.168, 69.322, 69.132, 68.972, 68.88, 69.002, 69.136, 69.326, 69.006, 69.28, 69.034, 69.016, 69.126, 69.324, 69.228, 69.216, 69.172, 68.892, 69.35, 69.196, 69.218, 68.998, 69.218, 69.212, 68.922, 69.442, 69.188, 69.112, 69.048, 69.372, 69.044, 69.232, 69.162, 69.03, 69.154, 69.384, 69.358, 69.218, 69.116, 69.154, 69.162, 69.28, 69.292, 69.18, 69.092, 68.904, 69.198, 69.074, 69.134, 69.176, 69.052, 69.208, 69.278, 69.192, 69.26, 69.172, 69.002, 69.448]\n",
      "Test Accuracy: 70.41%\n",
      "Losses: [1.724474196848662, 1.4405883065879803, 1.3185397743264122, 1.2381050856521978, 1.179436774509947, 1.128202419604182, 1.0844172913095225, 1.0505554310196197, 1.0237168302316495, 1.003004414041329, 0.9419107391401325, 0.9267143788545028, 0.9203384321973757, 0.9129753385663337, 0.908821108396096, 0.905233443698005, 0.9033166476527749, 0.8989562651385432, 0.8955225830187883, 0.8914945092042694, 0.8802994190884368, 0.8781515776043962, 0.8855320504864158, 0.8766073054059997, 0.8783817480287284, 0.8809950452326508, 0.8808857519608324, 0.8841507015630717, 0.8775199685255279, 0.8782186608790131, 0.877034549365568, 0.8767230957365402, 0.8795205979700893, 0.8778922571550549, 0.8769966007193641, 0.8756370992611742, 0.8767600146400959, 0.8791876409364783, 0.8803637532321998, 0.8761750659369447, 0.8765823167303334, 0.8792317174279781, 0.8750129848185098, 0.8786756095983793, 0.8747652136456326, 0.8757035435008271, 0.8756492203458801, 0.8794658845647827, 0.8750059452203228, 0.8773231088657818, 0.8776518844277658, 0.8774753804401974, 0.8751775293094118, 0.87805481700946, 0.8783177497136928, 0.8803609493748307, 0.8795771742110972, 0.8769577592230209, 0.874304344891892, 0.8752409424013494, 0.8801643827077373, 0.8756307069297946, 0.8793428177418916, 0.8747501928178246, 0.8797700103286588, 0.8738363815085662, 0.8774795013925304, 0.8793511343429156, 0.8752828895893243, 0.8738572949643635, 0.8800904679176448, 0.8749789030045805, 0.8837341356765279, 0.8781157972867532, 0.875983382429918, 0.8756817681405246, 0.8780281255617166, 0.8760617936358732, 0.8763015026326679, 0.8734507357982724, 0.8746258681997314, 0.8772544626079862, 0.8746437995939913, 0.8822706681688118, 0.8794281936972342, 0.8781669338036071, 0.8817428524231972, 0.8735733498697695, 0.8771554521282615, 0.8783881552993794, 0.881564152515148, 0.8774412701196987, 0.8786320860123695, 0.8777155470665153, 0.876011665519851, 0.8774820015863385, 0.8767200709913697, 0.8771929818650951, 0.8751320236784113, 0.8778591281007928, 0.8775611259138493, 0.8765167889692594, 0.8787027167542206, 0.8772461940260494, 0.8747358793187934, 0.8751722263253253, 0.8753462042040228, 0.8739888956174826, 0.8808737494756499, 0.8718823321030268, 0.8772681875302054, 0.8780654548379161, 0.8815069570565772, 0.8797204752102532, 0.8798466112912463, 0.8778430566458446, 0.8764299846366238, 0.8760893611651858, 0.8743272832287546, 0.8754236335339753, 0.8781822407641984, 0.8784693831677937, 0.8790733520789524, 0.8745633496347901, 0.8777783714291995, 0.8783284762631292, 0.8740641773509248, 0.8793711714122606, 0.8761370124109565, 0.8784324290502407, 0.8774027629276676, 0.8758084560599169, 0.8731695816035161, 0.8758802569430807, 0.8777306182000338, 0.8758183460101447, 0.876767193722298, 0.8771473071764192, 0.8779421031017742, 0.8791431344073751, 0.8771951879991595, 0.8818038049561289, 0.8738997014587188, 0.8717767594720397, 0.8802983462048308, 0.8797023148487901, 0.8755641921097056, 0.8775698437410242, 0.8781365166844615, 0.8773369792172366, 0.8763591043479607, 0.8781732268955397, 0.8699419623445672, 0.8745178362292707, 0.8781852845645621, 0.8800748347321434, 0.8781804189352733, 0.8734194531160242, 0.8780313275964059, 0.878181018335435, 0.878913169169365, 0.8754104932250879, 0.8809329301804838, 0.8764620580331749, 0.8770077332206394, 0.8775372424393969, 0.877298294888128, 0.8768847545089624, 0.875928015355259, 0.8769824120699597, 0.8780529945707687, 0.8772313538414743, 0.8754612233327783, 0.8780185458300364, 0.8762068891769175]\n",
      "Starting run 3/5\n",
      "Run 3, Epoch 1, Loss: 1.699, Training Accuracy: 38.06%\n",
      "Run 3, Epoch 2, Loss: 1.429, Training Accuracy: 48.03%\n",
      "Run 3, Epoch 3, Loss: 1.309, Training Accuracy: 52.93%\n",
      "Run 3, Epoch 4, Loss: 1.216, Training Accuracy: 56.32%\n",
      "Run 3, Epoch 5, Loss: 1.151, Training Accuracy: 59.08%\n",
      "Run 3, Epoch 6, Loss: 1.100, Training Accuracy: 60.63%\n",
      "Run 3, Epoch 7, Loss: 1.067, Training Accuracy: 62.26%\n",
      "Run 3, Epoch 8, Loss: 1.039, Training Accuracy: 63.00%\n",
      "Run 3, Epoch 9, Loss: 1.014, Training Accuracy: 64.14%\n",
      "Run 3, Epoch 10, Loss: 0.983, Training Accuracy: 65.21%\n",
      "Run 3, Epoch 11, Loss: 0.918, Training Accuracy: 67.51%\n",
      "Run 3, Epoch 12, Loss: 0.911, Training Accuracy: 67.74%\n",
      "Run 3, Epoch 13, Loss: 0.903, Training Accuracy: 68.16%\n",
      "Run 3, Epoch 14, Loss: 0.899, Training Accuracy: 68.24%\n",
      "Run 3, Epoch 15, Loss: 0.894, Training Accuracy: 68.29%\n",
      "Run 3, Epoch 16, Loss: 0.893, Training Accuracy: 68.51%\n",
      "Run 3, Epoch 17, Loss: 0.885, Training Accuracy: 68.79%\n",
      "Run 3, Epoch 18, Loss: 0.884, Training Accuracy: 68.91%\n",
      "Run 3, Epoch 19, Loss: 0.874, Training Accuracy: 69.30%\n",
      "Run 3, Epoch 20, Loss: 0.877, Training Accuracy: 68.94%\n",
      "Run 3, Epoch 21, Loss: 0.870, Training Accuracy: 69.31%\n",
      "Run 3, Epoch 22, Loss: 0.867, Training Accuracy: 69.75%\n",
      "Run 3, Epoch 23, Loss: 0.863, Training Accuracy: 69.67%\n",
      "Run 3, Epoch 24, Loss: 0.865, Training Accuracy: 69.47%\n",
      "Run 3, Epoch 25, Loss: 0.864, Training Accuracy: 69.62%\n",
      "Run 3, Epoch 26, Loss: 0.865, Training Accuracy: 69.51%\n",
      "Run 3, Epoch 27, Loss: 0.865, Training Accuracy: 69.37%\n",
      "Run 3, Epoch 28, Loss: 0.865, Training Accuracy: 69.32%\n",
      "Run 3, Epoch 29, Loss: 0.862, Training Accuracy: 69.43%\n",
      "Run 3, Epoch 30, Loss: 0.860, Training Accuracy: 69.56%\n",
      "Run 3, Epoch 31, Loss: 0.860, Training Accuracy: 69.43%\n",
      "Run 3, Epoch 32, Loss: 0.864, Training Accuracy: 69.53%\n",
      "Run 3, Epoch 33, Loss: 0.862, Training Accuracy: 69.71%\n",
      "Run 3, Epoch 34, Loss: 0.861, Training Accuracy: 69.78%\n",
      "Run 3, Epoch 35, Loss: 0.861, Training Accuracy: 69.57%\n",
      "Run 3, Epoch 36, Loss: 0.859, Training Accuracy: 69.69%\n",
      "Run 3, Epoch 37, Loss: 0.862, Training Accuracy: 69.76%\n",
      "Run 3, Epoch 38, Loss: 0.865, Training Accuracy: 69.37%\n",
      "Run 3, Epoch 39, Loss: 0.864, Training Accuracy: 69.39%\n",
      "Run 3, Epoch 40, Loss: 0.866, Training Accuracy: 69.56%\n",
      "Run 3, Epoch 41, Loss: 0.864, Training Accuracy: 69.50%\n",
      "Run 3, Epoch 42, Loss: 0.865, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 43, Loss: 0.864, Training Accuracy: 69.57%\n",
      "Run 3, Epoch 44, Loss: 0.860, Training Accuracy: 69.50%\n",
      "Run 3, Epoch 45, Loss: 0.862, Training Accuracy: 69.70%\n",
      "Run 3, Epoch 46, Loss: 0.862, Training Accuracy: 69.72%\n",
      "Run 3, Epoch 47, Loss: 0.861, Training Accuracy: 69.59%\n",
      "Run 3, Epoch 48, Loss: 0.863, Training Accuracy: 69.69%\n",
      "Run 3, Epoch 49, Loss: 0.864, Training Accuracy: 69.46%\n",
      "Run 3, Epoch 50, Loss: 0.861, Training Accuracy: 69.75%\n",
      "Run 3, Epoch 51, Loss: 0.861, Training Accuracy: 69.62%\n",
      "Run 3, Epoch 52, Loss: 0.864, Training Accuracy: 69.45%\n",
      "Run 3, Epoch 53, Loss: 0.861, Training Accuracy: 69.70%\n",
      "Run 3, Epoch 54, Loss: 0.861, Training Accuracy: 69.77%\n",
      "Run 3, Epoch 55, Loss: 0.861, Training Accuracy: 69.71%\n",
      "Run 3, Epoch 56, Loss: 0.860, Training Accuracy: 69.60%\n",
      "Run 3, Epoch 57, Loss: 0.860, Training Accuracy: 69.67%\n",
      "Run 3, Epoch 58, Loss: 0.861, Training Accuracy: 69.76%\n",
      "Run 3, Epoch 59, Loss: 0.859, Training Accuracy: 69.93%\n",
      "Run 3, Epoch 60, Loss: 0.864, Training Accuracy: 69.53%\n",
      "Run 3, Epoch 61, Loss: 0.865, Training Accuracy: 69.37%\n",
      "Run 3, Epoch 62, Loss: 0.864, Training Accuracy: 69.53%\n",
      "Run 3, Epoch 63, Loss: 0.861, Training Accuracy: 69.69%\n",
      "Run 3, Epoch 64, Loss: 0.864, Training Accuracy: 69.60%\n",
      "Run 3, Epoch 65, Loss: 0.865, Training Accuracy: 69.51%\n",
      "Run 3, Epoch 66, Loss: 0.864, Training Accuracy: 69.63%\n",
      "Run 3, Epoch 67, Loss: 0.859, Training Accuracy: 69.76%\n",
      "Run 3, Epoch 68, Loss: 0.863, Training Accuracy: 69.71%\n",
      "Run 3, Epoch 69, Loss: 0.863, Training Accuracy: 69.54%\n",
      "Run 3, Epoch 70, Loss: 0.859, Training Accuracy: 69.55%\n",
      "Run 3, Epoch 71, Loss: 0.858, Training Accuracy: 69.76%\n",
      "Run 3, Epoch 72, Loss: 0.866, Training Accuracy: 69.45%\n",
      "Run 3, Epoch 73, Loss: 0.863, Training Accuracy: 69.62%\n",
      "Run 3, Epoch 74, Loss: 0.860, Training Accuracy: 69.52%\n",
      "Run 3, Epoch 75, Loss: 0.861, Training Accuracy: 69.60%\n",
      "Run 3, Epoch 76, Loss: 0.861, Training Accuracy: 69.51%\n",
      "Run 3, Epoch 77, Loss: 0.864, Training Accuracy: 69.44%\n",
      "Run 3, Epoch 78, Loss: 0.862, Training Accuracy: 69.52%\n",
      "Run 3, Epoch 79, Loss: 0.863, Training Accuracy: 69.68%\n",
      "Run 3, Epoch 80, Loss: 0.862, Training Accuracy: 69.46%\n",
      "Run 3, Epoch 81, Loss: 0.860, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 82, Loss: 0.862, Training Accuracy: 69.73%\n",
      "Run 3, Epoch 83, Loss: 0.864, Training Accuracy: 69.55%\n",
      "Run 3, Epoch 84, Loss: 0.862, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 85, Loss: 0.863, Training Accuracy: 69.71%\n",
      "Run 3, Epoch 86, Loss: 0.863, Training Accuracy: 69.57%\n",
      "Run 3, Epoch 87, Loss: 0.862, Training Accuracy: 69.70%\n",
      "Run 3, Epoch 88, Loss: 0.860, Training Accuracy: 69.93%\n",
      "Run 3, Epoch 89, Loss: 0.864, Training Accuracy: 69.44%\n",
      "Run 3, Epoch 90, Loss: 0.861, Training Accuracy: 69.56%\n",
      "Run 3, Epoch 91, Loss: 0.862, Training Accuracy: 69.60%\n",
      "Run 3, Epoch 92, Loss: 0.858, Training Accuracy: 69.61%\n",
      "Run 3, Epoch 93, Loss: 0.858, Training Accuracy: 69.63%\n",
      "Run 3, Epoch 94, Loss: 0.861, Training Accuracy: 69.73%\n",
      "Run 3, Epoch 95, Loss: 0.863, Training Accuracy: 69.74%\n",
      "Run 3, Epoch 96, Loss: 0.863, Training Accuracy: 69.36%\n",
      "Run 3, Epoch 97, Loss: 0.861, Training Accuracy: 69.70%\n",
      "Run 3, Epoch 98, Loss: 0.864, Training Accuracy: 69.60%\n",
      "Run 3, Epoch 99, Loss: 0.865, Training Accuracy: 69.43%\n",
      "Run 3, Epoch 100, Loss: 0.863, Training Accuracy: 69.58%\n",
      "Run 3, Epoch 101, Loss: 0.859, Training Accuracy: 69.66%\n",
      "Run 3, Epoch 102, Loss: 0.859, Training Accuracy: 69.68%\n",
      "Run 3, Epoch 103, Loss: 0.863, Training Accuracy: 69.52%\n",
      "Run 3, Epoch 104, Loss: 0.862, Training Accuracy: 69.72%\n",
      "Run 3, Epoch 105, Loss: 0.859, Training Accuracy: 69.84%\n",
      "Run 3, Epoch 106, Loss: 0.864, Training Accuracy: 69.46%\n",
      "Run 3, Epoch 107, Loss: 0.862, Training Accuracy: 69.61%\n",
      "Run 3, Epoch 108, Loss: 0.860, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 109, Loss: 0.858, Training Accuracy: 69.66%\n",
      "Run 3, Epoch 110, Loss: 0.862, Training Accuracy: 69.44%\n",
      "Run 3, Epoch 111, Loss: 0.861, Training Accuracy: 69.62%\n",
      "Run 3, Epoch 112, Loss: 0.863, Training Accuracy: 69.63%\n",
      "Run 3, Epoch 113, Loss: 0.864, Training Accuracy: 69.68%\n",
      "Run 3, Epoch 114, Loss: 0.861, Training Accuracy: 69.76%\n",
      "Run 3, Epoch 115, Loss: 0.864, Training Accuracy: 69.47%\n",
      "Run 3, Epoch 116, Loss: 0.866, Training Accuracy: 69.46%\n",
      "Run 3, Epoch 117, Loss: 0.862, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 118, Loss: 0.860, Training Accuracy: 69.26%\n",
      "Run 3, Epoch 119, Loss: 0.862, Training Accuracy: 69.61%\n",
      "Run 3, Epoch 120, Loss: 0.863, Training Accuracy: 69.73%\n",
      "Run 3, Epoch 121, Loss: 0.859, Training Accuracy: 69.60%\n",
      "Run 3, Epoch 122, Loss: 0.862, Training Accuracy: 69.73%\n",
      "Run 3, Epoch 123, Loss: 0.860, Training Accuracy: 69.90%\n",
      "Run 3, Epoch 124, Loss: 0.863, Training Accuracy: 69.72%\n",
      "Run 3, Epoch 125, Loss: 0.862, Training Accuracy: 69.72%\n",
      "Run 3, Epoch 126, Loss: 0.862, Training Accuracy: 69.69%\n",
      "Run 3, Epoch 127, Loss: 0.861, Training Accuracy: 69.71%\n",
      "Run 3, Epoch 128, Loss: 0.862, Training Accuracy: 69.55%\n",
      "Run 3, Epoch 129, Loss: 0.867, Training Accuracy: 69.47%\n",
      "Run 3, Epoch 130, Loss: 0.860, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 131, Loss: 0.862, Training Accuracy: 69.70%\n",
      "Run 3, Epoch 132, Loss: 0.864, Training Accuracy: 69.52%\n",
      "Run 3, Epoch 133, Loss: 0.864, Training Accuracy: 69.54%\n",
      "Run 3, Epoch 134, Loss: 0.859, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 135, Loss: 0.861, Training Accuracy: 69.39%\n",
      "Run 3, Epoch 136, Loss: 0.862, Training Accuracy: 69.78%\n",
      "Run 3, Epoch 137, Loss: 0.860, Training Accuracy: 69.66%\n",
      "Run 3, Epoch 138, Loss: 0.863, Training Accuracy: 69.75%\n",
      "Run 3, Epoch 139, Loss: 0.861, Training Accuracy: 69.55%\n",
      "Run 3, Epoch 140, Loss: 0.865, Training Accuracy: 69.37%\n",
      "Run 3, Epoch 141, Loss: 0.859, Training Accuracy: 69.81%\n",
      "Run 3, Epoch 142, Loss: 0.861, Training Accuracy: 69.66%\n",
      "Run 3, Epoch 143, Loss: 0.866, Training Accuracy: 69.49%\n",
      "Run 3, Epoch 144, Loss: 0.864, Training Accuracy: 69.45%\n",
      "Run 3, Epoch 145, Loss: 0.861, Training Accuracy: 69.52%\n",
      "Run 3, Epoch 146, Loss: 0.865, Training Accuracy: 69.36%\n",
      "Run 3, Epoch 147, Loss: 0.862, Training Accuracy: 69.61%\n",
      "Run 3, Epoch 148, Loss: 0.860, Training Accuracy: 69.68%\n",
      "Run 3, Epoch 149, Loss: 0.860, Training Accuracy: 69.85%\n",
      "Run 3, Epoch 150, Loss: 0.864, Training Accuracy: 69.37%\n",
      "Run 3, Epoch 151, Loss: 0.863, Training Accuracy: 69.73%\n",
      "Run 3, Epoch 152, Loss: 0.862, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 153, Loss: 0.857, Training Accuracy: 69.74%\n",
      "Run 3, Epoch 154, Loss: 0.864, Training Accuracy: 69.69%\n",
      "Run 3, Epoch 155, Loss: 0.863, Training Accuracy: 69.54%\n",
      "Run 3, Epoch 156, Loss: 0.863, Training Accuracy: 69.59%\n",
      "Run 3, Epoch 157, Loss: 0.861, Training Accuracy: 69.74%\n",
      "Run 3, Epoch 158, Loss: 0.861, Training Accuracy: 69.78%\n",
      "Run 3, Epoch 159, Loss: 0.865, Training Accuracy: 69.35%\n",
      "Run 3, Epoch 160, Loss: 0.861, Training Accuracy: 69.59%\n",
      "Run 3, Epoch 161, Loss: 0.862, Training Accuracy: 69.72%\n",
      "Run 3, Epoch 162, Loss: 0.860, Training Accuracy: 69.70%\n",
      "Run 3, Epoch 163, Loss: 0.862, Training Accuracy: 69.75%\n",
      "Run 3, Epoch 164, Loss: 0.863, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 165, Loss: 0.861, Training Accuracy: 69.65%\n",
      "Run 3, Epoch 166, Loss: 0.862, Training Accuracy: 69.37%\n",
      "Run 3, Epoch 167, Loss: 0.861, Training Accuracy: 69.47%\n",
      "Run 3, Epoch 168, Loss: 0.865, Training Accuracy: 69.51%\n",
      "Run 3, Epoch 169, Loss: 0.860, Training Accuracy: 69.72%\n",
      "Run 3, Epoch 170, Loss: 0.863, Training Accuracy: 69.56%\n",
      "Run 3, Epoch 171, Loss: 0.863, Training Accuracy: 69.79%\n",
      "Run 3, Epoch 172, Loss: 0.867, Training Accuracy: 69.43%\n",
      "Run 3, Epoch 173, Loss: 0.863, Training Accuracy: 69.56%\n",
      "Run 3, Epoch 174, Loss: 0.861, Training Accuracy: 69.66%\n",
      "Run 3, Epoch 175, Loss: 0.862, Training Accuracy: 69.45%\n",
      "Run 3, Final Accuracy on test set: 71.33%\n",
      "Results after run 3:\n",
      "Training Accuracies: [38.06, 48.026, 52.93, 56.324, 59.082, 60.626, 62.26, 63.002, 64.14, 65.206, 67.514, 67.738, 68.164, 68.242, 68.294, 68.506, 68.786, 68.906, 69.302, 68.936, 69.308, 69.748, 69.668, 69.474, 69.616, 69.506, 69.374, 69.324, 69.428, 69.56, 69.426, 69.528, 69.708, 69.78, 69.57, 69.688, 69.764, 69.372, 69.394, 69.556, 69.502, 69.648, 69.566, 69.5, 69.7, 69.724, 69.588, 69.692, 69.458, 69.748, 69.618, 69.45, 69.704, 69.768, 69.714, 69.604, 69.668, 69.764, 69.926, 69.532, 69.368, 69.532, 69.686, 69.602, 69.512, 69.626, 69.762, 69.712, 69.542, 69.548, 69.756, 69.45, 69.616, 69.516, 69.598, 69.514, 69.442, 69.522, 69.684, 69.456, 69.648, 69.728, 69.55, 69.65, 69.714, 69.57, 69.696, 69.93, 69.436, 69.562, 69.598, 69.61, 69.634, 69.732, 69.736, 69.364, 69.7, 69.6, 69.434, 69.578, 69.658, 69.684, 69.518, 69.716, 69.838, 69.458, 69.608, 69.648, 69.662, 69.44, 69.624, 69.628, 69.676, 69.762, 69.474, 69.456, 69.654, 69.262, 69.608, 69.728, 69.596, 69.734, 69.9, 69.722, 69.716, 69.688, 69.706, 69.548, 69.472, 69.646, 69.702, 69.52, 69.544, 69.646, 69.394, 69.784, 69.656, 69.754, 69.548, 69.368, 69.812, 69.662, 69.492, 69.446, 69.52, 69.364, 69.606, 69.678, 69.852, 69.366, 69.728, 69.648, 69.738, 69.688, 69.542, 69.588, 69.738, 69.78, 69.354, 69.588, 69.718, 69.696, 69.746, 69.654, 69.648, 69.368, 69.472, 69.506, 69.716, 69.562, 69.792, 69.43, 69.56, 69.658, 69.452]\n",
      "Test Accuracy: 71.33%\n",
      "Losses: [1.69884412428912, 1.4285972014717434, 1.3093017231472923, 1.216057458649511, 1.1513296201101044, 1.0996464762236455, 1.0674550065299129, 1.038686795307852, 1.0143794989037087, 0.9826142383963251, 0.9179209707033299, 0.9110360288863901, 0.9025092467932445, 0.8993898396906645, 0.8935178660065927, 0.8933245952781814, 0.88539324468359, 0.8835963949827892, 0.874144565113975, 0.8768749352916122, 0.8702625377708689, 0.8667706392915048, 0.8632295662179932, 0.8654995673452802, 0.8641676657340106, 0.865236547292041, 0.864894590261952, 0.8649835751184722, 0.8620018715138935, 0.8604469288645498, 0.8604251458821699, 0.8638704823106146, 0.8623522892022681, 0.8605957224850764, 0.8612433667378048, 0.8590275962334459, 0.8615586102161261, 0.8648550963157888, 0.8644342437729506, 0.8655801695935866, 0.86350080073642, 0.8652778702318821, 0.8643737478024515, 0.859630689291698, 0.8623614585613046, 0.8621621070920354, 0.8605640318692492, 0.8626508798135821, 0.8639979441757397, 0.861143736430751, 0.8609494365694578, 0.8635080525332399, 0.861459759647584, 0.8612894325914895, 0.8606367955732224, 0.8601591394990301, 0.8597792887016941, 0.860798281324489, 0.859487038134309, 0.8638574474912775, 0.8648106169212809, 0.8636294400600522, 0.8605381872342981, 0.8639439845938817, 0.8651720186328644, 0.8640897882259105, 0.8593536011703179, 0.8628457726724922, 0.8626753782372341, 0.85943344212554, 0.8582833653215862, 0.8661228281152827, 0.8631466300896061, 0.8603345503282669, 0.8608983078271227, 0.8608649836476806, 0.8641511102771515, 0.8619383021693705, 0.8627824847350645, 0.8620822893079284, 0.8600087790842861, 0.8618966651999432, 0.8635473242196281, 0.8623343347893346, 0.8631306728133765, 0.8632371360078797, 0.8616637081441367, 0.8600093018063499, 0.8641573766918134, 0.8610520731762547, 0.8624536100860751, 0.8578021175721112, 0.858304521769209, 0.8613175434224746, 0.8632953596846832, 0.8631462098082618, 0.8611974688747045, 0.8635908728060515, 0.8646136999435132, 0.8631044036287177, 0.8591404045024491, 0.8590020694391197, 0.8629701910421367, 0.8617894820240147, 0.858717587140515, 0.8644648983960261, 0.8616187281315894, 0.8598749369306637, 0.8583811896536356, 0.8623257000428026, 0.8606019989608804, 0.8625650944002449, 0.863682014710458, 0.8613809339530633, 0.863688009016959, 0.8655878517328931, 0.8618918919502316, 0.8595146224321917, 0.861683716554471, 0.8630580818256759, 0.8587543007053072, 0.8623482299887616, 0.8598921181600722, 0.8629929591017915, 0.8615356387994478, 0.8624651104288028, 0.8610260991184303, 0.8620591477664841, 0.8672795405473246, 0.8597018180600823, 0.862407914665349, 0.8640978746401989, 0.8637187229397961, 0.8593866389120937, 0.8606867907602159, 0.861912904950359, 0.8601518131582938, 0.8626725153849862, 0.8608165342179711, 0.8650407160029692, 0.8589894571877501, 0.8614963352527765, 0.8661257411207994, 0.8643890415006281, 0.8612822341492109, 0.8652370384587046, 0.861564193235334, 0.8597204212642386, 0.8595951655331779, 0.8642077926174759, 0.8625301049493462, 0.8621452551363679, 0.8574543166953279, 0.8637184724783349, 0.8625698601803207, 0.8629207278761413, 0.861101661801643, 0.8614158677627973, 0.8650197406559039, 0.861035448968258, 0.8624908594829043, 0.859765787716107, 0.8624684963079975, 0.8627989414098013, 0.8611297086071785, 0.8618908094628083, 0.8610382793504564, 0.8650509408672752, 0.8603544008091587, 0.8634249337798799, 0.8631864903528063, 0.8672424422200683, 0.8625858205053812, 0.8610268433380615, 0.8619048941470778]\n",
      "Starting run 4/5\n",
      "Run 4, Epoch 1, Loss: 1.684, Training Accuracy: 38.98%\n",
      "Run 4, Epoch 2, Loss: 1.428, Training Accuracy: 48.37%\n",
      "Run 4, Epoch 3, Loss: 1.309, Training Accuracy: 52.79%\n",
      "Run 4, Epoch 4, Loss: 1.216, Training Accuracy: 56.48%\n",
      "Run 4, Epoch 5, Loss: 1.153, Training Accuracy: 58.67%\n",
      "Run 4, Epoch 6, Loss: 1.102, Training Accuracy: 60.73%\n",
      "Run 4, Epoch 7, Loss: 1.059, Training Accuracy: 62.47%\n",
      "Run 4, Epoch 8, Loss: 1.037, Training Accuracy: 63.06%\n",
      "Run 4, Epoch 9, Loss: 0.999, Training Accuracy: 64.55%\n",
      "Run 4, Epoch 10, Loss: 0.977, Training Accuracy: 65.44%\n",
      "Run 4, Epoch 11, Loss: 0.913, Training Accuracy: 67.63%\n",
      "Run 4, Epoch 12, Loss: 0.899, Training Accuracy: 68.49%\n",
      "Run 4, Epoch 13, Loss: 0.893, Training Accuracy: 68.54%\n",
      "Run 4, Epoch 14, Loss: 0.891, Training Accuracy: 68.65%\n",
      "Run 4, Epoch 15, Loss: 0.883, Training Accuracy: 68.76%\n",
      "Run 4, Epoch 16, Loss: 0.877, Training Accuracy: 68.96%\n",
      "Run 4, Epoch 17, Loss: 0.876, Training Accuracy: 69.15%\n",
      "Run 4, Epoch 18, Loss: 0.874, Training Accuracy: 69.40%\n",
      "Run 4, Epoch 19, Loss: 0.867, Training Accuracy: 69.33%\n",
      "Run 4, Epoch 20, Loss: 0.870, Training Accuracy: 69.37%\n",
      "Run 4, Epoch 21, Loss: 0.860, Training Accuracy: 69.55%\n",
      "Run 4, Epoch 22, Loss: 0.857, Training Accuracy: 69.77%\n",
      "Run 4, Epoch 23, Loss: 0.859, Training Accuracy: 69.40%\n",
      "Run 4, Epoch 24, Loss: 0.859, Training Accuracy: 69.77%\n",
      "Run 4, Epoch 25, Loss: 0.855, Training Accuracy: 69.87%\n",
      "Run 4, Epoch 26, Loss: 0.861, Training Accuracy: 69.66%\n",
      "Run 4, Epoch 27, Loss: 0.856, Training Accuracy: 69.85%\n",
      "Run 4, Epoch 28, Loss: 0.855, Training Accuracy: 70.03%\n",
      "Run 4, Epoch 29, Loss: 0.857, Training Accuracy: 69.89%\n",
      "Run 4, Epoch 30, Loss: 0.857, Training Accuracy: 69.81%\n",
      "Run 4, Epoch 31, Loss: 0.852, Training Accuracy: 69.82%\n",
      "Run 4, Epoch 32, Loss: 0.855, Training Accuracy: 69.79%\n",
      "Run 4, Epoch 33, Loss: 0.859, Training Accuracy: 69.71%\n",
      "Run 4, Epoch 34, Loss: 0.855, Training Accuracy: 69.97%\n",
      "Run 4, Epoch 35, Loss: 0.853, Training Accuracy: 69.89%\n",
      "Run 4, Epoch 36, Loss: 0.854, Training Accuracy: 69.89%\n",
      "Run 4, Epoch 37, Loss: 0.854, Training Accuracy: 69.94%\n",
      "Run 4, Epoch 38, Loss: 0.852, Training Accuracy: 69.82%\n",
      "Run 4, Epoch 39, Loss: 0.854, Training Accuracy: 70.04%\n",
      "Run 4, Epoch 40, Loss: 0.852, Training Accuracy: 69.88%\n",
      "Run 4, Epoch 41, Loss: 0.854, Training Accuracy: 69.94%\n",
      "Run 4, Epoch 42, Loss: 0.856, Training Accuracy: 69.68%\n",
      "Run 4, Epoch 43, Loss: 0.851, Training Accuracy: 70.05%\n",
      "Run 4, Epoch 44, Loss: 0.851, Training Accuracy: 70.04%\n",
      "Run 4, Epoch 45, Loss: 0.854, Training Accuracy: 69.71%\n",
      "Run 4, Epoch 46, Loss: 0.853, Training Accuracy: 69.77%\n",
      "Run 4, Epoch 47, Loss: 0.850, Training Accuracy: 69.79%\n",
      "Run 4, Epoch 48, Loss: 0.853, Training Accuracy: 69.90%\n",
      "Run 4, Epoch 49, Loss: 0.850, Training Accuracy: 70.16%\n",
      "Run 4, Epoch 50, Loss: 0.855, Training Accuracy: 70.00%\n",
      "Run 4, Epoch 51, Loss: 0.853, Training Accuracy: 69.92%\n",
      "Run 4, Epoch 52, Loss: 0.852, Training Accuracy: 70.07%\n",
      "Run 4, Epoch 53, Loss: 0.854, Training Accuracy: 69.87%\n",
      "Run 4, Epoch 54, Loss: 0.856, Training Accuracy: 69.91%\n",
      "Run 4, Epoch 55, Loss: 0.851, Training Accuracy: 69.86%\n",
      "Run 4, Epoch 56, Loss: 0.853, Training Accuracy: 69.83%\n",
      "Run 4, Epoch 57, Loss: 0.850, Training Accuracy: 70.04%\n",
      "Run 4, Epoch 58, Loss: 0.853, Training Accuracy: 69.87%\n",
      "Run 4, Epoch 59, Loss: 0.856, Training Accuracy: 69.84%\n",
      "Run 4, Epoch 60, Loss: 0.855, Training Accuracy: 70.11%\n",
      "Run 4, Epoch 61, Loss: 0.853, Training Accuracy: 69.87%\n",
      "Run 4, Epoch 62, Loss: 0.852, Training Accuracy: 69.83%\n",
      "Run 4, Epoch 63, Loss: 0.851, Training Accuracy: 70.16%\n",
      "Run 4, Epoch 64, Loss: 0.856, Training Accuracy: 69.78%\n",
      "Run 4, Epoch 65, Loss: 0.854, Training Accuracy: 69.99%\n",
      "Run 4, Epoch 66, Loss: 0.855, Training Accuracy: 69.83%\n",
      "Run 4, Epoch 67, Loss: 0.853, Training Accuracy: 70.03%\n",
      "Run 4, Epoch 68, Loss: 0.853, Training Accuracy: 69.84%\n",
      "Run 4, Epoch 69, Loss: 0.851, Training Accuracy: 70.22%\n",
      "Run 4, Epoch 70, Loss: 0.850, Training Accuracy: 70.02%\n",
      "Run 4, Epoch 71, Loss: 0.854, Training Accuracy: 69.92%\n",
      "Run 4, Epoch 72, Loss: 0.852, Training Accuracy: 70.08%\n",
      "Run 4, Epoch 73, Loss: 0.853, Training Accuracy: 69.96%\n",
      "Run 4, Epoch 74, Loss: 0.855, Training Accuracy: 69.56%\n",
      "Run 4, Epoch 75, Loss: 0.856, Training Accuracy: 69.77%\n",
      "Run 4, Epoch 76, Loss: 0.855, Training Accuracy: 69.80%\n",
      "Run 4, Epoch 77, Loss: 0.853, Training Accuracy: 69.98%\n",
      "Run 4, Epoch 78, Loss: 0.852, Training Accuracy: 70.10%\n",
      "Run 4, Epoch 79, Loss: 0.851, Training Accuracy: 70.02%\n",
      "Run 4, Epoch 80, Loss: 0.853, Training Accuracy: 69.87%\n",
      "Run 4, Epoch 81, Loss: 0.853, Training Accuracy: 69.89%\n",
      "Run 4, Epoch 82, Loss: 0.856, Training Accuracy: 69.60%\n",
      "Run 4, Epoch 83, Loss: 0.852, Training Accuracy: 70.09%\n",
      "Run 4, Epoch 84, Loss: 0.849, Training Accuracy: 70.23%\n",
      "Run 4, Epoch 85, Loss: 0.851, Training Accuracy: 69.98%\n",
      "Run 4, Epoch 86, Loss: 0.853, Training Accuracy: 70.14%\n",
      "Run 4, Epoch 87, Loss: 0.852, Training Accuracy: 70.05%\n",
      "Run 4, Epoch 88, Loss: 0.851, Training Accuracy: 70.12%\n",
      "Run 4, Epoch 89, Loss: 0.855, Training Accuracy: 69.78%\n",
      "Run 4, Epoch 90, Loss: 0.850, Training Accuracy: 69.87%\n",
      "Run 4, Epoch 91, Loss: 0.853, Training Accuracy: 70.14%\n",
      "Run 4, Epoch 92, Loss: 0.856, Training Accuracy: 69.65%\n",
      "Run 4, Epoch 93, Loss: 0.852, Training Accuracy: 69.91%\n",
      "Run 4, Epoch 94, Loss: 0.856, Training Accuracy: 69.79%\n",
      "Run 4, Epoch 95, Loss: 0.856, Training Accuracy: 70.00%\n",
      "Run 4, Epoch 96, Loss: 0.853, Training Accuracy: 69.86%\n",
      "Run 4, Epoch 97, Loss: 0.852, Training Accuracy: 70.01%\n",
      "Run 4, Epoch 98, Loss: 0.853, Training Accuracy: 70.12%\n",
      "Run 4, Epoch 99, Loss: 0.853, Training Accuracy: 69.96%\n",
      "Run 4, Epoch 100, Loss: 0.853, Training Accuracy: 70.10%\n",
      "Run 4, Epoch 101, Loss: 0.854, Training Accuracy: 69.89%\n",
      "Run 4, Epoch 102, Loss: 0.854, Training Accuracy: 69.77%\n",
      "Run 4, Epoch 103, Loss: 0.851, Training Accuracy: 70.07%\n",
      "Run 4, Epoch 104, Loss: 0.852, Training Accuracy: 69.92%\n",
      "Run 4, Epoch 105, Loss: 0.848, Training Accuracy: 70.05%\n",
      "Run 4, Epoch 106, Loss: 0.852, Training Accuracy: 69.83%\n",
      "Run 4, Epoch 107, Loss: 0.854, Training Accuracy: 69.82%\n",
      "Run 4, Epoch 108, Loss: 0.853, Training Accuracy: 69.94%\n",
      "Run 4, Epoch 109, Loss: 0.853, Training Accuracy: 70.13%\n",
      "Run 4, Epoch 110, Loss: 0.853, Training Accuracy: 69.89%\n",
      "Run 4, Epoch 111, Loss: 0.854, Training Accuracy: 70.14%\n",
      "Run 4, Epoch 112, Loss: 0.856, Training Accuracy: 69.78%\n",
      "Run 4, Epoch 113, Loss: 0.854, Training Accuracy: 70.01%\n",
      "Run 4, Epoch 114, Loss: 0.856, Training Accuracy: 69.85%\n",
      "Run 4, Epoch 115, Loss: 0.852, Training Accuracy: 69.86%\n",
      "Run 4, Epoch 116, Loss: 0.855, Training Accuracy: 70.00%\n",
      "Run 4, Epoch 117, Loss: 0.854, Training Accuracy: 69.87%\n",
      "Run 4, Epoch 118, Loss: 0.853, Training Accuracy: 69.95%\n",
      "Run 4, Epoch 119, Loss: 0.854, Training Accuracy: 69.65%\n",
      "Run 4, Epoch 120, Loss: 0.850, Training Accuracy: 69.86%\n",
      "Run 4, Epoch 121, Loss: 0.851, Training Accuracy: 69.92%\n",
      "Run 4, Epoch 122, Loss: 0.853, Training Accuracy: 69.77%\n",
      "Run 4, Epoch 123, Loss: 0.855, Training Accuracy: 69.81%\n",
      "Run 4, Epoch 124, Loss: 0.851, Training Accuracy: 70.01%\n",
      "Run 4, Epoch 125, Loss: 0.849, Training Accuracy: 69.99%\n",
      "Run 4, Epoch 126, Loss: 0.853, Training Accuracy: 69.83%\n",
      "Run 4, Epoch 127, Loss: 0.849, Training Accuracy: 70.00%\n",
      "Run 4, Epoch 128, Loss: 0.858, Training Accuracy: 69.79%\n",
      "Run 4, Epoch 129, Loss: 0.855, Training Accuracy: 70.04%\n",
      "Run 4, Epoch 130, Loss: 0.850, Training Accuracy: 70.08%\n",
      "Run 4, Epoch 131, Loss: 0.851, Training Accuracy: 70.01%\n",
      "Run 4, Epoch 132, Loss: 0.855, Training Accuracy: 69.89%\n",
      "Run 4, Epoch 133, Loss: 0.855, Training Accuracy: 69.99%\n",
      "Run 4, Epoch 134, Loss: 0.855, Training Accuracy: 69.85%\n",
      "Run 4, Epoch 135, Loss: 0.853, Training Accuracy: 70.05%\n",
      "Run 4, Epoch 136, Loss: 0.853, Training Accuracy: 70.02%\n",
      "Run 4, Epoch 137, Loss: 0.854, Training Accuracy: 69.95%\n",
      "Run 4, Epoch 138, Loss: 0.852, Training Accuracy: 70.01%\n",
      "Run 4, Epoch 139, Loss: 0.853, Training Accuracy: 69.96%\n",
      "Run 4, Epoch 140, Loss: 0.851, Training Accuracy: 69.99%\n",
      "Run 4, Epoch 141, Loss: 0.852, Training Accuracy: 69.84%\n",
      "Run 4, Epoch 142, Loss: 0.853, Training Accuracy: 69.96%\n",
      "Run 4, Epoch 143, Loss: 0.854, Training Accuracy: 69.69%\n",
      "Run 4, Epoch 144, Loss: 0.850, Training Accuracy: 69.92%\n",
      "Run 4, Epoch 145, Loss: 0.850, Training Accuracy: 70.16%\n",
      "Run 4, Epoch 146, Loss: 0.856, Training Accuracy: 69.70%\n",
      "Run 4, Epoch 147, Loss: 0.853, Training Accuracy: 69.93%\n",
      "Run 4, Epoch 148, Loss: 0.854, Training Accuracy: 69.83%\n",
      "Run 4, Epoch 149, Loss: 0.856, Training Accuracy: 69.99%\n",
      "Run 4, Epoch 150, Loss: 0.856, Training Accuracy: 69.98%\n",
      "Run 4, Epoch 151, Loss: 0.851, Training Accuracy: 69.99%\n",
      "Run 4, Epoch 152, Loss: 0.858, Training Accuracy: 69.75%\n",
      "Run 4, Epoch 153, Loss: 0.860, Training Accuracy: 69.70%\n",
      "Run 4, Epoch 154, Loss: 0.860, Training Accuracy: 69.75%\n",
      "Run 4, Epoch 155, Loss: 0.854, Training Accuracy: 70.01%\n",
      "Run 4, Epoch 156, Loss: 0.856, Training Accuracy: 69.80%\n",
      "Run 4, Epoch 157, Loss: 0.857, Training Accuracy: 69.65%\n",
      "Run 4, Epoch 158, Loss: 0.856, Training Accuracy: 70.08%\n",
      "Run 4, Epoch 159, Loss: 0.850, Training Accuracy: 70.02%\n",
      "Run 4, Epoch 160, Loss: 0.852, Training Accuracy: 70.06%\n",
      "Run 4, Epoch 161, Loss: 0.855, Training Accuracy: 69.89%\n",
      "Run 4, Epoch 162, Loss: 0.851, Training Accuracy: 70.10%\n",
      "Run 4, Epoch 163, Loss: 0.853, Training Accuracy: 70.11%\n",
      "Run 4, Epoch 164, Loss: 0.853, Training Accuracy: 69.81%\n",
      "Run 4, Epoch 165, Loss: 0.854, Training Accuracy: 69.85%\n",
      "Run 4, Epoch 166, Loss: 0.852, Training Accuracy: 69.94%\n",
      "Run 4, Epoch 167, Loss: 0.849, Training Accuracy: 70.03%\n",
      "Run 4, Epoch 168, Loss: 0.855, Training Accuracy: 69.91%\n",
      "Run 4, Epoch 169, Loss: 0.850, Training Accuracy: 69.96%\n",
      "Run 4, Epoch 170, Loss: 0.856, Training Accuracy: 70.01%\n",
      "Run 4, Epoch 171, Loss: 0.852, Training Accuracy: 69.94%\n",
      "Run 4, Epoch 172, Loss: 0.855, Training Accuracy: 70.01%\n",
      "Run 4, Epoch 173, Loss: 0.853, Training Accuracy: 69.84%\n",
      "Run 4, Epoch 174, Loss: 0.851, Training Accuracy: 69.99%\n",
      "Run 4, Epoch 175, Loss: 0.853, Training Accuracy: 69.98%\n",
      "Run 4, Final Accuracy on test set: 72.17%\n",
      "Results after run 4:\n",
      "Training Accuracies: [38.978, 48.372, 52.788, 56.476, 58.666, 60.728, 62.474, 63.056, 64.55, 65.436, 67.628, 68.486, 68.544, 68.654, 68.762, 68.958, 69.154, 69.4, 69.328, 69.374, 69.552, 69.768, 69.4, 69.772, 69.866, 69.664, 69.85, 70.028, 69.894, 69.814, 69.82, 69.79, 69.706, 69.968, 69.888, 69.888, 69.936, 69.818, 70.044, 69.884, 69.938, 69.682, 70.048, 70.038, 69.712, 69.77, 69.794, 69.896, 70.156, 70.004, 69.916, 70.068, 69.868, 69.908, 69.858, 69.832, 70.042, 69.87, 69.84, 70.112, 69.874, 69.83, 70.162, 69.778, 69.994, 69.826, 70.028, 69.838, 70.216, 70.022, 69.924, 70.078, 69.962, 69.564, 69.77, 69.804, 69.98, 70.1, 70.02, 69.87, 69.892, 69.6, 70.088, 70.228, 69.98, 70.142, 70.046, 70.122, 69.778, 69.874, 70.142, 69.652, 69.91, 69.786, 70.0, 69.862, 70.008, 70.116, 69.962, 70.096, 69.886, 69.766, 70.072, 69.924, 70.046, 69.826, 69.816, 69.944, 70.134, 69.894, 70.144, 69.782, 70.014, 69.846, 69.864, 69.998, 69.874, 69.954, 69.646, 69.864, 69.922, 69.766, 69.806, 70.014, 69.988, 69.834, 70.0, 69.788, 70.044, 70.076, 70.006, 69.894, 69.99, 69.854, 70.054, 70.02, 69.948, 70.014, 69.958, 69.986, 69.836, 69.964, 69.694, 69.918, 70.16, 69.704, 69.932, 69.828, 69.988, 69.982, 69.988, 69.754, 69.7, 69.752, 70.01, 69.796, 69.652, 70.084, 70.024, 70.058, 69.888, 70.1, 70.108, 69.81, 69.85, 69.944, 70.03, 69.906, 69.962, 70.008, 69.936, 70.012, 69.842, 69.992, 69.98]\n",
      "Test Accuracy: 72.17%\n",
      "Losses: [1.6836820825591416, 1.4275458157824739, 1.3092173075736941, 1.216274984657307, 1.1526965730635406, 1.1024881232425074, 1.0593524189556347, 1.0367839113830606, 0.9988372693281344, 0.9772699886880567, 0.913359282724083, 0.8985396940689867, 0.8929477091640463, 0.8905538825122902, 0.8834803770570194, 0.8765016024374901, 0.8762293194261048, 0.8739776301871786, 0.8666706437345051, 0.870143566290131, 0.8601798936534111, 0.8571596112092743, 0.8593336497731221, 0.8586754013815194, 0.855153147521836, 0.8611562140762349, 0.8555229970866152, 0.8545249910915599, 0.8573243349714352, 0.8572626780061161, 0.8521875075976867, 0.8554074976145459, 0.8587489288176418, 0.8546096467605943, 0.8533827534417058, 0.853820242845189, 0.8540278725002123, 0.8523604221965956, 0.8539134154234396, 0.8524207396580435, 0.8540267828480362, 0.8563759095223663, 0.8513425326408328, 0.8513661889773806, 0.8544281439098251, 0.8528336134103253, 0.8504233110286391, 0.8533528695630905, 0.8495222781320362, 0.8547855312257167, 0.8532476161447022, 0.852295458316803, 0.8538792607424509, 0.855915802823918, 0.8513670923459865, 0.8534834517542359, 0.8501274771702564, 0.8531537667259841, 0.8556280802277958, 0.8546913487222189, 0.853376335652588, 0.8522089481963526, 0.8510533184041758, 0.8561634775012961, 0.8536290419680993, 0.8548498774123618, 0.8534451866393808, 0.8529548500200062, 0.8512760063876277, 0.8496823637076961, 0.8541473112142909, 0.8523681813188831, 0.8533604404200679, 0.8546607643746964, 0.8559996075642383, 0.8554742317980207, 0.8527344332631591, 0.8515467636115716, 0.8506790090095052, 0.8534108600043275, 0.8533441718581998, 0.8564790494911506, 0.8519871244040291, 0.8494463174239449, 0.8507582825772902, 0.8534421120458247, 0.851568598881402, 0.8506114996607651, 0.8552786255126719, 0.8498177403379279, 0.8526894142255759, 0.855511494914589, 0.8521910950046061, 0.855672821821764, 0.8563066164550879, 0.8527196395732558, 0.8521340577803609, 0.8528099217073387, 0.8534661694560819, 0.8534563278298244, 0.8544602982528374, 0.8539370966079595, 0.8509983537752001, 0.8516112296172725, 0.8482022863214888, 0.8519709623988022, 0.8535068105248844, 0.852933788543467, 0.8533101706858486, 0.8525979040223924, 0.8542142165896228, 0.8560319093182264, 0.8536140613848596, 0.855877483897197, 0.8517925963377404, 0.8547456095285733, 0.8535770424796493, 0.8533263197335441, 0.8539799741466941, 0.8498025272813294, 0.8511106896278499, 0.8532374490557424, 0.8545617763038791, 0.8514389314919787, 0.8486551438146235, 0.8534689492276867, 0.8488721748447174, 0.8579081235944158, 0.8545042550777231, 0.8503936059639582, 0.8512323533787447, 0.8546708220106256, 0.8547471679385056, 0.8546222417860689, 0.8527804718298071, 0.8527336340121296, 0.8542703525794436, 0.8520719168131309, 0.8529742873843064, 0.8514460817627285, 0.8522281568983326, 0.852724170440908, 0.854469725542971, 0.8504790627133206, 0.8501166571741519, 0.8557362234805856, 0.8533967875153817, 0.8539568358065223, 0.8559271438652293, 0.8558452269610237, 0.8508841575259138, 0.8577347400853091, 0.8602963522876925, 0.8598080353663705, 0.8543050299824961, 0.8561486960067164, 0.8573968457748823, 0.8555077602491354, 0.8504179351774933, 0.8518856096145747, 0.8552835967839526, 0.8514088108716413, 0.8528821085724989, 0.8531344068019896, 0.8539813101444098, 0.8517964976218045, 0.8494067434459696, 0.8551561518398392, 0.8498148838882251, 0.8556192311484491, 0.8521185056937625, 0.8551779739996966, 0.8531002534929749, 0.8507096951879809, 0.8532810221852549]\n",
      "Starting run 5/5\n",
      "Run 5, Epoch 1, Loss: 1.699, Training Accuracy: 37.80%\n",
      "Run 5, Epoch 2, Loss: 1.421, Training Accuracy: 48.63%\n",
      "Run 5, Epoch 3, Loss: 1.294, Training Accuracy: 53.50%\n",
      "Run 5, Epoch 4, Loss: 1.204, Training Accuracy: 56.93%\n",
      "Run 5, Epoch 5, Loss: 1.138, Training Accuracy: 59.58%\n",
      "Run 5, Epoch 6, Loss: 1.081, Training Accuracy: 61.53%\n",
      "Run 5, Epoch 7, Loss: 1.043, Training Accuracy: 62.97%\n",
      "Run 5, Epoch 8, Loss: 1.009, Training Accuracy: 64.39%\n",
      "Run 5, Epoch 9, Loss: 0.989, Training Accuracy: 64.95%\n",
      "Run 5, Epoch 10, Loss: 0.961, Training Accuracy: 66.08%\n",
      "Run 5, Epoch 11, Loss: 0.902, Training Accuracy: 68.23%\n",
      "Run 5, Epoch 12, Loss: 0.886, Training Accuracy: 68.59%\n",
      "Run 5, Epoch 13, Loss: 0.879, Training Accuracy: 68.87%\n",
      "Run 5, Epoch 14, Loss: 0.875, Training Accuracy: 69.08%\n",
      "Run 5, Epoch 15, Loss: 0.870, Training Accuracy: 69.20%\n",
      "Run 5, Epoch 16, Loss: 0.870, Training Accuracy: 69.30%\n",
      "Run 5, Epoch 17, Loss: 0.862, Training Accuracy: 69.48%\n",
      "Run 5, Epoch 18, Loss: 0.859, Training Accuracy: 69.62%\n",
      "Run 5, Epoch 19, Loss: 0.858, Training Accuracy: 69.77%\n",
      "Run 5, Epoch 20, Loss: 0.852, Training Accuracy: 70.05%\n",
      "Run 5, Epoch 21, Loss: 0.848, Training Accuracy: 70.00%\n",
      "Run 5, Epoch 22, Loss: 0.850, Training Accuracy: 70.16%\n",
      "Run 5, Epoch 23, Loss: 0.847, Training Accuracy: 70.31%\n",
      "Run 5, Epoch 24, Loss: 0.842, Training Accuracy: 70.28%\n",
      "Run 5, Epoch 25, Loss: 0.842, Training Accuracy: 70.21%\n",
      "Run 5, Epoch 26, Loss: 0.842, Training Accuracy: 70.40%\n",
      "Run 5, Epoch 27, Loss: 0.844, Training Accuracy: 70.09%\n",
      "Run 5, Epoch 28, Loss: 0.844, Training Accuracy: 70.10%\n",
      "Run 5, Epoch 29, Loss: 0.842, Training Accuracy: 70.20%\n",
      "Run 5, Epoch 30, Loss: 0.842, Training Accuracy: 70.48%\n",
      "Run 5, Epoch 31, Loss: 0.845, Training Accuracy: 70.29%\n",
      "Run 5, Epoch 32, Loss: 0.837, Training Accuracy: 70.58%\n",
      "Run 5, Epoch 33, Loss: 0.841, Training Accuracy: 70.31%\n",
      "Run 5, Epoch 34, Loss: 0.841, Training Accuracy: 70.34%\n",
      "Run 5, Epoch 35, Loss: 0.839, Training Accuracy: 70.29%\n",
      "Run 5, Epoch 36, Loss: 0.840, Training Accuracy: 70.44%\n",
      "Run 5, Epoch 37, Loss: 0.839, Training Accuracy: 70.41%\n",
      "Run 5, Epoch 38, Loss: 0.838, Training Accuracy: 70.47%\n",
      "Run 5, Epoch 39, Loss: 0.842, Training Accuracy: 70.42%\n",
      "Run 5, Epoch 40, Loss: 0.842, Training Accuracy: 70.29%\n",
      "Run 5, Epoch 41, Loss: 0.843, Training Accuracy: 70.22%\n",
      "Run 5, Epoch 42, Loss: 0.841, Training Accuracy: 70.43%\n",
      "Run 5, Epoch 43, Loss: 0.841, Training Accuracy: 70.31%\n",
      "Run 5, Epoch 44, Loss: 0.840, Training Accuracy: 70.27%\n",
      "Run 5, Epoch 45, Loss: 0.838, Training Accuracy: 70.46%\n",
      "Run 5, Epoch 46, Loss: 0.842, Training Accuracy: 70.24%\n",
      "Run 5, Epoch 47, Loss: 0.837, Training Accuracy: 70.63%\n",
      "Run 5, Epoch 48, Loss: 0.844, Training Accuracy: 70.27%\n",
      "Run 5, Epoch 49, Loss: 0.841, Training Accuracy: 70.36%\n",
      "Run 5, Epoch 50, Loss: 0.840, Training Accuracy: 70.26%\n",
      "Run 5, Epoch 51, Loss: 0.838, Training Accuracy: 70.43%\n",
      "Run 5, Epoch 52, Loss: 0.842, Training Accuracy: 70.40%\n",
      "Run 5, Epoch 53, Loss: 0.839, Training Accuracy: 70.41%\n",
      "Run 5, Epoch 54, Loss: 0.840, Training Accuracy: 70.41%\n",
      "Run 5, Epoch 55, Loss: 0.838, Training Accuracy: 70.51%\n",
      "Run 5, Epoch 56, Loss: 0.842, Training Accuracy: 70.29%\n",
      "Run 5, Epoch 57, Loss: 0.842, Training Accuracy: 70.22%\n",
      "Run 5, Epoch 58, Loss: 0.841, Training Accuracy: 70.36%\n",
      "Run 5, Epoch 59, Loss: 0.838, Training Accuracy: 70.50%\n",
      "Run 5, Epoch 60, Loss: 0.841, Training Accuracy: 70.51%\n",
      "Run 5, Epoch 61, Loss: 0.841, Training Accuracy: 70.38%\n",
      "Run 5, Epoch 62, Loss: 0.842, Training Accuracy: 70.24%\n",
      "Run 5, Epoch 63, Loss: 0.838, Training Accuracy: 70.40%\n",
      "Run 5, Epoch 64, Loss: 0.835, Training Accuracy: 70.59%\n",
      "Run 5, Epoch 65, Loss: 0.841, Training Accuracy: 70.54%\n",
      "Run 5, Epoch 66, Loss: 0.841, Training Accuracy: 70.35%\n",
      "Run 5, Epoch 67, Loss: 0.837, Training Accuracy: 70.42%\n",
      "Run 5, Epoch 68, Loss: 0.844, Training Accuracy: 70.04%\n",
      "Run 5, Epoch 69, Loss: 0.839, Training Accuracy: 70.38%\n",
      "Run 5, Epoch 70, Loss: 0.847, Training Accuracy: 69.89%\n",
      "Run 5, Epoch 71, Loss: 0.839, Training Accuracy: 70.21%\n",
      "Run 5, Epoch 72, Loss: 0.836, Training Accuracy: 70.40%\n",
      "Run 5, Epoch 73, Loss: 0.837, Training Accuracy: 70.53%\n",
      "Run 5, Epoch 74, Loss: 0.840, Training Accuracy: 70.38%\n",
      "Run 5, Epoch 75, Loss: 0.841, Training Accuracy: 70.42%\n",
      "Run 5, Epoch 76, Loss: 0.840, Training Accuracy: 70.30%\n",
      "Run 5, Epoch 77, Loss: 0.841, Training Accuracy: 70.33%\n",
      "Run 5, Epoch 78, Loss: 0.842, Training Accuracy: 70.31%\n",
      "Run 5, Epoch 79, Loss: 0.843, Training Accuracy: 70.07%\n",
      "Run 5, Epoch 80, Loss: 0.839, Training Accuracy: 70.47%\n",
      "Run 5, Epoch 81, Loss: 0.839, Training Accuracy: 70.40%\n",
      "Run 5, Epoch 82, Loss: 0.840, Training Accuracy: 70.77%\n",
      "Run 5, Epoch 83, Loss: 0.844, Training Accuracy: 70.19%\n",
      "Run 5, Epoch 84, Loss: 0.841, Training Accuracy: 70.55%\n",
      "Run 5, Epoch 85, Loss: 0.838, Training Accuracy: 70.44%\n",
      "Run 5, Epoch 86, Loss: 0.840, Training Accuracy: 70.33%\n",
      "Run 5, Epoch 87, Loss: 0.843, Training Accuracy: 70.36%\n",
      "Run 5, Epoch 88, Loss: 0.846, Training Accuracy: 70.23%\n",
      "Run 5, Epoch 89, Loss: 0.840, Training Accuracy: 70.35%\n",
      "Run 5, Epoch 90, Loss: 0.841, Training Accuracy: 70.46%\n",
      "Run 5, Epoch 91, Loss: 0.844, Training Accuracy: 70.21%\n",
      "Run 5, Epoch 92, Loss: 0.840, Training Accuracy: 70.56%\n",
      "Run 5, Epoch 93, Loss: 0.840, Training Accuracy: 70.46%\n",
      "Run 5, Epoch 94, Loss: 0.843, Training Accuracy: 70.23%\n",
      "Run 5, Epoch 95, Loss: 0.840, Training Accuracy: 70.32%\n",
      "Run 5, Epoch 96, Loss: 0.843, Training Accuracy: 70.15%\n",
      "Run 5, Epoch 97, Loss: 0.841, Training Accuracy: 70.23%\n",
      "Run 5, Epoch 98, Loss: 0.844, Training Accuracy: 70.32%\n",
      "Run 5, Epoch 99, Loss: 0.838, Training Accuracy: 70.47%\n",
      "Run 5, Epoch 100, Loss: 0.842, Training Accuracy: 70.38%\n",
      "Run 5, Epoch 101, Loss: 0.840, Training Accuracy: 70.32%\n",
      "Run 5, Epoch 102, Loss: 0.841, Training Accuracy: 70.63%\n",
      "Run 5, Epoch 103, Loss: 0.841, Training Accuracy: 70.26%\n",
      "Run 5, Epoch 104, Loss: 0.842, Training Accuracy: 70.11%\n",
      "Run 5, Epoch 105, Loss: 0.843, Training Accuracy: 70.36%\n",
      "Run 5, Epoch 106, Loss: 0.840, Training Accuracy: 70.39%\n",
      "Run 5, Epoch 107, Loss: 0.837, Training Accuracy: 70.62%\n",
      "Run 5, Epoch 108, Loss: 0.842, Training Accuracy: 70.32%\n",
      "Run 5, Epoch 109, Loss: 0.837, Training Accuracy: 70.54%\n",
      "Run 5, Epoch 110, Loss: 0.838, Training Accuracy: 70.37%\n",
      "Run 5, Epoch 111, Loss: 0.843, Training Accuracy: 70.15%\n",
      "Run 5, Epoch 112, Loss: 0.838, Training Accuracy: 70.49%\n",
      "Run 5, Epoch 113, Loss: 0.842, Training Accuracy: 70.32%\n",
      "Run 5, Epoch 114, Loss: 0.841, Training Accuracy: 70.36%\n",
      "Run 5, Epoch 115, Loss: 0.842, Training Accuracy: 70.28%\n",
      "Run 5, Epoch 116, Loss: 0.841, Training Accuracy: 70.19%\n",
      "Run 5, Epoch 117, Loss: 0.841, Training Accuracy: 70.46%\n",
      "Run 5, Epoch 118, Loss: 0.839, Training Accuracy: 70.56%\n",
      "Run 5, Epoch 119, Loss: 0.842, Training Accuracy: 70.36%\n",
      "Run 5, Epoch 120, Loss: 0.843, Training Accuracy: 70.34%\n",
      "Run 5, Epoch 121, Loss: 0.840, Training Accuracy: 70.44%\n",
      "Run 5, Epoch 122, Loss: 0.841, Training Accuracy: 70.24%\n",
      "Run 5, Epoch 123, Loss: 0.839, Training Accuracy: 70.43%\n",
      "Run 5, Epoch 124, Loss: 0.843, Training Accuracy: 70.25%\n",
      "Run 5, Epoch 125, Loss: 0.843, Training Accuracy: 70.28%\n",
      "Run 5, Epoch 126, Loss: 0.837, Training Accuracy: 70.48%\n",
      "Run 5, Epoch 127, Loss: 0.843, Training Accuracy: 70.35%\n",
      "Run 5, Epoch 128, Loss: 0.839, Training Accuracy: 70.45%\n",
      "Run 5, Epoch 129, Loss: 0.840, Training Accuracy: 70.14%\n",
      "Run 5, Epoch 130, Loss: 0.840, Training Accuracy: 70.19%\n",
      "Run 5, Epoch 131, Loss: 0.841, Training Accuracy: 70.34%\n",
      "Run 5, Epoch 132, Loss: 0.839, Training Accuracy: 70.46%\n",
      "Run 5, Epoch 133, Loss: 0.840, Training Accuracy: 70.37%\n",
      "Run 5, Epoch 134, Loss: 0.837, Training Accuracy: 70.44%\n",
      "Run 5, Epoch 135, Loss: 0.839, Training Accuracy: 70.67%\n",
      "Run 5, Epoch 136, Loss: 0.842, Training Accuracy: 70.29%\n",
      "Run 5, Epoch 137, Loss: 0.839, Training Accuracy: 70.43%\n",
      "Run 5, Epoch 138, Loss: 0.840, Training Accuracy: 70.24%\n",
      "Run 5, Epoch 139, Loss: 0.840, Training Accuracy: 70.42%\n",
      "Run 5, Epoch 140, Loss: 0.839, Training Accuracy: 70.63%\n",
      "Run 5, Epoch 141, Loss: 0.840, Training Accuracy: 70.18%\n",
      "Run 5, Epoch 142, Loss: 0.842, Training Accuracy: 70.32%\n",
      "Run 5, Epoch 143, Loss: 0.839, Training Accuracy: 70.43%\n",
      "Run 5, Epoch 144, Loss: 0.838, Training Accuracy: 70.50%\n",
      "Run 5, Epoch 145, Loss: 0.841, Training Accuracy: 70.31%\n",
      "Run 5, Epoch 146, Loss: 0.841, Training Accuracy: 70.41%\n",
      "Run 5, Epoch 147, Loss: 0.847, Training Accuracy: 70.22%\n",
      "Run 5, Epoch 148, Loss: 0.840, Training Accuracy: 70.29%\n",
      "Run 5, Epoch 149, Loss: 0.842, Training Accuracy: 70.23%\n",
      "Run 5, Epoch 150, Loss: 0.834, Training Accuracy: 70.60%\n",
      "Run 5, Epoch 151, Loss: 0.843, Training Accuracy: 70.05%\n",
      "Run 5, Epoch 152, Loss: 0.839, Training Accuracy: 70.35%\n",
      "Run 5, Epoch 153, Loss: 0.841, Training Accuracy: 70.24%\n",
      "Run 5, Epoch 154, Loss: 0.841, Training Accuracy: 70.38%\n",
      "Run 5, Epoch 155, Loss: 0.838, Training Accuracy: 70.39%\n",
      "Run 5, Epoch 156, Loss: 0.838, Training Accuracy: 70.20%\n",
      "Run 5, Epoch 157, Loss: 0.840, Training Accuracy: 70.38%\n",
      "Run 5, Epoch 158, Loss: 0.839, Training Accuracy: 70.33%\n",
      "Run 5, Epoch 159, Loss: 0.839, Training Accuracy: 70.26%\n",
      "Run 5, Epoch 160, Loss: 0.840, Training Accuracy: 70.40%\n",
      "Run 5, Epoch 161, Loss: 0.838, Training Accuracy: 70.53%\n",
      "Run 5, Epoch 162, Loss: 0.840, Training Accuracy: 70.27%\n",
      "Run 5, Epoch 163, Loss: 0.841, Training Accuracy: 70.30%\n",
      "Run 5, Epoch 164, Loss: 0.838, Training Accuracy: 70.58%\n",
      "Run 5, Epoch 165, Loss: 0.843, Training Accuracy: 70.18%\n",
      "Run 5, Epoch 166, Loss: 0.842, Training Accuracy: 70.30%\n",
      "Run 5, Epoch 167, Loss: 0.837, Training Accuracy: 70.57%\n",
      "Run 5, Epoch 168, Loss: 0.839, Training Accuracy: 70.30%\n",
      "Run 5, Epoch 169, Loss: 0.841, Training Accuracy: 70.22%\n",
      "Run 5, Epoch 170, Loss: 0.838, Training Accuracy: 70.54%\n",
      "Run 5, Epoch 171, Loss: 0.841, Training Accuracy: 70.14%\n",
      "Run 5, Epoch 172, Loss: 0.838, Training Accuracy: 70.59%\n",
      "Run 5, Epoch 173, Loss: 0.838, Training Accuracy: 70.42%\n",
      "Run 5, Epoch 174, Loss: 0.844, Training Accuracy: 70.18%\n",
      "Run 5, Epoch 175, Loss: 0.842, Training Accuracy: 70.40%\n",
      "Run 5, Final Accuracy on test set: 71.81%\n",
      "Results after run 5:\n",
      "Training Accuracies: [37.796, 48.628, 53.498, 56.932, 59.582, 61.532, 62.968, 64.39, 64.952, 66.082, 68.234, 68.594, 68.874, 69.08, 69.2, 69.296, 69.482, 69.624, 69.774, 70.05, 70.0, 70.16, 70.314, 70.278, 70.206, 70.396, 70.09, 70.098, 70.204, 70.478, 70.288, 70.584, 70.314, 70.344, 70.294, 70.444, 70.406, 70.472, 70.422, 70.29, 70.218, 70.426, 70.31, 70.272, 70.456, 70.238, 70.628, 70.268, 70.356, 70.26, 70.432, 70.396, 70.414, 70.412, 70.514, 70.288, 70.218, 70.356, 70.496, 70.508, 70.384, 70.24, 70.402, 70.59, 70.54, 70.354, 70.416, 70.044, 70.382, 69.89, 70.21, 70.398, 70.53, 70.38, 70.424, 70.304, 70.332, 70.314, 70.066, 70.466, 70.402, 70.772, 70.188, 70.55, 70.444, 70.328, 70.362, 70.226, 70.352, 70.464, 70.206, 70.562, 70.464, 70.23, 70.322, 70.148, 70.232, 70.318, 70.474, 70.382, 70.322, 70.628, 70.258, 70.108, 70.358, 70.39, 70.62, 70.324, 70.538, 70.368, 70.152, 70.492, 70.32, 70.36, 70.276, 70.194, 70.458, 70.556, 70.36, 70.344, 70.444, 70.244, 70.43, 70.252, 70.28, 70.48, 70.35, 70.452, 70.138, 70.188, 70.344, 70.46, 70.372, 70.438, 70.674, 70.288, 70.426, 70.24, 70.42, 70.63, 70.182, 70.322, 70.432, 70.5, 70.312, 70.414, 70.224, 70.29, 70.226, 70.6, 70.046, 70.346, 70.24, 70.38, 70.394, 70.202, 70.376, 70.328, 70.258, 70.404, 70.526, 70.268, 70.296, 70.576, 70.176, 70.3, 70.566, 70.298, 70.218, 70.542, 70.136, 70.59, 70.42, 70.178, 70.398]\n",
      "Test Accuracy: 71.81%\n",
      "Losses: [1.6991618954007277, 1.4208724904243293, 1.2942604799099895, 1.2044161242597244, 1.1376365737232101, 1.081372227510223, 1.0432782371330749, 1.0087273806867088, 0.9892614976219509, 0.961286405468231, 0.9015597355030381, 0.8855322367699859, 0.8785460830649452, 0.875086689849034, 0.8698969251664398, 0.8700587180874232, 0.8621114652480006, 0.8586010292667867, 0.8583596243577845, 0.8517859256480966, 0.8484143428790295, 0.8498923552920447, 0.8467354452823435, 0.8421800357606405, 0.8419358358358788, 0.8415131340246371, 0.8440668166750838, 0.8439056696489339, 0.8424258983653524, 0.8424511559479072, 0.8451942796902279, 0.8372595322406505, 0.8409713046325137, 0.841193324921991, 0.8392301750609942, 0.8403739802672735, 0.8389020310643384, 0.837600964109611, 0.8418157908617688, 0.8420528028627186, 0.8428309629945194, 0.840588860652026, 0.8411082445508073, 0.8404963944879029, 0.8378451179970255, 0.8419979926570297, 0.8373194097557946, 0.8442913857872224, 0.8413094803500358, 0.8399517496528528, 0.837710189240058, 0.8421486681684509, 0.8393433880623039, 0.8402754005873599, 0.8384847798006004, 0.8422621837662309, 0.8419091600896148, 0.8406266649360852, 0.8376336666324254, 0.8411105583086038, 0.8406803338119136, 0.8421331223319558, 0.8381320869221407, 0.8349560186686114, 0.8407935042820318, 0.8405502908065191, 0.8370521855171379, 0.8441369917691516, 0.8391372469989845, 0.8467075785102747, 0.838992913970557, 0.8358963593802489, 0.836960980501931, 0.8403840415617999, 0.8406855343552806, 0.8402827196109021, 0.8406739969692572, 0.8424171822150345, 0.8430075634775869, 0.838729656108505, 0.8387984083131756, 0.8404808867617947, 0.8442603063095561, 0.8412983698003432, 0.8376494742110562, 0.8403173895443187, 0.842551584286458, 0.8455271208682633, 0.839724301986987, 0.8409999023617991, 0.8436720075509737, 0.8402720695871222, 0.8401804813338668, 0.8425663060239513, 0.8395851571541613, 0.8432822184794394, 0.8412402534423886, 0.8436577567054183, 0.8383555877239198, 0.8415170394246231, 0.8396094080127413, 0.8405960243376319, 0.8408379400782573, 0.8421187289535542, 0.8428377221002603, 0.8396347005044102, 0.8366118492677693, 0.8419619699573273, 0.8373043188048751, 0.8377069836992133, 0.8425130309046381, 0.8377409658163709, 0.842286835850962, 0.8413881401881538, 0.8416308464906405, 0.8408698912166879, 0.8413870535848086, 0.8394796031210429, 0.84218226308408, 0.8425754049549932, 0.8402928581932927, 0.8414842201315839, 0.8389291270919468, 0.8425701345933978, 0.8431776947987354, 0.8372875817901339, 0.8432010607341366, 0.8392058089566048, 0.8403368287379175, 0.839818075794698, 0.8411648096635823, 0.8393098381169312, 0.8404013319393558, 0.8372913193519768, 0.838800503164911, 0.8419440472522355, 0.8393475050511567, 0.8399734394934476, 0.8399048700661915, 0.8386702653392196, 0.8403562632058282, 0.8422589791400353, 0.8394604087485682, 0.8376649596806988, 0.8408368687190668, 0.8408498381409804, 0.8471985116334217, 0.8396772432815084, 0.8424558348363013, 0.8343943618142696, 0.8428860631440301, 0.839455827422764, 0.8405345996932301, 0.8408121792861568, 0.8382936647481016, 0.838018166287171, 0.839985547315739, 0.838798132393976, 0.8394479192126437, 0.8400604808726884, 0.8376624797616163, 0.8395527578375833, 0.8410227573131357, 0.8383751324070688, 0.843259527250324, 0.8420925509289402, 0.8374494513892152, 0.839365473336271, 0.84138749032984, 0.8383316312299665, 0.8406815438929116, 0.8381379704036371, 0.8381411719810018, 0.8444360302537298, 0.8423644834772095]\n",
      "All Training Accuracies over Epochs for each run: [[38.392 48.536 52.714 56.304 58.292 60.35  62.018 63.188 64.464 65.266\n",
      "  67.692 68.23  68.808 68.792 68.842 69.012 68.986 69.16  69.376 69.684\n",
      "  69.752 69.842 69.772 70.188 70.116 70.074 69.974 69.918 69.872 70.036\n",
      "  70.154 70.04  69.802 70.024 69.766 70.1   70.244 69.892 70.27  70.336\n",
      "  69.812 70.072 69.978 70.078 70.122 69.896 70.052 69.822 69.852 70.248\n",
      "  70.17  69.96  70.044 70.078 69.83  70.092 69.964 70.076 69.842 70.23\n",
      "  69.976 70.068 69.852 70.018 70.002 70.224 69.656 69.712 69.914 69.95\n",
      "  70.19  70.186 70.14  70.39  70.114 69.9   69.948 69.794 69.976 69.926\n",
      "  70.02  69.74  69.988 70.048 69.938 69.906 69.864 69.836 70.038 70.248\n",
      "  70.096 70.17  70.134 70.182 70.016 70.222 70.152 70.028 70.128 70.268\n",
      "  69.938 70.064 70.058 70.09  70.088 70.022 70.13  70.128 70.038 70.124\n",
      "  70.062 70.132 70.08  70.142 69.908 69.908 70.226 70.072 70.076 69.806\n",
      "  70.406 70.024 69.842 69.988 70.082 69.956 70.128 69.982 70.19  70.348\n",
      "  70.106 70.016 70.092 69.982 70.1   70.014 70.024 69.968 70.014 70.046\n",
      "  69.796 69.994 69.912 69.954 69.936 69.966 70.288 69.898 69.87  69.888\n",
      "  70.132 70.146 69.834 70.016 70.29  69.922 69.968 70.088 69.944 70.264\n",
      "  70.024 70.068 70.114 70.092 70.052 69.994 70.012 70.012 69.902 70.014\n",
      "  70.29  69.876 70.014 69.952 69.864]\n",
      " [37.178 47.826 52.72  55.712 58.006 60.072 61.344 62.788 63.864 64.702\n",
      "  66.87  67.412 67.618 68.098 67.736 68.188 68.182 68.396 68.47  68.834\n",
      "  69.024 69.134 68.836 69.33  69.014 69.19  69.08  68.642 69.072 69.396\n",
      "  69.32  69.334 69.06  69.132 69.358 69.308 69.472 68.916 69.156 69.094\n",
      "  69.118 69.116 69.24  69.132 69.172 69.112 69.318 69.198 69.318 69.234\n",
      "  69.148 69.244 69.172 69.232 69.312 68.926 69.222 69.152 69.214 69.298\n",
      "  69.168 69.38  69.198 69.4   68.926 69.504 69.274 68.95  69.074 69.104\n",
      "  69.026 69.422 68.85  69.236 69.072 69.14  69.    69.094 69.238 69.416\n",
      "  69.196 69.112 69.342 68.672 69.136 69.33  68.946 69.138 69.122 69.142\n",
      "  68.906 69.206 69.068 69.068 69.15  69.198 69.02  69.058 69.194 69.076\n",
      "  69.096 69.33  69.318 69.112 69.178 69.28  69.27  69.182 68.95  69.306\n",
      "  69.132 69.108 69.172 69.014 69.032 69.112 69.412 69.168 69.322 69.132\n",
      "  68.972 68.88  69.002 69.136 69.326 69.006 69.28  69.034 69.016 69.126\n",
      "  69.324 69.228 69.216 69.172 68.892 69.35  69.196 69.218 68.998 69.218\n",
      "  69.212 68.922 69.442 69.188 69.112 69.048 69.372 69.044 69.232 69.162\n",
      "  69.03  69.154 69.384 69.358 69.218 69.116 69.154 69.162 69.28  69.292\n",
      "  69.18  69.092 68.904 69.198 69.074 69.134 69.176 69.052 69.208 69.278\n",
      "  69.192 69.26  69.172 69.002 69.448]\n",
      " [38.06  48.026 52.93  56.324 59.082 60.626 62.26  63.002 64.14  65.206\n",
      "  67.514 67.738 68.164 68.242 68.294 68.506 68.786 68.906 69.302 68.936\n",
      "  69.308 69.748 69.668 69.474 69.616 69.506 69.374 69.324 69.428 69.56\n",
      "  69.426 69.528 69.708 69.78  69.57  69.688 69.764 69.372 69.394 69.556\n",
      "  69.502 69.648 69.566 69.5   69.7   69.724 69.588 69.692 69.458 69.748\n",
      "  69.618 69.45  69.704 69.768 69.714 69.604 69.668 69.764 69.926 69.532\n",
      "  69.368 69.532 69.686 69.602 69.512 69.626 69.762 69.712 69.542 69.548\n",
      "  69.756 69.45  69.616 69.516 69.598 69.514 69.442 69.522 69.684 69.456\n",
      "  69.648 69.728 69.55  69.65  69.714 69.57  69.696 69.93  69.436 69.562\n",
      "  69.598 69.61  69.634 69.732 69.736 69.364 69.7   69.6   69.434 69.578\n",
      "  69.658 69.684 69.518 69.716 69.838 69.458 69.608 69.648 69.662 69.44\n",
      "  69.624 69.628 69.676 69.762 69.474 69.456 69.654 69.262 69.608 69.728\n",
      "  69.596 69.734 69.9   69.722 69.716 69.688 69.706 69.548 69.472 69.646\n",
      "  69.702 69.52  69.544 69.646 69.394 69.784 69.656 69.754 69.548 69.368\n",
      "  69.812 69.662 69.492 69.446 69.52  69.364 69.606 69.678 69.852 69.366\n",
      "  69.728 69.648 69.738 69.688 69.542 69.588 69.738 69.78  69.354 69.588\n",
      "  69.718 69.696 69.746 69.654 69.648 69.368 69.472 69.506 69.716 69.562\n",
      "  69.792 69.43  69.56  69.658 69.452]\n",
      " [38.978 48.372 52.788 56.476 58.666 60.728 62.474 63.056 64.55  65.436\n",
      "  67.628 68.486 68.544 68.654 68.762 68.958 69.154 69.4   69.328 69.374\n",
      "  69.552 69.768 69.4   69.772 69.866 69.664 69.85  70.028 69.894 69.814\n",
      "  69.82  69.79  69.706 69.968 69.888 69.888 69.936 69.818 70.044 69.884\n",
      "  69.938 69.682 70.048 70.038 69.712 69.77  69.794 69.896 70.156 70.004\n",
      "  69.916 70.068 69.868 69.908 69.858 69.832 70.042 69.87  69.84  70.112\n",
      "  69.874 69.83  70.162 69.778 69.994 69.826 70.028 69.838 70.216 70.022\n",
      "  69.924 70.078 69.962 69.564 69.77  69.804 69.98  70.1   70.02  69.87\n",
      "  69.892 69.6   70.088 70.228 69.98  70.142 70.046 70.122 69.778 69.874\n",
      "  70.142 69.652 69.91  69.786 70.    69.862 70.008 70.116 69.962 70.096\n",
      "  69.886 69.766 70.072 69.924 70.046 69.826 69.816 69.944 70.134 69.894\n",
      "  70.144 69.782 70.014 69.846 69.864 69.998 69.874 69.954 69.646 69.864\n",
      "  69.922 69.766 69.806 70.014 69.988 69.834 70.    69.788 70.044 70.076\n",
      "  70.006 69.894 69.99  69.854 70.054 70.02  69.948 70.014 69.958 69.986\n",
      "  69.836 69.964 69.694 69.918 70.16  69.704 69.932 69.828 69.988 69.982\n",
      "  69.988 69.754 69.7   69.752 70.01  69.796 69.652 70.084 70.024 70.058\n",
      "  69.888 70.1   70.108 69.81  69.85  69.944 70.03  69.906 69.962 70.008\n",
      "  69.936 70.012 69.842 69.992 69.98 ]\n",
      " [37.796 48.628 53.498 56.932 59.582 61.532 62.968 64.39  64.952 66.082\n",
      "  68.234 68.594 68.874 69.08  69.2   69.296 69.482 69.624 69.774 70.05\n",
      "  70.    70.16  70.314 70.278 70.206 70.396 70.09  70.098 70.204 70.478\n",
      "  70.288 70.584 70.314 70.344 70.294 70.444 70.406 70.472 70.422 70.29\n",
      "  70.218 70.426 70.31  70.272 70.456 70.238 70.628 70.268 70.356 70.26\n",
      "  70.432 70.396 70.414 70.412 70.514 70.288 70.218 70.356 70.496 70.508\n",
      "  70.384 70.24  70.402 70.59  70.54  70.354 70.416 70.044 70.382 69.89\n",
      "  70.21  70.398 70.53  70.38  70.424 70.304 70.332 70.314 70.066 70.466\n",
      "  70.402 70.772 70.188 70.55  70.444 70.328 70.362 70.226 70.352 70.464\n",
      "  70.206 70.562 70.464 70.23  70.322 70.148 70.232 70.318 70.474 70.382\n",
      "  70.322 70.628 70.258 70.108 70.358 70.39  70.62  70.324 70.538 70.368\n",
      "  70.152 70.492 70.32  70.36  70.276 70.194 70.458 70.556 70.36  70.344\n",
      "  70.444 70.244 70.43  70.252 70.28  70.48  70.35  70.452 70.138 70.188\n",
      "  70.344 70.46  70.372 70.438 70.674 70.288 70.426 70.24  70.42  70.63\n",
      "  70.182 70.322 70.432 70.5   70.312 70.414 70.224 70.29  70.226 70.6\n",
      "  70.046 70.346 70.24  70.38  70.394 70.202 70.376 70.328 70.258 70.404\n",
      "  70.526 70.268 70.296 70.576 70.176 70.3   70.566 70.298 70.218 70.542\n",
      "  70.136 70.59  70.42  70.178 70.398]]\n",
      "All Test Accuracies after each run: [[71.72]\n",
      " [70.41]\n",
      " [71.33]\n",
      " [72.17]\n",
      " [71.81]]\n",
      "All Losses over Epochs for each run: [[1.68184862 1.42130175 1.31619598 1.2243249  1.16695279 1.11349578\n",
      "  1.07261073 1.03003702 0.99725121 0.9787916  0.9130463  0.90164344\n",
      "  0.89090333 0.88903211 0.88019651 0.88068502 0.87814845 0.8726144\n",
      "  0.87018148 0.86329612 0.85751089 0.85749612 0.85660668 0.85286473\n",
      "  0.8551274  0.85143275 0.85433585 0.85178282 0.85239093 0.85081292\n",
      "  0.85013988 0.8507156  0.85681426 0.85007889 0.8520631  0.85263432\n",
      "  0.84528373 0.85571101 0.84774979 0.8474114  0.85822389 0.84804371\n",
      "  0.85384541 0.85118442 0.84997823 0.85078388 0.85148486 0.85295219\n",
      "  0.85274581 0.84789611 0.85181201 0.85038297 0.85384497 0.84847725\n",
      "  0.85041252 0.84735486 0.85185809 0.84869349 0.85065689 0.8498309\n",
      "  0.84926479 0.85301301 0.85155119 0.84883166 0.8548577  0.84652284\n",
      "  0.85578303 0.85538014 0.85066923 0.85064882 0.852496   0.84953255\n",
      "  0.85054579 0.8477436  0.8515094  0.8549689  0.85183587 0.85115195\n",
      "  0.85173465 0.85251335 0.84948887 0.85481369 0.85337564 0.84894064\n",
      "  0.85566645 0.85231564 0.85146642 0.84978817 0.85314625 0.85157842\n",
      "  0.84977781 0.85216362 0.85056119 0.85179928 0.85080154 0.84844157\n",
      "  0.84847303 0.84978639 0.84949544 0.8486531  0.85126841 0.84997989\n",
      "  0.85107445 0.85281946 0.8478999  0.84676291 0.8500095  0.85025769\n",
      "  0.84541246 0.84983896 0.84906164 0.84753159 0.85309462 0.84846933\n",
      "  0.85436391 0.8559425  0.8506417  0.85206818 0.85231642 0.85549447\n",
      "  0.84810212 0.84788897 0.85304175 0.84930746 0.85186523 0.85097638\n",
      "  0.84939261 0.8517166  0.84959368 0.85109969 0.85020492 0.84768517\n",
      "  0.85184766 0.85116925 0.85039547 0.85158264 0.84767841 0.84857222\n",
      "  0.85040592 0.85249026 0.85162821 0.84780286 0.85339709 0.85367005\n",
      "  0.8540945  0.85275487 0.84702648 0.85106984 0.85088791 0.85135023\n",
      "  0.85116356 0.84997056 0.85479608 0.84883141 0.84826867 0.850982\n",
      "  0.85239381 0.84923134 0.85208805 0.85255575 0.85107518 0.84895906\n",
      "  0.85068773 0.85098748 0.84952953 0.84505402 0.85131005 0.85111446\n",
      "  0.85229309 0.84710544 0.85043391 0.85156881 0.84923396 0.85211931\n",
      "  0.85057764]\n",
      " [1.7244742  1.44058831 1.31853977 1.23810509 1.17943677 1.12820242\n",
      "  1.08441729 1.05055543 1.02371683 1.00300441 0.94191074 0.92671438\n",
      "  0.92033843 0.91297534 0.90882111 0.90523344 0.90331665 0.89895627\n",
      "  0.89552258 0.89149451 0.88029942 0.87815158 0.88553205 0.87660731\n",
      "  0.87838175 0.88099505 0.88088575 0.8841507  0.87751997 0.87821866\n",
      "  0.87703455 0.8767231  0.8795206  0.87789226 0.8769966  0.8756371\n",
      "  0.87676001 0.87918764 0.88036375 0.87617507 0.87658232 0.87923172\n",
      "  0.87501298 0.87867561 0.87476521 0.87570354 0.87564922 0.87946588\n",
      "  0.87500595 0.87732311 0.87765188 0.87747538 0.87517753 0.87805482\n",
      "  0.87831775 0.88036095 0.87957717 0.87695776 0.87430434 0.87524094\n",
      "  0.88016438 0.87563071 0.87934282 0.87475019 0.87977001 0.87383638\n",
      "  0.8774795  0.87935113 0.87528289 0.87385729 0.88009047 0.8749789\n",
      "  0.88373414 0.8781158  0.87598338 0.87568177 0.87802813 0.87606179\n",
      "  0.8763015  0.87345074 0.87462587 0.87725446 0.8746438  0.88227067\n",
      "  0.87942819 0.87816693 0.88174285 0.87357335 0.87715545 0.87838816\n",
      "  0.88156415 0.87744127 0.87863209 0.87771555 0.87601167 0.877482\n",
      "  0.87672007 0.87719298 0.87513202 0.87785913 0.87756113 0.87651679\n",
      "  0.87870272 0.87724619 0.87473588 0.87517223 0.8753462  0.8739889\n",
      "  0.88087375 0.87188233 0.87726819 0.87806545 0.88150696 0.87972048\n",
      "  0.87984661 0.87784306 0.87642998 0.87608936 0.87432728 0.87542363\n",
      "  0.87818224 0.87846938 0.87907335 0.87456335 0.87777837 0.87832848\n",
      "  0.87406418 0.87937117 0.87613701 0.87843243 0.87740276 0.87580846\n",
      "  0.87316958 0.87588026 0.87773062 0.87581835 0.87676719 0.87714731\n",
      "  0.8779421  0.87914313 0.87719519 0.8818038  0.8738997  0.87177676\n",
      "  0.88029835 0.87970231 0.87556419 0.87756984 0.87813652 0.87733698\n",
      "  0.8763591  0.87817323 0.86994196 0.87451784 0.87818528 0.88007483\n",
      "  0.87818042 0.87341945 0.87803133 0.87818102 0.87891317 0.87541049\n",
      "  0.88093293 0.87646206 0.87700773 0.87753724 0.87729829 0.87688475\n",
      "  0.87592802 0.87698241 0.87805299 0.87723135 0.87546122 0.87801855\n",
      "  0.87620689]\n",
      " [1.69884412 1.4285972  1.30930172 1.21605746 1.15132962 1.09964648\n",
      "  1.06745501 1.0386868  1.0143795  0.98261424 0.91792097 0.91103603\n",
      "  0.90250925 0.89938984 0.89351787 0.8933246  0.88539324 0.88359639\n",
      "  0.87414457 0.87687494 0.87026254 0.86677064 0.86322957 0.86549957\n",
      "  0.86416767 0.86523655 0.86489459 0.86498358 0.86200187 0.86044693\n",
      "  0.86042515 0.86387048 0.86235229 0.86059572 0.86124337 0.8590276\n",
      "  0.86155861 0.8648551  0.86443424 0.86558017 0.8635008  0.86527787\n",
      "  0.86437375 0.85963069 0.86236146 0.86216211 0.86056403 0.86265088\n",
      "  0.86399794 0.86114374 0.86094944 0.86350805 0.86145976 0.86128943\n",
      "  0.8606368  0.86015914 0.85977929 0.86079828 0.85948704 0.86385745\n",
      "  0.86481062 0.86362944 0.86053819 0.86394398 0.86517202 0.86408979\n",
      "  0.8593536  0.86284577 0.86267538 0.85943344 0.85828337 0.86612283\n",
      "  0.86314663 0.86033455 0.86089831 0.86086498 0.86415111 0.8619383\n",
      "  0.86278248 0.86208229 0.86000878 0.86189667 0.86354732 0.86233433\n",
      "  0.86313067 0.86323714 0.86166371 0.8600093  0.86415738 0.86105207\n",
      "  0.86245361 0.85780212 0.85830452 0.86131754 0.86329536 0.86314621\n",
      "  0.86119747 0.86359087 0.8646137  0.8631044  0.8591404  0.85900207\n",
      "  0.86297019 0.86178948 0.85871759 0.8644649  0.86161873 0.85987494\n",
      "  0.85838119 0.8623257  0.860602   0.86256509 0.86368201 0.86138093\n",
      "  0.86368801 0.86558785 0.86189189 0.85951462 0.86168372 0.86305808\n",
      "  0.8587543  0.86234823 0.85989212 0.86299296 0.86153564 0.86246511\n",
      "  0.8610261  0.86205915 0.86727954 0.85970182 0.86240791 0.86409787\n",
      "  0.86371872 0.85938664 0.86068679 0.8619129  0.86015181 0.86267252\n",
      "  0.86081653 0.86504072 0.85898946 0.86149634 0.86612574 0.86438904\n",
      "  0.86128223 0.86523704 0.86156419 0.85972042 0.85959517 0.86420779\n",
      "  0.8625301  0.86214526 0.85745432 0.86371847 0.86256986 0.86292073\n",
      "  0.86110166 0.86141587 0.86501974 0.86103545 0.86249086 0.85976579\n",
      "  0.8624685  0.86279894 0.86112971 0.86189081 0.86103828 0.86505094\n",
      "  0.8603544  0.86342493 0.86318649 0.86724244 0.86258582 0.86102684\n",
      "  0.86190489]\n",
      " [1.68368208 1.42754582 1.30921731 1.21627498 1.15269657 1.10248812\n",
      "  1.05935242 1.03678391 0.99883727 0.97726999 0.91335928 0.89853969\n",
      "  0.89294771 0.89055388 0.88348038 0.8765016  0.87622932 0.87397763\n",
      "  0.86667064 0.87014357 0.86017989 0.85715961 0.85933365 0.8586754\n",
      "  0.85515315 0.86115621 0.855523   0.85452499 0.85732433 0.85726268\n",
      "  0.85218751 0.8554075  0.85874893 0.85460965 0.85338275 0.85382024\n",
      "  0.85402787 0.85236042 0.85391342 0.85242074 0.85402678 0.85637591\n",
      "  0.85134253 0.85136619 0.85442814 0.85283361 0.85042331 0.85335287\n",
      "  0.84952228 0.85478553 0.85324762 0.85229546 0.85387926 0.8559158\n",
      "  0.85136709 0.85348345 0.85012748 0.85315377 0.85562808 0.85469135\n",
      "  0.85337634 0.85220895 0.85105332 0.85616348 0.85362904 0.85484988\n",
      "  0.85344519 0.85295485 0.85127601 0.84968236 0.85414731 0.85236818\n",
      "  0.85336044 0.85466076 0.85599961 0.85547423 0.85273443 0.85154676\n",
      "  0.85067901 0.85341086 0.85334417 0.85647905 0.85198712 0.84944632\n",
      "  0.85075828 0.85344211 0.8515686  0.8506115  0.85527863 0.84981774\n",
      "  0.85268941 0.85551149 0.8521911  0.85567282 0.85630662 0.85271964\n",
      "  0.85213406 0.85280992 0.85346617 0.85345633 0.8544603  0.8539371\n",
      "  0.85099835 0.85161123 0.84820229 0.85197096 0.85350681 0.85293379\n",
      "  0.85331017 0.8525979  0.85421422 0.85603191 0.85361406 0.85587748\n",
      "  0.8517926  0.85474561 0.85357704 0.85332632 0.85397997 0.84980253\n",
      "  0.85111069 0.85323745 0.85456178 0.85143893 0.84865514 0.85346895\n",
      "  0.84887217 0.85790812 0.85450426 0.85039361 0.85123235 0.85467082\n",
      "  0.85474717 0.85462224 0.85278047 0.85273363 0.85427035 0.85207192\n",
      "  0.85297429 0.85144608 0.85222816 0.85272417 0.85446973 0.85047906\n",
      "  0.85011666 0.85573622 0.85339679 0.85395684 0.85592714 0.85584523\n",
      "  0.85088416 0.85773474 0.86029635 0.85980804 0.85430503 0.8561487\n",
      "  0.85739685 0.85550776 0.85041794 0.85188561 0.8552836  0.85140881\n",
      "  0.85288211 0.85313441 0.85398131 0.8517965  0.84940674 0.85515615\n",
      "  0.84981488 0.85561923 0.85211851 0.85517797 0.85310025 0.8507097\n",
      "  0.85328102]\n",
      " [1.6991619  1.42087249 1.29426048 1.20441612 1.13763657 1.08137223\n",
      "  1.04327824 1.00872738 0.9892615  0.96128641 0.90155974 0.88553224\n",
      "  0.87854608 0.87508669 0.86989693 0.87005872 0.86211147 0.85860103\n",
      "  0.85835962 0.85178593 0.84841434 0.84989236 0.84673545 0.84218004\n",
      "  0.84193584 0.84151313 0.84406682 0.84390567 0.8424259  0.84245116\n",
      "  0.84519428 0.83725953 0.8409713  0.84119332 0.83923018 0.84037398\n",
      "  0.83890203 0.83760096 0.84181579 0.8420528  0.84283096 0.84058886\n",
      "  0.84110824 0.84049639 0.83784512 0.84199799 0.83731941 0.84429139\n",
      "  0.84130948 0.83995175 0.83771019 0.84214867 0.83934339 0.8402754\n",
      "  0.83848478 0.84226218 0.84190916 0.84062666 0.83763367 0.84111056\n",
      "  0.84068033 0.84213312 0.83813209 0.83495602 0.8407935  0.84055029\n",
      "  0.83705219 0.84413699 0.83913725 0.84670758 0.83899291 0.83589636\n",
      "  0.83696098 0.84038404 0.84068553 0.84028272 0.840674   0.84241718\n",
      "  0.84300756 0.83872966 0.83879841 0.84048089 0.84426031 0.84129837\n",
      "  0.83764947 0.84031739 0.84255158 0.84552712 0.8397243  0.8409999\n",
      "  0.84367201 0.84027207 0.84018048 0.84256631 0.83958516 0.84328222\n",
      "  0.84124025 0.84365776 0.83835559 0.84151704 0.83960941 0.84059602\n",
      "  0.84083794 0.84211873 0.84283772 0.8396347  0.83661185 0.84196197\n",
      "  0.83730432 0.83770698 0.84251303 0.83774097 0.84228684 0.84138814\n",
      "  0.84163085 0.84086989 0.84138705 0.8394796  0.84218226 0.8425754\n",
      "  0.84029286 0.84148422 0.83892913 0.84257013 0.84317769 0.83728758\n",
      "  0.84320106 0.83920581 0.84033683 0.83981808 0.84116481 0.83930984\n",
      "  0.84040133 0.83729132 0.8388005  0.84194405 0.83934751 0.83997344\n",
      "  0.83990487 0.83867027 0.84035626 0.84225898 0.83946041 0.83766496\n",
      "  0.84083687 0.84084984 0.84719851 0.83967724 0.84245583 0.83439436\n",
      "  0.84288606 0.83945583 0.8405346  0.84081218 0.83829366 0.83801817\n",
      "  0.83998555 0.83879813 0.83944792 0.84006048 0.83766248 0.83955276\n",
      "  0.84102276 0.83837513 0.84325953 0.84209255 0.83744945 0.83936547\n",
      "  0.84138749 0.83833163 0.84068154 0.83813797 0.83814117 0.84443603\n",
      "  0.84236448]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "num_epochs = 175\n",
    "num_runs = 5\n",
    "\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "all_losses = []\n",
    "\n",
    "def save_checkpoint(run, model, optimizer, scheduler, train_accuracies, losses):\n",
    "    checkpoint = {\n",
    "        'run': run,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'losses': losses\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth'))\n",
    "\n",
    "def load_checkpoint(run):\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth'))\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Starting run {run + 1}/{num_runs}\")\n",
    "\n",
    "    \n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    \n",
    "    if os.path.exists(os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth')):\n",
    "        print(f\"Loading checkpoint for run {run + 1}\")\n",
    "        checkpoint = load_checkpoint(run)\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        train_accuracies = checkpoint['train_accuracies']\n",
    "        losses = checkpoint['losses']\n",
    "    else:\n",
    "        train_accuracies = []\n",
    "        losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()  \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        losses.append(running_loss / len(trainloader))\n",
    "        print(f\"Run {run+1}, Epoch {epoch+1}, Loss: {running_loss / len(trainloader):.3f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        scheduler.step()  \n",
    "\n",
    "    \n",
    "    net.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f\"Run {run+1}, Final Accuracy on test set: {test_accuracy:.2f}%\")\n",
    "\n",
    "    \n",
    "    all_train_accuracies.append(train_accuracies)\n",
    "    all_test_accuracies.append(test_accuracies)\n",
    "    all_losses.append(losses)\n",
    "\n",
    "\n",
    "    save_checkpoint(run, net, optimizer, scheduler, train_accuracies, losses)\n",
    "\n",
    "    \n",
    "    print(f\"Results after run {run + 1}:\")\n",
    "    print(f\"Training Accuracies: {train_accuracies}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"Losses: {losses}\")\n",
    "\n",
    "\n",
    "all_train_accuracies = np.array(all_train_accuracies)\n",
    "all_test_accuracies = np.array(all_test_accuracies)\n",
    "all_losses = np.array(all_losses)\n",
    "\n",
    "\n",
    "np.savetxt('train_accuracies.txt', all_train_accuracies)\n",
    "np.savetxt('test_accuracies.txt', all_test_accuracies)\n",
    "np.savetxt('losses.txt', all_losses)\n",
    "\n",
    "print(\"All Training Accuracies over Epochs for each run:\", all_train_accuracies)\n",
    "print(\"All Test Accuracies after each run:\", all_test_accuracies)\n",
    "print(\"All Losses over Epochs for each run:\", all_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26410.814491,
   "end_time": "2024-08-29T22:59:56.221176",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-29T15:39:45.406685",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
