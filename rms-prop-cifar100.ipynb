{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531de4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T12:40:48.171835Z",
     "iopub.status.busy": "2024-10-23T12:40:48.171484Z",
     "iopub.status.idle": "2024-10-23T20:18:29.553888Z",
     "shell.execute_reply": "2024-10-23T20:18:29.552718Z"
    },
    "papermill": {
     "duration": 27461.459445,
     "end_time": "2024-10-23T20:18:29.627236",
     "exception": false,
     "start_time": "2024-10-23T12:40:48.167791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:02<00:00, 80268725.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Starting run 1/5\n",
      "Run 1, Epoch 1, Loss: 4.224, Training Accuracy: 6.01%\n",
      "Run 1, Epoch 2, Loss: 3.983, Training Accuracy: 9.64%\n",
      "Run 1, Epoch 3, Loss: 3.862, Training Accuracy: 11.59%\n",
      "Run 1, Epoch 4, Loss: 3.759, Training Accuracy: 13.28%\n",
      "Run 1, Epoch 5, Loss: 3.685, Training Accuracy: 14.68%\n",
      "Run 1, Epoch 6, Loss: 3.622, Training Accuracy: 15.69%\n",
      "Run 1, Epoch 7, Loss: 3.570, Training Accuracy: 16.44%\n",
      "Run 1, Epoch 8, Loss: 3.531, Training Accuracy: 17.27%\n",
      "Run 1, Epoch 9, Loss: 3.490, Training Accuracy: 17.75%\n",
      "Run 1, Epoch 10, Loss: 3.458, Training Accuracy: 18.51%\n",
      "Run 1, Epoch 11, Loss: 3.409, Training Accuracy: 19.59%\n",
      "Run 1, Epoch 12, Loss: 3.400, Training Accuracy: 19.76%\n",
      "Run 1, Epoch 13, Loss: 3.389, Training Accuracy: 20.11%\n",
      "Run 1, Epoch 14, Loss: 3.387, Training Accuracy: 20.01%\n",
      "Run 1, Epoch 15, Loss: 3.384, Training Accuracy: 19.96%\n",
      "Run 1, Epoch 16, Loss: 3.385, Training Accuracy: 20.09%\n",
      "Run 1, Epoch 17, Loss: 3.380, Training Accuracy: 19.98%\n",
      "Run 1, Epoch 18, Loss: 3.378, Training Accuracy: 20.26%\n",
      "Run 1, Epoch 19, Loss: 3.379, Training Accuracy: 20.14%\n",
      "Run 1, Epoch 20, Loss: 3.370, Training Accuracy: 20.54%\n",
      "Run 1, Epoch 21, Loss: 3.365, Training Accuracy: 20.46%\n",
      "Run 1, Epoch 22, Loss: 3.363, Training Accuracy: 20.42%\n",
      "Run 1, Epoch 23, Loss: 3.364, Training Accuracy: 20.35%\n",
      "Run 1, Epoch 24, Loss: 3.363, Training Accuracy: 20.52%\n",
      "Run 1, Epoch 25, Loss: 3.365, Training Accuracy: 20.39%\n",
      "Run 1, Epoch 26, Loss: 3.361, Training Accuracy: 20.45%\n",
      "Run 1, Epoch 27, Loss: 3.362, Training Accuracy: 20.50%\n",
      "Run 1, Epoch 28, Loss: 3.366, Training Accuracy: 20.42%\n",
      "Run 1, Epoch 29, Loss: 3.362, Training Accuracy: 20.28%\n",
      "Run 1, Epoch 30, Loss: 3.359, Training Accuracy: 20.40%\n",
      "Run 1, Epoch 31, Loss: 3.362, Training Accuracy: 20.29%\n",
      "Run 1, Epoch 32, Loss: 3.360, Training Accuracy: 20.56%\n",
      "Run 1, Epoch 33, Loss: 3.353, Training Accuracy: 20.53%\n",
      "Run 1, Epoch 34, Loss: 3.359, Training Accuracy: 20.54%\n",
      "Run 1, Epoch 35, Loss: 3.360, Training Accuracy: 20.49%\n",
      "Run 1, Epoch 36, Loss: 3.366, Training Accuracy: 20.41%\n",
      "Run 1, Epoch 37, Loss: 3.359, Training Accuracy: 20.62%\n",
      "Run 1, Epoch 38, Loss: 3.358, Training Accuracy: 20.37%\n",
      "Run 1, Epoch 39, Loss: 3.359, Training Accuracy: 20.43%\n",
      "Run 1, Epoch 40, Loss: 3.361, Training Accuracy: 20.50%\n",
      "Run 1, Epoch 41, Loss: 3.355, Training Accuracy: 20.60%\n",
      "Run 1, Epoch 42, Loss: 3.357, Training Accuracy: 20.50%\n",
      "Run 1, Epoch 43, Loss: 3.360, Training Accuracy: 20.43%\n",
      "Run 1, Epoch 44, Loss: 3.362, Training Accuracy: 20.54%\n",
      "Run 1, Epoch 45, Loss: 3.360, Training Accuracy: 20.45%\n",
      "Run 1, Epoch 46, Loss: 3.357, Training Accuracy: 20.67%\n",
      "Run 1, Epoch 47, Loss: 3.360, Training Accuracy: 20.37%\n",
      "Run 1, Epoch 48, Loss: 3.361, Training Accuracy: 20.51%\n",
      "Run 1, Epoch 49, Loss: 3.359, Training Accuracy: 20.37%\n",
      "Run 1, Epoch 50, Loss: 3.361, Training Accuracy: 20.39%\n",
      "Run 1, Epoch 51, Loss: 3.360, Training Accuracy: 20.46%\n",
      "Run 1, Epoch 52, Loss: 3.357, Training Accuracy: 20.77%\n",
      "Run 1, Epoch 53, Loss: 3.354, Training Accuracy: 20.46%\n",
      "Run 1, Epoch 54, Loss: 3.359, Training Accuracy: 20.58%\n",
      "Run 1, Epoch 55, Loss: 3.358, Training Accuracy: 20.66%\n",
      "Run 1, Epoch 56, Loss: 3.357, Training Accuracy: 20.57%\n",
      "Run 1, Epoch 57, Loss: 3.360, Training Accuracy: 20.40%\n",
      "Run 1, Epoch 58, Loss: 3.357, Training Accuracy: 20.58%\n",
      "Run 1, Epoch 59, Loss: 3.360, Training Accuracy: 20.34%\n",
      "Run 1, Epoch 60, Loss: 3.358, Training Accuracy: 20.55%\n",
      "Run 1, Epoch 61, Loss: 3.362, Training Accuracy: 20.43%\n",
      "Run 1, Epoch 62, Loss: 3.357, Training Accuracy: 20.65%\n",
      "Run 1, Epoch 63, Loss: 3.357, Training Accuracy: 20.60%\n",
      "Run 1, Epoch 64, Loss: 3.355, Training Accuracy: 20.53%\n",
      "Run 1, Epoch 65, Loss: 3.361, Training Accuracy: 20.54%\n",
      "Run 1, Epoch 66, Loss: 3.357, Training Accuracy: 20.56%\n",
      "Run 1, Epoch 67, Loss: 3.359, Training Accuracy: 20.27%\n",
      "Run 1, Epoch 68, Loss: 3.356, Training Accuracy: 20.48%\n",
      "Run 1, Epoch 69, Loss: 3.363, Training Accuracy: 20.28%\n",
      "Run 1, Epoch 70, Loss: 3.360, Training Accuracy: 20.49%\n",
      "Run 1, Epoch 71, Loss: 3.359, Training Accuracy: 20.52%\n",
      "Run 1, Epoch 72, Loss: 3.365, Training Accuracy: 20.35%\n",
      "Run 1, Epoch 73, Loss: 3.353, Training Accuracy: 20.56%\n",
      "Run 1, Epoch 74, Loss: 3.358, Training Accuracy: 20.48%\n",
      "Run 1, Epoch 75, Loss: 3.360, Training Accuracy: 20.42%\n",
      "Run 1, Epoch 76, Loss: 3.361, Training Accuracy: 20.66%\n",
      "Run 1, Epoch 77, Loss: 3.362, Training Accuracy: 20.57%\n",
      "Run 1, Epoch 78, Loss: 3.360, Training Accuracy: 20.55%\n",
      "Run 1, Epoch 79, Loss: 3.364, Training Accuracy: 20.36%\n",
      "Run 1, Epoch 80, Loss: 3.363, Training Accuracy: 20.37%\n",
      "Run 1, Epoch 81, Loss: 3.361, Training Accuracy: 20.41%\n",
      "Run 1, Epoch 82, Loss: 3.358, Training Accuracy: 20.48%\n",
      "Run 1, Epoch 83, Loss: 3.363, Training Accuracy: 20.44%\n",
      "Run 1, Epoch 84, Loss: 3.359, Training Accuracy: 20.28%\n",
      "Run 1, Epoch 85, Loss: 3.362, Training Accuracy: 20.33%\n",
      "Run 1, Epoch 86, Loss: 3.357, Training Accuracy: 20.44%\n",
      "Run 1, Epoch 87, Loss: 3.357, Training Accuracy: 20.44%\n",
      "Run 1, Epoch 88, Loss: 3.357, Training Accuracy: 20.63%\n",
      "Run 1, Epoch 89, Loss: 3.361, Training Accuracy: 20.27%\n",
      "Run 1, Epoch 90, Loss: 3.360, Training Accuracy: 20.53%\n",
      "Run 1, Epoch 91, Loss: 3.354, Training Accuracy: 20.59%\n",
      "Run 1, Epoch 92, Loss: 3.359, Training Accuracy: 20.51%\n",
      "Run 1, Epoch 93, Loss: 3.358, Training Accuracy: 20.42%\n",
      "Run 1, Epoch 94, Loss: 3.359, Training Accuracy: 20.30%\n",
      "Run 1, Epoch 95, Loss: 3.363, Training Accuracy: 20.45%\n",
      "Run 1, Epoch 96, Loss: 3.360, Training Accuracy: 20.67%\n",
      "Run 1, Epoch 97, Loss: 3.356, Training Accuracy: 20.62%\n",
      "Run 1, Epoch 98, Loss: 3.357, Training Accuracy: 20.37%\n",
      "Run 1, Epoch 99, Loss: 3.361, Training Accuracy: 20.47%\n",
      "Run 1, Epoch 100, Loss: 3.358, Training Accuracy: 20.41%\n",
      "Run 1, Epoch 101, Loss: 3.359, Training Accuracy: 20.67%\n",
      "Run 1, Epoch 102, Loss: 3.363, Training Accuracy: 20.48%\n",
      "Run 1, Epoch 103, Loss: 3.361, Training Accuracy: 20.38%\n",
      "Run 1, Epoch 104, Loss: 3.357, Training Accuracy: 20.65%\n",
      "Run 1, Epoch 105, Loss: 3.354, Training Accuracy: 20.62%\n",
      "Run 1, Epoch 106, Loss: 3.361, Training Accuracy: 20.48%\n",
      "Run 1, Epoch 107, Loss: 3.360, Training Accuracy: 20.46%\n",
      "Run 1, Epoch 108, Loss: 3.357, Training Accuracy: 20.55%\n",
      "Run 1, Epoch 109, Loss: 3.359, Training Accuracy: 20.43%\n",
      "Run 1, Epoch 110, Loss: 3.362, Training Accuracy: 20.31%\n",
      "Run 1, Epoch 111, Loss: 3.354, Training Accuracy: 20.72%\n",
      "Run 1, Epoch 112, Loss: 3.361, Training Accuracy: 20.59%\n",
      "Run 1, Epoch 113, Loss: 3.359, Training Accuracy: 20.57%\n",
      "Run 1, Epoch 114, Loss: 3.362, Training Accuracy: 20.36%\n",
      "Run 1, Epoch 115, Loss: 3.363, Training Accuracy: 20.48%\n",
      "Run 1, Epoch 116, Loss: 3.361, Training Accuracy: 20.42%\n",
      "Run 1, Epoch 117, Loss: 3.359, Training Accuracy: 20.35%\n",
      "Run 1, Epoch 118, Loss: 3.359, Training Accuracy: 20.25%\n",
      "Run 1, Epoch 119, Loss: 3.359, Training Accuracy: 20.37%\n",
      "Run 1, Epoch 120, Loss: 3.360, Training Accuracy: 20.83%\n",
      "Run 1, Epoch 121, Loss: 3.360, Training Accuracy: 20.69%\n",
      "Run 1, Epoch 122, Loss: 3.364, Training Accuracy: 20.41%\n",
      "Run 1, Epoch 123, Loss: 3.360, Training Accuracy: 20.38%\n",
      "Run 1, Epoch 124, Loss: 3.361, Training Accuracy: 20.36%\n",
      "Run 1, Epoch 125, Loss: 3.361, Training Accuracy: 20.45%\n",
      "Run 1, Epoch 126, Loss: 3.357, Training Accuracy: 20.38%\n",
      "Run 1, Epoch 127, Loss: 3.357, Training Accuracy: 20.71%\n",
      "Run 1, Epoch 128, Loss: 3.355, Training Accuracy: 20.54%\n",
      "Run 1, Epoch 129, Loss: 3.356, Training Accuracy: 20.53%\n",
      "Run 1, Epoch 130, Loss: 3.359, Training Accuracy: 20.44%\n",
      "Run 1, Epoch 131, Loss: 3.359, Training Accuracy: 20.48%\n",
      "Run 1, Epoch 132, Loss: 3.357, Training Accuracy: 20.30%\n",
      "Run 1, Epoch 133, Loss: 3.360, Training Accuracy: 20.47%\n",
      "Run 1, Epoch 134, Loss: 3.357, Training Accuracy: 20.49%\n",
      "Run 1, Epoch 135, Loss: 3.360, Training Accuracy: 20.42%\n",
      "Run 1, Epoch 136, Loss: 3.362, Training Accuracy: 20.53%\n",
      "Run 1, Epoch 137, Loss: 3.362, Training Accuracy: 20.53%\n",
      "Run 1, Epoch 138, Loss: 3.355, Training Accuracy: 20.53%\n",
      "Run 1, Epoch 139, Loss: 3.359, Training Accuracy: 20.64%\n",
      "Run 1, Epoch 140, Loss: 3.357, Training Accuracy: 20.55%\n",
      "Run 1, Epoch 141, Loss: 3.358, Training Accuracy: 20.65%\n",
      "Run 1, Epoch 142, Loss: 3.355, Training Accuracy: 20.52%\n",
      "Run 1, Epoch 143, Loss: 3.360, Training Accuracy: 20.43%\n",
      "Run 1, Epoch 144, Loss: 3.360, Training Accuracy: 20.70%\n",
      "Run 1, Epoch 145, Loss: 3.359, Training Accuracy: 20.47%\n",
      "Run 1, Epoch 146, Loss: 3.355, Training Accuracy: 20.68%\n",
      "Run 1, Epoch 147, Loss: 3.361, Training Accuracy: 20.55%\n",
      "Run 1, Epoch 148, Loss: 3.361, Training Accuracy: 20.46%\n",
      "Run 1, Epoch 149, Loss: 3.361, Training Accuracy: 20.60%\n",
      "Run 1, Epoch 150, Loss: 3.359, Training Accuracy: 20.42%\n",
      "Run 1, Epoch 151, Loss: 3.360, Training Accuracy: 20.34%\n",
      "Run 1, Epoch 152, Loss: 3.355, Training Accuracy: 20.62%\n",
      "Run 1, Epoch 153, Loss: 3.362, Training Accuracy: 20.56%\n",
      "Run 1, Epoch 154, Loss: 3.360, Training Accuracy: 20.32%\n",
      "Run 1, Epoch 155, Loss: 3.357, Training Accuracy: 20.55%\n",
      "Run 1, Epoch 156, Loss: 3.360, Training Accuracy: 20.52%\n",
      "Run 1, Epoch 157, Loss: 3.358, Training Accuracy: 20.56%\n",
      "Run 1, Epoch 158, Loss: 3.357, Training Accuracy: 20.64%\n",
      "Run 1, Epoch 159, Loss: 3.356, Training Accuracy: 20.45%\n",
      "Run 1, Epoch 160, Loss: 3.361, Training Accuracy: 20.53%\n",
      "Run 1, Epoch 161, Loss: 3.360, Training Accuracy: 20.49%\n",
      "Run 1, Epoch 162, Loss: 3.360, Training Accuracy: 20.35%\n",
      "Run 1, Epoch 163, Loss: 3.354, Training Accuracy: 20.64%\n",
      "Run 1, Epoch 164, Loss: 3.359, Training Accuracy: 20.43%\n",
      "Run 1, Epoch 165, Loss: 3.361, Training Accuracy: 20.44%\n",
      "Run 1, Epoch 166, Loss: 3.354, Training Accuracy: 20.45%\n",
      "Run 1, Epoch 167, Loss: 3.361, Training Accuracy: 20.49%\n",
      "Run 1, Epoch 168, Loss: 3.362, Training Accuracy: 20.32%\n",
      "Run 1, Epoch 169, Loss: 3.356, Training Accuracy: 20.56%\n",
      "Run 1, Epoch 170, Loss: 3.360, Training Accuracy: 20.34%\n",
      "Run 1, Epoch 171, Loss: 3.357, Training Accuracy: 20.69%\n",
      "Run 1, Epoch 172, Loss: 3.358, Training Accuracy: 20.74%\n",
      "Run 1, Epoch 173, Loss: 3.354, Training Accuracy: 20.64%\n",
      "Run 1, Epoch 174, Loss: 3.355, Training Accuracy: 20.62%\n",
      "Run 1, Epoch 175, Loss: 3.365, Training Accuracy: 20.61%\n",
      "Run 1, Final Accuracy on test set: 22.22%\n",
      "Results after run 1:\n",
      "Training Accuracies: [6.012, 9.642, 11.594, 13.278, 14.682, 15.69, 16.442, 17.27, 17.748, 18.506, 19.588, 19.764, 20.108, 20.012, 19.964, 20.092, 19.982, 20.26, 20.14, 20.54, 20.456, 20.416, 20.346, 20.518, 20.39, 20.45, 20.5, 20.418, 20.282, 20.404, 20.294, 20.556, 20.53, 20.538, 20.49, 20.412, 20.624, 20.37, 20.426, 20.496, 20.596, 20.502, 20.434, 20.544, 20.454, 20.668, 20.366, 20.514, 20.366, 20.386, 20.458, 20.772, 20.464, 20.578, 20.664, 20.572, 20.4, 20.576, 20.342, 20.55, 20.428, 20.648, 20.598, 20.528, 20.538, 20.558, 20.266, 20.48, 20.276, 20.486, 20.516, 20.35, 20.564, 20.482, 20.418, 20.66, 20.566, 20.55, 20.364, 20.368, 20.412, 20.48, 20.44, 20.276, 20.326, 20.442, 20.438, 20.634, 20.266, 20.534, 20.588, 20.514, 20.42, 20.296, 20.452, 20.666, 20.624, 20.368, 20.47, 20.408, 20.666, 20.478, 20.38, 20.648, 20.622, 20.484, 20.456, 20.552, 20.434, 20.306, 20.716, 20.586, 20.566, 20.364, 20.478, 20.416, 20.348, 20.252, 20.366, 20.83, 20.69, 20.41, 20.382, 20.36, 20.454, 20.384, 20.71, 20.54, 20.534, 20.44, 20.476, 20.298, 20.474, 20.492, 20.416, 20.526, 20.526, 20.532, 20.64, 20.552, 20.654, 20.52, 20.432, 20.704, 20.47, 20.678, 20.552, 20.456, 20.596, 20.416, 20.34, 20.624, 20.556, 20.32, 20.546, 20.516, 20.56, 20.638, 20.45, 20.534, 20.49, 20.35, 20.642, 20.43, 20.444, 20.448, 20.486, 20.322, 20.56, 20.342, 20.688, 20.738, 20.638, 20.62, 20.614]\n",
      "Test Accuracy: 22.22%\n",
      "Losses: [4.22398068593896, 3.982602045664092, 3.8619770357370986, 3.7592333216801324, 3.6851437146706347, 3.6217030173982194, 3.5704678098868836, 3.531395924365734, 3.490226215108886, 3.4576507584213294, 3.4086997905350707, 3.4004000629610416, 3.3889379190362017, 3.387403710723838, 3.3841701249027496, 3.3852225057304364, 3.3803668949000367, 3.3780832168696175, 3.3788389275445962, 3.3697201730040334, 3.364734926492052, 3.3626377375229546, 3.3638622785163355, 3.3630743142588972, 3.3654833224118517, 3.360938568554266, 3.361631323309506, 3.3660096664867742, 3.362048013131027, 3.35864120127295, 3.3619371375159535, 3.3595561365337323, 3.352974652024486, 3.358923902901847, 3.3595146858478753, 3.3655502046160684, 3.3590840646982802, 3.358379896339553, 3.3585699324107843, 3.3605956677585613, 3.355360550648721, 3.3570975842683213, 3.3597506019465455, 3.3622026839829466, 3.3601955215034582, 3.3572324999153156, 3.3597971082038587, 3.360587188349965, 3.3588274305738754, 3.36074350069246, 3.359739173098903, 3.357256671656733, 3.3542822193916497, 3.3591746991247775, 3.3581336209231325, 3.3565347737363536, 3.3602990555336407, 3.357470649587529, 3.360219648731944, 3.3576033316609806, 3.3622290386873135, 3.357088765219959, 3.3568837734134607, 3.3545335442818645, 3.3609844449231083, 3.357329865550751, 3.358774527259495, 3.356124402921828, 3.3625747785543845, 3.360259833543197, 3.358701569344991, 3.3648390477270724, 3.3531731333574064, 3.35823356404024, 3.3597143178095905, 3.3610555271968208, 3.362339731372531, 3.359892942106632, 3.3641760818793647, 3.362670562456331, 3.3605220183692017, 3.3576725303669415, 3.362711041784652, 3.3586510392406104, 3.362498744369468, 3.356584834320771, 3.356968651037387, 3.3573079688469774, 3.361114514758215, 3.35964232210613, 3.353649361359189, 3.3592445002797313, 3.358108815024881, 3.3586752268359485, 3.3626998361114344, 3.3596850923260155, 3.355970411959207, 3.3567476925032826, 3.3610158905653695, 3.3580830810624924, 3.3587367687079, 3.363016970017377, 3.361266698373858, 3.35734456518422, 3.3538773261067814, 3.3605790876061716, 3.3601301307873346, 3.357370807081842, 3.3590723669437494, 3.362226849626702, 3.354247740772374, 3.3611545879822557, 3.359440868772814, 3.3617261108535024, 3.36251710077076, 3.3612966720405444, 3.358902826333595, 3.3592752922526405, 3.3591965214370765, 3.36027648991636, 3.3597338614256484, 3.3643247434855117, 3.3599514534406345, 3.361294878108422, 3.360695458434122, 3.3566190391550284, 3.3570660270388473, 3.3554347421202206, 3.355655247598048, 3.359081343921554, 3.3587015236125275, 3.3566186470753703, 3.3600545813665366, 3.357106180752025, 3.3596605992378175, 3.3624450648227313, 3.361821259988848, 3.354976776615738, 3.358844799153945, 3.3566550885320017, 3.3580497225837025, 3.3554366878841235, 3.360283482714992, 3.3601818060326147, 3.3588704093337975, 3.354895573442854, 3.361338941032624, 3.3609853461575323, 3.360738010967479, 3.359047361651955, 3.36046335825225, 3.354931502086122, 3.3616008929279455, 3.360033253574615, 3.357191709606239, 3.360141905982171, 3.35830932748897, 3.3571678433576815, 3.356257576466826, 3.361283587067938, 3.359607694094138, 3.35968728565499, 3.353869690614588, 3.359184151720208, 3.360860723363774, 3.3538906123022287, 3.360664422554738, 3.361753652162869, 3.3564429722173745, 3.360435033393333, 3.3570341862680966, 3.357584670376595, 3.3540455173043644, 3.3554249047623266, 3.364616139160703]\n",
      "Starting run 2/5\n",
      "Run 2, Epoch 1, Loss: 4.245, Training Accuracy: 6.09%\n",
      "Run 2, Epoch 2, Loss: 3.992, Training Accuracy: 9.73%\n",
      "Run 2, Epoch 3, Loss: 3.888, Training Accuracy: 11.38%\n",
      "Run 2, Epoch 4, Loss: 3.796, Training Accuracy: 12.85%\n",
      "Run 2, Epoch 5, Loss: 3.718, Training Accuracy: 14.08%\n",
      "Run 2, Epoch 6, Loss: 3.648, Training Accuracy: 15.43%\n",
      "Run 2, Epoch 7, Loss: 3.595, Training Accuracy: 16.03%\n",
      "Run 2, Epoch 8, Loss: 3.553, Training Accuracy: 16.78%\n",
      "Run 2, Epoch 9, Loss: 3.507, Training Accuracy: 17.57%\n",
      "Run 2, Epoch 10, Loss: 3.466, Training Accuracy: 18.28%\n",
      "Run 2, Epoch 11, Loss: 3.417, Training Accuracy: 19.41%\n",
      "Run 2, Epoch 12, Loss: 3.408, Training Accuracy: 19.73%\n",
      "Run 2, Epoch 13, Loss: 3.409, Training Accuracy: 19.79%\n",
      "Run 2, Epoch 14, Loss: 3.400, Training Accuracy: 19.93%\n",
      "Run 2, Epoch 15, Loss: 3.401, Training Accuracy: 19.81%\n",
      "Run 2, Epoch 16, Loss: 3.391, Training Accuracy: 20.08%\n",
      "Run 2, Epoch 17, Loss: 3.385, Training Accuracy: 20.03%\n",
      "Run 2, Epoch 18, Loss: 3.384, Training Accuracy: 20.11%\n",
      "Run 2, Epoch 19, Loss: 3.380, Training Accuracy: 20.37%\n",
      "Run 2, Epoch 20, Loss: 3.378, Training Accuracy: 20.20%\n",
      "Run 2, Epoch 21, Loss: 3.366, Training Accuracy: 20.60%\n",
      "Run 2, Epoch 22, Loss: 3.373, Training Accuracy: 20.24%\n",
      "Run 2, Epoch 23, Loss: 3.372, Training Accuracy: 20.30%\n",
      "Run 2, Epoch 24, Loss: 3.364, Training Accuracy: 20.60%\n",
      "Run 2, Epoch 25, Loss: 3.369, Training Accuracy: 20.37%\n",
      "Run 2, Epoch 26, Loss: 3.365, Training Accuracy: 20.62%\n",
      "Run 2, Epoch 27, Loss: 3.369, Training Accuracy: 20.41%\n",
      "Run 2, Epoch 28, Loss: 3.366, Training Accuracy: 20.62%\n",
      "Run 2, Epoch 29, Loss: 3.368, Training Accuracy: 20.37%\n",
      "Run 2, Epoch 30, Loss: 3.368, Training Accuracy: 20.49%\n",
      "Run 2, Epoch 31, Loss: 3.362, Training Accuracy: 20.53%\n",
      "Run 2, Epoch 32, Loss: 3.365, Training Accuracy: 20.35%\n",
      "Run 2, Epoch 33, Loss: 3.368, Training Accuracy: 20.49%\n",
      "Run 2, Epoch 34, Loss: 3.361, Training Accuracy: 20.52%\n",
      "Run 2, Epoch 35, Loss: 3.365, Training Accuracy: 20.48%\n",
      "Run 2, Epoch 36, Loss: 3.368, Training Accuracy: 20.44%\n",
      "Run 2, Epoch 37, Loss: 3.363, Training Accuracy: 20.61%\n",
      "Run 2, Epoch 38, Loss: 3.362, Training Accuracy: 20.50%\n",
      "Run 2, Epoch 39, Loss: 3.364, Training Accuracy: 20.65%\n",
      "Run 2, Epoch 40, Loss: 3.368, Training Accuracy: 20.18%\n",
      "Run 2, Epoch 41, Loss: 3.363, Training Accuracy: 20.61%\n",
      "Run 2, Epoch 42, Loss: 3.370, Training Accuracy: 20.36%\n",
      "Run 2, Epoch 43, Loss: 3.365, Training Accuracy: 20.45%\n",
      "Run 2, Epoch 44, Loss: 3.363, Training Accuracy: 20.44%\n",
      "Run 2, Epoch 45, Loss: 3.365, Training Accuracy: 20.67%\n",
      "Run 2, Epoch 46, Loss: 3.363, Training Accuracy: 20.42%\n",
      "Run 2, Epoch 47, Loss: 3.365, Training Accuracy: 20.56%\n",
      "Run 2, Epoch 48, Loss: 3.363, Training Accuracy: 20.50%\n",
      "Run 2, Epoch 49, Loss: 3.362, Training Accuracy: 20.50%\n",
      "Run 2, Epoch 50, Loss: 3.361, Training Accuracy: 20.48%\n",
      "Run 2, Epoch 51, Loss: 3.366, Training Accuracy: 20.38%\n",
      "Run 2, Epoch 52, Loss: 3.364, Training Accuracy: 20.57%\n",
      "Run 2, Epoch 53, Loss: 3.366, Training Accuracy: 20.48%\n",
      "Run 2, Epoch 54, Loss: 3.365, Training Accuracy: 20.59%\n",
      "Run 2, Epoch 55, Loss: 3.364, Training Accuracy: 20.45%\n",
      "Run 2, Epoch 56, Loss: 3.363, Training Accuracy: 20.45%\n",
      "Run 2, Epoch 57, Loss: 3.364, Training Accuracy: 20.62%\n",
      "Run 2, Epoch 58, Loss: 3.362, Training Accuracy: 20.65%\n",
      "Run 2, Epoch 59, Loss: 3.365, Training Accuracy: 20.56%\n",
      "Run 2, Epoch 60, Loss: 3.364, Training Accuracy: 20.55%\n",
      "Run 2, Epoch 61, Loss: 3.364, Training Accuracy: 20.55%\n",
      "Run 2, Epoch 62, Loss: 3.363, Training Accuracy: 20.58%\n",
      "Run 2, Epoch 63, Loss: 3.364, Training Accuracy: 20.51%\n",
      "Run 2, Epoch 64, Loss: 3.362, Training Accuracy: 20.36%\n",
      "Run 2, Epoch 65, Loss: 3.362, Training Accuracy: 20.45%\n",
      "Run 2, Epoch 66, Loss: 3.364, Training Accuracy: 20.54%\n",
      "Run 2, Epoch 67, Loss: 3.364, Training Accuracy: 20.54%\n",
      "Run 2, Epoch 68, Loss: 3.357, Training Accuracy: 20.59%\n",
      "Run 2, Epoch 69, Loss: 3.364, Training Accuracy: 20.51%\n",
      "Run 2, Epoch 70, Loss: 3.363, Training Accuracy: 20.52%\n",
      "Run 2, Epoch 71, Loss: 3.364, Training Accuracy: 20.55%\n",
      "Run 2, Epoch 72, Loss: 3.364, Training Accuracy: 20.59%\n",
      "Run 2, Epoch 73, Loss: 3.368, Training Accuracy: 20.42%\n",
      "Run 2, Epoch 74, Loss: 3.361, Training Accuracy: 20.44%\n",
      "Run 2, Epoch 75, Loss: 3.367, Training Accuracy: 20.39%\n",
      "Run 2, Epoch 76, Loss: 3.367, Training Accuracy: 20.38%\n",
      "Run 2, Epoch 77, Loss: 3.369, Training Accuracy: 20.33%\n",
      "Run 2, Epoch 78, Loss: 3.368, Training Accuracy: 20.76%\n",
      "Run 2, Epoch 79, Loss: 3.368, Training Accuracy: 20.24%\n",
      "Run 2, Epoch 80, Loss: 3.362, Training Accuracy: 20.52%\n",
      "Run 2, Epoch 81, Loss: 3.367, Training Accuracy: 20.34%\n",
      "Run 2, Epoch 82, Loss: 3.367, Training Accuracy: 20.60%\n",
      "Run 2, Epoch 83, Loss: 3.366, Training Accuracy: 20.53%\n",
      "Run 2, Epoch 84, Loss: 3.362, Training Accuracy: 20.57%\n",
      "Run 2, Epoch 85, Loss: 3.369, Training Accuracy: 20.29%\n",
      "Run 2, Epoch 86, Loss: 3.365, Training Accuracy: 20.39%\n",
      "Run 2, Epoch 87, Loss: 3.363, Training Accuracy: 20.60%\n",
      "Run 2, Epoch 88, Loss: 3.368, Training Accuracy: 20.42%\n",
      "Run 2, Epoch 89, Loss: 3.359, Training Accuracy: 20.60%\n",
      "Run 2, Epoch 90, Loss: 3.367, Training Accuracy: 20.22%\n",
      "Run 2, Epoch 91, Loss: 3.368, Training Accuracy: 20.47%\n",
      "Run 2, Epoch 92, Loss: 3.370, Training Accuracy: 20.38%\n",
      "Run 2, Epoch 93, Loss: 3.363, Training Accuracy: 20.57%\n",
      "Run 2, Epoch 94, Loss: 3.367, Training Accuracy: 20.34%\n",
      "Run 2, Epoch 95, Loss: 3.365, Training Accuracy: 20.49%\n",
      "Run 2, Epoch 96, Loss: 3.366, Training Accuracy: 20.63%\n",
      "Run 2, Epoch 97, Loss: 3.368, Training Accuracy: 20.30%\n",
      "Run 2, Epoch 98, Loss: 3.365, Training Accuracy: 20.63%\n",
      "Run 2, Epoch 99, Loss: 3.362, Training Accuracy: 20.55%\n",
      "Run 2, Epoch 100, Loss: 3.364, Training Accuracy: 20.63%\n",
      "Run 2, Epoch 101, Loss: 3.364, Training Accuracy: 20.40%\n",
      "Run 2, Epoch 102, Loss: 3.365, Training Accuracy: 20.46%\n",
      "Run 2, Epoch 103, Loss: 3.364, Training Accuracy: 20.40%\n",
      "Run 2, Epoch 104, Loss: 3.361, Training Accuracy: 20.46%\n",
      "Run 2, Epoch 105, Loss: 3.365, Training Accuracy: 20.39%\n",
      "Run 2, Epoch 106, Loss: 3.367, Training Accuracy: 20.42%\n",
      "Run 2, Epoch 107, Loss: 3.363, Training Accuracy: 20.71%\n",
      "Run 2, Epoch 108, Loss: 3.365, Training Accuracy: 20.50%\n",
      "Run 2, Epoch 109, Loss: 3.370, Training Accuracy: 20.35%\n",
      "Run 2, Epoch 110, Loss: 3.363, Training Accuracy: 20.63%\n",
      "Run 2, Epoch 111, Loss: 3.368, Training Accuracy: 20.53%\n",
      "Run 2, Epoch 112, Loss: 3.362, Training Accuracy: 20.48%\n",
      "Run 2, Epoch 113, Loss: 3.364, Training Accuracy: 20.40%\n",
      "Run 2, Epoch 114, Loss: 3.359, Training Accuracy: 20.65%\n",
      "Run 2, Epoch 115, Loss: 3.365, Training Accuracy: 20.39%\n",
      "Run 2, Epoch 116, Loss: 3.368, Training Accuracy: 20.38%\n",
      "Run 2, Epoch 117, Loss: 3.363, Training Accuracy: 20.63%\n",
      "Run 2, Epoch 118, Loss: 3.363, Training Accuracy: 20.43%\n",
      "Run 2, Epoch 119, Loss: 3.365, Training Accuracy: 20.38%\n",
      "Run 2, Epoch 120, Loss: 3.366, Training Accuracy: 20.57%\n",
      "Run 2, Epoch 121, Loss: 3.362, Training Accuracy: 20.55%\n",
      "Run 2, Epoch 122, Loss: 3.363, Training Accuracy: 20.61%\n",
      "Run 2, Epoch 123, Loss: 3.363, Training Accuracy: 20.34%\n",
      "Run 2, Epoch 124, Loss: 3.358, Training Accuracy: 20.81%\n",
      "Run 2, Epoch 125, Loss: 3.364, Training Accuracy: 20.58%\n",
      "Run 2, Epoch 126, Loss: 3.363, Training Accuracy: 20.54%\n",
      "Run 2, Epoch 127, Loss: 3.365, Training Accuracy: 20.37%\n",
      "Run 2, Epoch 128, Loss: 3.362, Training Accuracy: 20.65%\n",
      "Run 2, Epoch 129, Loss: 3.368, Training Accuracy: 20.43%\n",
      "Run 2, Epoch 130, Loss: 3.362, Training Accuracy: 20.47%\n",
      "Run 2, Epoch 131, Loss: 3.363, Training Accuracy: 20.54%\n",
      "Run 2, Epoch 132, Loss: 3.363, Training Accuracy: 20.51%\n",
      "Run 2, Epoch 133, Loss: 3.362, Training Accuracy: 20.39%\n",
      "Run 2, Epoch 134, Loss: 3.362, Training Accuracy: 20.58%\n",
      "Run 2, Epoch 135, Loss: 3.366, Training Accuracy: 20.33%\n",
      "Run 2, Epoch 136, Loss: 3.362, Training Accuracy: 20.59%\n",
      "Run 2, Epoch 137, Loss: 3.363, Training Accuracy: 20.67%\n",
      "Run 2, Epoch 138, Loss: 3.358, Training Accuracy: 20.77%\n",
      "Run 2, Epoch 139, Loss: 3.365, Training Accuracy: 20.58%\n",
      "Run 2, Epoch 140, Loss: 3.367, Training Accuracy: 20.16%\n",
      "Run 2, Epoch 141, Loss: 3.362, Training Accuracy: 20.65%\n",
      "Run 2, Epoch 142, Loss: 3.364, Training Accuracy: 20.50%\n",
      "Run 2, Epoch 143, Loss: 3.369, Training Accuracy: 20.43%\n",
      "Run 2, Epoch 144, Loss: 3.362, Training Accuracy: 20.56%\n",
      "Run 2, Epoch 145, Loss: 3.367, Training Accuracy: 20.48%\n",
      "Run 2, Epoch 146, Loss: 3.361, Training Accuracy: 20.29%\n",
      "Run 2, Epoch 147, Loss: 3.365, Training Accuracy: 20.67%\n",
      "Run 2, Epoch 148, Loss: 3.366, Training Accuracy: 20.44%\n",
      "Run 2, Epoch 149, Loss: 3.363, Training Accuracy: 20.41%\n",
      "Run 2, Epoch 150, Loss: 3.362, Training Accuracy: 20.44%\n",
      "Run 2, Epoch 151, Loss: 3.366, Training Accuracy: 20.53%\n",
      "Run 2, Epoch 152, Loss: 3.367, Training Accuracy: 20.49%\n",
      "Run 2, Epoch 153, Loss: 3.364, Training Accuracy: 20.37%\n",
      "Run 2, Epoch 154, Loss: 3.364, Training Accuracy: 20.59%\n",
      "Run 2, Epoch 155, Loss: 3.367, Training Accuracy: 20.47%\n",
      "Run 2, Epoch 156, Loss: 3.365, Training Accuracy: 20.26%\n",
      "Run 2, Epoch 157, Loss: 3.361, Training Accuracy: 20.52%\n",
      "Run 2, Epoch 158, Loss: 3.367, Training Accuracy: 20.45%\n",
      "Run 2, Epoch 159, Loss: 3.363, Training Accuracy: 20.57%\n",
      "Run 2, Epoch 160, Loss: 3.364, Training Accuracy: 20.44%\n",
      "Run 2, Epoch 161, Loss: 3.366, Training Accuracy: 20.49%\n",
      "Run 2, Epoch 162, Loss: 3.363, Training Accuracy: 20.72%\n",
      "Run 2, Epoch 163, Loss: 3.360, Training Accuracy: 20.62%\n",
      "Run 2, Epoch 164, Loss: 3.366, Training Accuracy: 20.53%\n",
      "Run 2, Epoch 165, Loss: 3.365, Training Accuracy: 20.61%\n",
      "Run 2, Epoch 166, Loss: 3.367, Training Accuracy: 20.69%\n",
      "Run 2, Epoch 167, Loss: 3.365, Training Accuracy: 20.61%\n",
      "Run 2, Epoch 168, Loss: 3.364, Training Accuracy: 20.25%\n",
      "Run 2, Epoch 169, Loss: 3.360, Training Accuracy: 20.44%\n",
      "Run 2, Epoch 170, Loss: 3.365, Training Accuracy: 20.59%\n",
      "Run 2, Epoch 171, Loss: 3.364, Training Accuracy: 20.70%\n",
      "Run 2, Epoch 172, Loss: 3.361, Training Accuracy: 20.73%\n",
      "Run 2, Epoch 173, Loss: 3.362, Training Accuracy: 20.54%\n",
      "Run 2, Epoch 174, Loss: 3.366, Training Accuracy: 20.53%\n",
      "Run 2, Epoch 175, Loss: 3.365, Training Accuracy: 20.47%\n",
      "Run 2, Final Accuracy on test set: 22.38%\n",
      "Results after run 2:\n",
      "Training Accuracies: [6.09, 9.726, 11.376, 12.848, 14.076, 15.428, 16.032, 16.782, 17.57, 18.278, 19.41, 19.726, 19.79, 19.928, 19.808, 20.078, 20.028, 20.112, 20.37, 20.202, 20.598, 20.24, 20.3, 20.6, 20.368, 20.62, 20.414, 20.62, 20.374, 20.486, 20.534, 20.35, 20.486, 20.52, 20.482, 20.44, 20.614, 20.502, 20.65, 20.184, 20.608, 20.364, 20.45, 20.436, 20.668, 20.416, 20.56, 20.498, 20.5, 20.48, 20.382, 20.57, 20.478, 20.588, 20.45, 20.448, 20.62, 20.646, 20.564, 20.548, 20.548, 20.582, 20.514, 20.358, 20.45, 20.536, 20.536, 20.588, 20.506, 20.52, 20.548, 20.586, 20.416, 20.44, 20.392, 20.378, 20.326, 20.756, 20.24, 20.516, 20.344, 20.598, 20.532, 20.574, 20.29, 20.39, 20.6, 20.416, 20.602, 20.22, 20.47, 20.382, 20.572, 20.342, 20.494, 20.634, 20.304, 20.63, 20.55, 20.634, 20.404, 20.462, 20.402, 20.464, 20.388, 20.42, 20.712, 20.502, 20.352, 20.634, 20.526, 20.478, 20.398, 20.65, 20.392, 20.384, 20.628, 20.432, 20.38, 20.572, 20.55, 20.608, 20.338, 20.806, 20.584, 20.54, 20.372, 20.652, 20.426, 20.474, 20.542, 20.512, 20.392, 20.584, 20.33, 20.59, 20.668, 20.768, 20.578, 20.164, 20.65, 20.504, 20.426, 20.564, 20.478, 20.29, 20.666, 20.438, 20.41, 20.442, 20.532, 20.49, 20.372, 20.592, 20.466, 20.26, 20.524, 20.448, 20.572, 20.436, 20.49, 20.718, 20.616, 20.534, 20.61, 20.69, 20.606, 20.254, 20.436, 20.586, 20.7, 20.732, 20.538, 20.526, 20.472]\n",
      "Test Accuracy: 22.38%\n",
      "Losses: [4.244624415931799, 3.991582582063992, 3.8884118030138333, 3.7959935506591407, 3.71770143691841, 3.648440976277032, 3.595421983762775, 3.5531216228709503, 3.5069584761129318, 3.466013374840817, 3.416850241858636, 3.4079018681860336, 3.4092486075428137, 3.399847516013533, 3.4008840545059167, 3.390582236487542, 3.3852374834172867, 3.383881156706749, 3.3799004426697636, 3.3775010987011065, 3.366172713696804, 3.3734137816807195, 3.3716880165402543, 3.3641286695094976, 3.368983278494052, 3.364837013547073, 3.369234584481515, 3.365735471096185, 3.3680420726766367, 3.367689991850987, 3.3622878128305422, 3.365367119879369, 3.3676042562860355, 3.3605008808243304, 3.3647559018391173, 3.3678957568410106, 3.363216437951988, 3.3615330283904012, 3.3639659284020933, 3.3678440937910543, 3.3629453407833947, 3.3696133627001283, 3.3648997469021538, 3.3625020877174707, 3.3651312556108244, 3.363178599521022, 3.3647895926404794, 3.362790627857608, 3.362300072789497, 3.3611171257770276, 3.365739455918217, 3.3637774136974987, 3.366035241300188, 3.3647050552660853, 3.364484030267467, 3.363422011475429, 3.363961575891051, 3.361929016040109, 3.3647937616118995, 3.363831515202437, 3.3637116687072206, 3.3626014204586254, 3.363569025493339, 3.3616319647835344, 3.3621205143306567, 3.3640502694317753, 3.3641516826951596, 3.3570196360273434, 3.3639763822336026, 3.3631443556617286, 3.364340699847092, 3.363719069439432, 3.3677142355448146, 3.361099440118541, 3.3673843404521113, 3.3668419326967594, 3.368588934164218, 3.3681623478374823, 3.3682220726061964, 3.361812002518598, 3.366861000695192, 3.3673312182316693, 3.366497147113771, 3.3621772213665118, 3.3693728178663327, 3.3645837855765888, 3.363161827292284, 3.367854007979488, 3.3594308771440744, 3.36689863729355, 3.3680449867492444, 3.370143931235194, 3.3627657408604534, 3.366988768662943, 3.3653765121079466, 3.365569948845202, 3.3681019730580126, 3.3650569909673824, 3.36224376271143, 3.363801079332981, 3.36393760781154, 3.3652059367245726, 3.3636361107496957, 3.361014341759255, 3.365071748528639, 3.3667963465766224, 3.3633172121804082, 3.3646571898399413, 3.370425755410548, 3.363384354754787, 3.3675566640351433, 3.3615545433805423, 3.3639200556918483, 3.359447668275565, 3.365062463009144, 3.367620730948875, 3.362852926449398, 3.362501145628712, 3.365269970101164, 3.3655848795800565, 3.362064660357697, 3.3629478597275133, 3.362790862007824, 3.3581730081602132, 3.3635321193948733, 3.3629647904954605, 3.3647349271018183, 3.3618206215636506, 3.3677834251042826, 3.3620136875630644, 3.362840179287259, 3.3634960346514613, 3.3624895978766633, 3.362097736514743, 3.365602976220953, 3.3618725925455313, 3.3630410893189024, 3.3583217258648492, 3.3647662648154646, 3.3672074381347814, 3.361948260870736, 3.3643775925307016, 3.368594142177221, 3.3624634779322786, 3.367182574918508, 3.3611831183323773, 3.3650490603483547, 3.3661600165354932, 3.3634482491046875, 3.3616600329308866, 3.3657495999885034, 3.366666685284861, 3.3643827255424634, 3.3644460582977063, 3.3674965545039655, 3.3647822980075848, 3.361178690820094, 3.3672368215477984, 3.3634876942695557, 3.3636625831389364, 3.366309131197917, 3.3626406540346268, 3.360158166617079, 3.3661873102797877, 3.3646892129307817, 3.3666608925060846, 3.3646340388471208, 3.3642834001185036, 3.3604381425911205, 3.3648181418933527, 3.3638023305731966, 3.360615467476418, 3.3624238919114213, 3.3657463200561835, 3.3646299314620856]\n",
      "Starting run 3/5\n",
      "Run 3, Epoch 1, Loss: 4.236, Training Accuracy: 6.21%\n",
      "Run 3, Epoch 2, Loss: 3.987, Training Accuracy: 9.71%\n",
      "Run 3, Epoch 3, Loss: 3.869, Training Accuracy: 11.73%\n",
      "Run 3, Epoch 4, Loss: 3.784, Training Accuracy: 13.19%\n",
      "Run 3, Epoch 5, Loss: 3.713, Training Accuracy: 14.24%\n",
      "Run 3, Epoch 6, Loss: 3.657, Training Accuracy: 15.18%\n",
      "Run 3, Epoch 7, Loss: 3.602, Training Accuracy: 16.23%\n",
      "Run 3, Epoch 8, Loss: 3.551, Training Accuracy: 16.84%\n",
      "Run 3, Epoch 9, Loss: 3.507, Training Accuracy: 17.62%\n",
      "Run 3, Epoch 10, Loss: 3.468, Training Accuracy: 18.37%\n",
      "Run 3, Epoch 11, Loss: 3.414, Training Accuracy: 19.40%\n",
      "Run 3, Epoch 12, Loss: 3.404, Training Accuracy: 19.88%\n",
      "Run 3, Epoch 13, Loss: 3.399, Training Accuracy: 19.86%\n",
      "Run 3, Epoch 14, Loss: 3.392, Training Accuracy: 20.02%\n",
      "Run 3, Epoch 15, Loss: 3.393, Training Accuracy: 20.06%\n",
      "Run 3, Epoch 16, Loss: 3.383, Training Accuracy: 20.38%\n",
      "Run 3, Epoch 17, Loss: 3.387, Training Accuracy: 20.21%\n",
      "Run 3, Epoch 18, Loss: 3.374, Training Accuracy: 20.21%\n",
      "Run 3, Epoch 19, Loss: 3.374, Training Accuracy: 20.28%\n",
      "Run 3, Epoch 20, Loss: 3.369, Training Accuracy: 20.50%\n",
      "Run 3, Epoch 21, Loss: 3.365, Training Accuracy: 20.61%\n",
      "Run 3, Epoch 22, Loss: 3.358, Training Accuracy: 20.78%\n",
      "Run 3, Epoch 23, Loss: 3.362, Training Accuracy: 20.63%\n",
      "Run 3, Epoch 24, Loss: 3.364, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 25, Loss: 3.362, Training Accuracy: 20.30%\n",
      "Run 3, Epoch 26, Loss: 3.361, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 27, Loss: 3.356, Training Accuracy: 20.84%\n",
      "Run 3, Epoch 28, Loss: 3.360, Training Accuracy: 20.48%\n",
      "Run 3, Epoch 29, Loss: 3.355, Training Accuracy: 20.65%\n",
      "Run 3, Epoch 30, Loss: 3.362, Training Accuracy: 20.56%\n",
      "Run 3, Epoch 31, Loss: 3.353, Training Accuracy: 20.73%\n",
      "Run 3, Epoch 32, Loss: 3.358, Training Accuracy: 20.72%\n",
      "Run 3, Epoch 33, Loss: 3.358, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 34, Loss: 3.356, Training Accuracy: 20.58%\n",
      "Run 3, Epoch 35, Loss: 3.361, Training Accuracy: 20.55%\n",
      "Run 3, Epoch 36, Loss: 3.363, Training Accuracy: 20.49%\n",
      "Run 3, Epoch 37, Loss: 3.357, Training Accuracy: 20.78%\n",
      "Run 3, Epoch 38, Loss: 3.358, Training Accuracy: 20.79%\n",
      "Run 3, Epoch 39, Loss: 3.351, Training Accuracy: 20.83%\n",
      "Run 3, Epoch 40, Loss: 3.357, Training Accuracy: 20.82%\n",
      "Run 3, Epoch 41, Loss: 3.356, Training Accuracy: 20.76%\n",
      "Run 3, Epoch 42, Loss: 3.354, Training Accuracy: 20.80%\n",
      "Run 3, Epoch 43, Loss: 3.359, Training Accuracy: 20.64%\n",
      "Run 3, Epoch 44, Loss: 3.361, Training Accuracy: 20.62%\n",
      "Run 3, Epoch 45, Loss: 3.354, Training Accuracy: 21.00%\n",
      "Run 3, Epoch 46, Loss: 3.351, Training Accuracy: 20.63%\n",
      "Run 3, Epoch 47, Loss: 3.360, Training Accuracy: 20.60%\n",
      "Run 3, Epoch 48, Loss: 3.357, Training Accuracy: 20.51%\n",
      "Run 3, Epoch 49, Loss: 3.358, Training Accuracy: 20.76%\n",
      "Run 3, Epoch 50, Loss: 3.360, Training Accuracy: 20.70%\n",
      "Run 3, Epoch 51, Loss: 3.356, Training Accuracy: 20.86%\n",
      "Run 3, Epoch 52, Loss: 3.356, Training Accuracy: 20.66%\n",
      "Run 3, Epoch 53, Loss: 3.360, Training Accuracy: 20.67%\n",
      "Run 3, Epoch 54, Loss: 3.360, Training Accuracy: 20.46%\n",
      "Run 3, Epoch 55, Loss: 3.354, Training Accuracy: 20.81%\n",
      "Run 3, Epoch 56, Loss: 3.360, Training Accuracy: 20.81%\n",
      "Run 3, Epoch 57, Loss: 3.354, Training Accuracy: 20.85%\n",
      "Run 3, Epoch 58, Loss: 3.361, Training Accuracy: 20.52%\n",
      "Run 3, Epoch 59, Loss: 3.351, Training Accuracy: 20.94%\n",
      "Run 3, Epoch 60, Loss: 3.358, Training Accuracy: 20.52%\n",
      "Run 3, Epoch 61, Loss: 3.359, Training Accuracy: 20.66%\n",
      "Run 3, Epoch 62, Loss: 3.357, Training Accuracy: 20.74%\n",
      "Run 3, Epoch 63, Loss: 3.357, Training Accuracy: 20.60%\n",
      "Run 3, Epoch 64, Loss: 3.355, Training Accuracy: 20.86%\n",
      "Run 3, Epoch 65, Loss: 3.356, Training Accuracy: 20.62%\n",
      "Run 3, Epoch 66, Loss: 3.361, Training Accuracy: 20.73%\n",
      "Run 3, Epoch 67, Loss: 3.360, Training Accuracy: 20.39%\n",
      "Run 3, Epoch 68, Loss: 3.362, Training Accuracy: 20.54%\n",
      "Run 3, Epoch 69, Loss: 3.360, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 70, Loss: 3.355, Training Accuracy: 20.64%\n",
      "Run 3, Epoch 71, Loss: 3.355, Training Accuracy: 20.72%\n",
      "Run 3, Epoch 72, Loss: 3.357, Training Accuracy: 20.67%\n",
      "Run 3, Epoch 73, Loss: 3.358, Training Accuracy: 20.70%\n",
      "Run 3, Epoch 74, Loss: 3.357, Training Accuracy: 20.63%\n",
      "Run 3, Epoch 75, Loss: 3.357, Training Accuracy: 20.63%\n",
      "Run 3, Epoch 76, Loss: 3.359, Training Accuracy: 20.84%\n",
      "Run 3, Epoch 77, Loss: 3.362, Training Accuracy: 20.79%\n",
      "Run 3, Epoch 78, Loss: 3.356, Training Accuracy: 20.81%\n",
      "Run 3, Epoch 79, Loss: 3.360, Training Accuracy: 20.52%\n",
      "Run 3, Epoch 80, Loss: 3.358, Training Accuracy: 20.73%\n",
      "Run 3, Epoch 81, Loss: 3.359, Training Accuracy: 20.68%\n",
      "Run 3, Epoch 82, Loss: 3.360, Training Accuracy: 20.61%\n",
      "Run 3, Epoch 83, Loss: 3.354, Training Accuracy: 20.72%\n",
      "Run 3, Epoch 84, Loss: 3.356, Training Accuracy: 20.57%\n",
      "Run 3, Epoch 85, Loss: 3.358, Training Accuracy: 20.79%\n",
      "Run 3, Epoch 86, Loss: 3.359, Training Accuracy: 20.67%\n",
      "Run 3, Epoch 87, Loss: 3.351, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 88, Loss: 3.356, Training Accuracy: 20.66%\n",
      "Run 3, Epoch 89, Loss: 3.356, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 90, Loss: 3.355, Training Accuracy: 20.98%\n",
      "Run 3, Epoch 91, Loss: 3.358, Training Accuracy: 20.75%\n",
      "Run 3, Epoch 92, Loss: 3.360, Training Accuracy: 20.78%\n",
      "Run 3, Epoch 93, Loss: 3.358, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 94, Loss: 3.354, Training Accuracy: 20.80%\n",
      "Run 3, Epoch 95, Loss: 3.355, Training Accuracy: 20.76%\n",
      "Run 3, Epoch 96, Loss: 3.359, Training Accuracy: 20.67%\n",
      "Run 3, Epoch 97, Loss: 3.360, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 98, Loss: 3.356, Training Accuracy: 20.47%\n",
      "Run 3, Epoch 99, Loss: 3.353, Training Accuracy: 20.84%\n",
      "Run 3, Epoch 100, Loss: 3.356, Training Accuracy: 20.73%\n",
      "Run 3, Epoch 101, Loss: 3.356, Training Accuracy: 20.57%\n",
      "Run 3, Epoch 102, Loss: 3.356, Training Accuracy: 20.78%\n",
      "Run 3, Epoch 103, Loss: 3.354, Training Accuracy: 20.89%\n",
      "Run 3, Epoch 104, Loss: 3.358, Training Accuracy: 20.60%\n",
      "Run 3, Epoch 105, Loss: 3.356, Training Accuracy: 20.66%\n",
      "Run 3, Epoch 106, Loss: 3.357, Training Accuracy: 20.62%\n",
      "Run 3, Epoch 107, Loss: 3.356, Training Accuracy: 20.80%\n",
      "Run 3, Epoch 108, Loss: 3.356, Training Accuracy: 20.68%\n",
      "Run 3, Epoch 109, Loss: 3.355, Training Accuracy: 20.61%\n",
      "Run 3, Epoch 110, Loss: 3.354, Training Accuracy: 20.70%\n",
      "Run 3, Epoch 111, Loss: 3.357, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 112, Loss: 3.358, Training Accuracy: 20.48%\n",
      "Run 3, Epoch 113, Loss: 3.357, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 114, Loss: 3.353, Training Accuracy: 20.76%\n",
      "Run 3, Epoch 115, Loss: 3.351, Training Accuracy: 20.55%\n",
      "Run 3, Epoch 116, Loss: 3.361, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 117, Loss: 3.356, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 118, Loss: 3.356, Training Accuracy: 20.67%\n",
      "Run 3, Epoch 119, Loss: 3.360, Training Accuracy: 20.61%\n",
      "Run 3, Epoch 120, Loss: 3.353, Training Accuracy: 20.77%\n",
      "Run 3, Epoch 121, Loss: 3.361, Training Accuracy: 20.69%\n",
      "Run 3, Epoch 122, Loss: 3.352, Training Accuracy: 20.78%\n",
      "Run 3, Epoch 123, Loss: 3.355, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 124, Loss: 3.358, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 125, Loss: 3.355, Training Accuracy: 20.52%\n",
      "Run 3, Epoch 126, Loss: 3.357, Training Accuracy: 20.60%\n",
      "Run 3, Epoch 127, Loss: 3.358, Training Accuracy: 20.73%\n",
      "Run 3, Epoch 128, Loss: 3.351, Training Accuracy: 20.86%\n",
      "Run 3, Epoch 129, Loss: 3.356, Training Accuracy: 20.77%\n",
      "Run 3, Epoch 130, Loss: 3.357, Training Accuracy: 20.64%\n",
      "Run 3, Epoch 131, Loss: 3.360, Training Accuracy: 20.48%\n",
      "Run 3, Epoch 132, Loss: 3.353, Training Accuracy: 20.66%\n",
      "Run 3, Epoch 133, Loss: 3.358, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 134, Loss: 3.360, Training Accuracy: 20.55%\n",
      "Run 3, Epoch 135, Loss: 3.353, Training Accuracy: 20.85%\n",
      "Run 3, Epoch 136, Loss: 3.358, Training Accuracy: 20.68%\n",
      "Run 3, Epoch 137, Loss: 3.359, Training Accuracy: 20.76%\n",
      "Run 3, Epoch 138, Loss: 3.363, Training Accuracy: 20.76%\n",
      "Run 3, Epoch 139, Loss: 3.355, Training Accuracy: 20.82%\n",
      "Run 3, Epoch 140, Loss: 3.358, Training Accuracy: 20.63%\n",
      "Run 3, Epoch 141, Loss: 3.355, Training Accuracy: 20.67%\n",
      "Run 3, Epoch 142, Loss: 3.357, Training Accuracy: 20.56%\n",
      "Run 3, Epoch 143, Loss: 3.357, Training Accuracy: 20.84%\n",
      "Run 3, Epoch 144, Loss: 3.361, Training Accuracy: 20.67%\n",
      "Run 3, Epoch 145, Loss: 3.359, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 146, Loss: 3.356, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 147, Loss: 3.354, Training Accuracy: 20.79%\n",
      "Run 3, Epoch 148, Loss: 3.356, Training Accuracy: 20.64%\n",
      "Run 3, Epoch 149, Loss: 3.358, Training Accuracy: 20.73%\n",
      "Run 3, Epoch 150, Loss: 3.357, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 151, Loss: 3.360, Training Accuracy: 20.84%\n",
      "Run 3, Epoch 152, Loss: 3.357, Training Accuracy: 20.72%\n",
      "Run 3, Epoch 153, Loss: 3.358, Training Accuracy: 20.65%\n",
      "Run 3, Epoch 154, Loss: 3.359, Training Accuracy: 20.64%\n",
      "Run 3, Epoch 155, Loss: 3.356, Training Accuracy: 20.69%\n",
      "Run 3, Epoch 156, Loss: 3.357, Training Accuracy: 20.71%\n",
      "Run 3, Epoch 157, Loss: 3.355, Training Accuracy: 20.75%\n",
      "Run 3, Epoch 158, Loss: 3.362, Training Accuracy: 20.58%\n",
      "Run 3, Epoch 159, Loss: 3.356, Training Accuracy: 20.66%\n",
      "Run 3, Epoch 160, Loss: 3.356, Training Accuracy: 20.83%\n",
      "Run 3, Epoch 161, Loss: 3.354, Training Accuracy: 20.65%\n",
      "Run 3, Epoch 162, Loss: 3.354, Training Accuracy: 20.81%\n",
      "Run 3, Epoch 163, Loss: 3.351, Training Accuracy: 20.59%\n",
      "Run 3, Epoch 164, Loss: 3.353, Training Accuracy: 20.91%\n",
      "Run 3, Epoch 165, Loss: 3.356, Training Accuracy: 20.88%\n",
      "Run 3, Epoch 166, Loss: 3.354, Training Accuracy: 20.81%\n",
      "Run 3, Epoch 167, Loss: 3.356, Training Accuracy: 20.88%\n",
      "Run 3, Epoch 168, Loss: 3.356, Training Accuracy: 20.68%\n",
      "Run 3, Epoch 169, Loss: 3.358, Training Accuracy: 20.50%\n",
      "Run 3, Epoch 170, Loss: 3.356, Training Accuracy: 20.67%\n",
      "Run 3, Epoch 171, Loss: 3.352, Training Accuracy: 20.93%\n",
      "Run 3, Epoch 172, Loss: 3.358, Training Accuracy: 20.58%\n",
      "Run 3, Epoch 173, Loss: 3.357, Training Accuracy: 20.85%\n",
      "Run 3, Epoch 174, Loss: 3.358, Training Accuracy: 20.62%\n",
      "Run 3, Epoch 175, Loss: 3.360, Training Accuracy: 20.78%\n",
      "Run 3, Final Accuracy on test set: 21.77%\n",
      "Results after run 3:\n",
      "Training Accuracies: [6.214, 9.708, 11.726, 13.188, 14.244, 15.176, 16.232, 16.844, 17.624, 18.374, 19.404, 19.882, 19.864, 20.02, 20.062, 20.382, 20.21, 20.21, 20.28, 20.496, 20.61, 20.776, 20.632, 20.592, 20.298, 20.59, 20.842, 20.478, 20.646, 20.558, 20.73, 20.72, 20.714, 20.578, 20.552, 20.492, 20.776, 20.786, 20.828, 20.818, 20.764, 20.8, 20.64, 20.62, 21.0, 20.628, 20.602, 20.514, 20.756, 20.698, 20.856, 20.66, 20.67, 20.456, 20.806, 20.808, 20.854, 20.524, 20.936, 20.524, 20.658, 20.742, 20.604, 20.856, 20.616, 20.732, 20.392, 20.54, 20.594, 20.64, 20.72, 20.666, 20.702, 20.626, 20.628, 20.84, 20.792, 20.806, 20.518, 20.734, 20.678, 20.608, 20.722, 20.568, 20.786, 20.666, 20.592, 20.664, 20.712, 20.98, 20.752, 20.782, 20.706, 20.804, 20.762, 20.67, 20.594, 20.472, 20.838, 20.726, 20.566, 20.776, 20.892, 20.596, 20.656, 20.62, 20.798, 20.676, 20.608, 20.696, 20.712, 20.484, 20.712, 20.756, 20.554, 20.592, 20.714, 20.672, 20.614, 20.768, 20.69, 20.784, 20.708, 20.712, 20.518, 20.598, 20.726, 20.858, 20.768, 20.64, 20.478, 20.66, 20.706, 20.552, 20.846, 20.684, 20.762, 20.764, 20.816, 20.628, 20.666, 20.564, 20.836, 20.67, 20.59, 20.712, 20.794, 20.636, 20.732, 20.594, 20.844, 20.724, 20.65, 20.644, 20.688, 20.712, 20.748, 20.578, 20.66, 20.826, 20.65, 20.814, 20.588, 20.908, 20.878, 20.814, 20.876, 20.678, 20.502, 20.67, 20.934, 20.582, 20.85, 20.62, 20.782]\n",
      "Test Accuracy: 21.77%\n",
      "Losses: [4.235795069228658, 3.9870263091133684, 3.8686093466970926, 3.7842250754461264, 3.713330532888622, 3.6572839933283188, 3.6019118149262255, 3.5505290805836163, 3.5073417269665264, 3.4676421164246776, 3.414271416261678, 3.4035414289635466, 3.3991007249983376, 3.3920280341906928, 3.392848672769259, 3.3832255952498493, 3.386930439478296, 3.3740469348400146, 3.374456286735242, 3.369053298555067, 3.364566876150458, 3.3575237657103085, 3.3621512404488176, 3.3643883384402145, 3.362279410252486, 3.3605847877004873, 3.3560900114991172, 3.360116655564369, 3.3549592446183305, 3.3617408659756944, 3.3526718281114194, 3.357611013495404, 3.358244708127073, 3.3559813078712013, 3.360728724838218, 3.3627381251596122, 3.3568302493571016, 3.3579548575994, 3.3506808902906333, 3.3565263418895204, 3.3564538925200167, 3.354217897595652, 3.359423376410209, 3.3605469821968956, 3.35409893952977, 3.350847193652102, 3.3595319623532505, 3.3565451665912445, 3.3575294554385993, 3.359762344823774, 3.3559577861405394, 3.3564337690163146, 3.360382916067567, 3.3601593879787512, 3.353735905474104, 3.3604773529960066, 3.3543690263157915, 3.3611385627170964, 3.3512561534676713, 3.3583533428514096, 3.3590883580620026, 3.35656960723955, 3.3565077123129763, 3.3545315643710554, 3.355952149461907, 3.361223158019278, 3.359959351742054, 3.3622208647715772, 3.3602082814706864, 3.355051666269522, 3.3548675028564374, 3.356820570538416, 3.3580691485148866, 3.3572342700665563, 3.356535175572271, 3.358574283702294, 3.36244720632158, 3.3558831964917197, 3.3603410422039763, 3.3584693533075436, 3.358603280523549, 3.359994826719279, 3.354055016242025, 3.3557719153821317, 3.3581879077969914, 3.3585550248470453, 3.3509057318158164, 3.3560095741925644, 3.356490415075551, 3.3546068235431488, 3.358483311465329, 3.359529708657423, 3.3581772594500685, 3.3543186364576334, 3.3549805102141006, 3.359173744230929, 3.3596711658760716, 3.35623575049593, 3.352757130132612, 3.3557716647682287, 3.356044360743764, 3.3561731409233855, 3.3543395752187277, 3.358063236221938, 3.3555157416311983, 3.3573858896484765, 3.355935593700165, 3.356078831436079, 3.3553522090472834, 3.3541274753677874, 3.357323398980338, 3.35839628502536, 3.3573193123273533, 3.3526358512966223, 3.35139944486301, 3.361496484493051, 3.3564015410440353, 3.356005486320047, 3.3599199834077256, 3.352854537841914, 3.3614898654810914, 3.3520559977997295, 3.354706789221605, 3.3576857122923713, 3.355353955417643, 3.3574651415695618, 3.3580556257301586, 3.350546999050833, 3.3559268648971985, 3.3573863067285483, 3.360043898262941, 3.3532267750986398, 3.3584788973679016, 3.3597839604253354, 3.352508410773314, 3.358186014472981, 3.359203834362957, 3.362580420111146, 3.3547582004381264, 3.3580205586865124, 3.3552198330764575, 3.357379720034197, 3.356605712105246, 3.360844387117859, 3.3593491812801117, 3.35564768649733, 3.3544816476914585, 3.3557094033721766, 3.357867352492974, 3.357060200722931, 3.3596777172039842, 3.35688713017632, 3.357865617708172, 3.3586239656219092, 3.3556868645846083, 3.3573225898206083, 3.3552939800350257, 3.361963347705734, 3.3561141350690056, 3.355933702815219, 3.354481042803401, 3.3536293774919437, 3.3512120826165086, 3.3527021212955876, 3.3559658259077145, 3.3539803991537265, 3.3556385674440037, 3.3558766347977818, 3.3577848121028424, 3.3558839782119714, 3.3523787665550056, 3.3581893785530346, 3.3573271191638447, 3.3579404695564525, 3.3595999061603985]\n",
      "Starting run 4/5\n",
      "Run 4, Epoch 1, Loss: 4.260, Training Accuracy: 6.19%\n",
      "Run 4, Epoch 2, Loss: 3.985, Training Accuracy: 9.89%\n",
      "Run 4, Epoch 3, Loss: 3.866, Training Accuracy: 11.79%\n",
      "Run 4, Epoch 4, Loss: 3.773, Training Accuracy: 13.21%\n",
      "Run 4, Epoch 5, Loss: 3.695, Training Accuracy: 14.79%\n",
      "Run 4, Epoch 6, Loss: 3.632, Training Accuracy: 15.69%\n",
      "Run 4, Epoch 7, Loss: 3.582, Training Accuracy: 16.69%\n",
      "Run 4, Epoch 8, Loss: 3.524, Training Accuracy: 17.50%\n",
      "Run 4, Epoch 9, Loss: 3.483, Training Accuracy: 18.40%\n",
      "Run 4, Epoch 10, Loss: 3.442, Training Accuracy: 18.93%\n",
      "Run 4, Epoch 11, Loss: 3.391, Training Accuracy: 20.09%\n",
      "Run 4, Epoch 12, Loss: 3.386, Training Accuracy: 19.98%\n",
      "Run 4, Epoch 13, Loss: 3.377, Training Accuracy: 20.08%\n",
      "Run 4, Epoch 14, Loss: 3.377, Training Accuracy: 20.36%\n",
      "Run 4, Epoch 15, Loss: 3.373, Training Accuracy: 20.30%\n",
      "Run 4, Epoch 16, Loss: 3.360, Training Accuracy: 20.46%\n",
      "Run 4, Epoch 17, Loss: 3.357, Training Accuracy: 20.50%\n",
      "Run 4, Epoch 18, Loss: 3.357, Training Accuracy: 20.73%\n",
      "Run 4, Epoch 19, Loss: 3.344, Training Accuracy: 20.75%\n",
      "Run 4, Epoch 20, Loss: 3.345, Training Accuracy: 20.68%\n",
      "Run 4, Epoch 21, Loss: 3.342, Training Accuracy: 20.77%\n",
      "Run 4, Epoch 22, Loss: 3.334, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 23, Loss: 3.334, Training Accuracy: 21.16%\n",
      "Run 4, Epoch 24, Loss: 3.328, Training Accuracy: 21.14%\n",
      "Run 4, Epoch 25, Loss: 3.338, Training Accuracy: 21.14%\n",
      "Run 4, Epoch 26, Loss: 3.333, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 27, Loss: 3.338, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 28, Loss: 3.335, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 29, Loss: 3.338, Training Accuracy: 20.97%\n",
      "Run 4, Epoch 30, Loss: 3.331, Training Accuracy: 21.17%\n",
      "Run 4, Epoch 31, Loss: 3.332, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 32, Loss: 3.330, Training Accuracy: 21.21%\n",
      "Run 4, Epoch 33, Loss: 3.334, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 34, Loss: 3.336, Training Accuracy: 21.05%\n",
      "Run 4, Epoch 35, Loss: 3.333, Training Accuracy: 21.01%\n",
      "Run 4, Epoch 36, Loss: 3.333, Training Accuracy: 21.03%\n",
      "Run 4, Epoch 37, Loss: 3.333, Training Accuracy: 21.04%\n",
      "Run 4, Epoch 38, Loss: 3.329, Training Accuracy: 21.27%\n",
      "Run 4, Epoch 39, Loss: 3.331, Training Accuracy: 21.11%\n",
      "Run 4, Epoch 40, Loss: 3.331, Training Accuracy: 21.10%\n",
      "Run 4, Epoch 41, Loss: 3.332, Training Accuracy: 21.09%\n",
      "Run 4, Epoch 42, Loss: 3.336, Training Accuracy: 21.17%\n",
      "Run 4, Epoch 43, Loss: 3.332, Training Accuracy: 21.15%\n",
      "Run 4, Epoch 44, Loss: 3.331, Training Accuracy: 21.11%\n",
      "Run 4, Epoch 45, Loss: 3.332, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 46, Loss: 3.329, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 47, Loss: 3.332, Training Accuracy: 21.21%\n",
      "Run 4, Epoch 48, Loss: 3.331, Training Accuracy: 21.15%\n",
      "Run 4, Epoch 49, Loss: 3.337, Training Accuracy: 21.01%\n",
      "Run 4, Epoch 50, Loss: 3.332, Training Accuracy: 21.08%\n",
      "Run 4, Epoch 51, Loss: 3.334, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 52, Loss: 3.337, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 53, Loss: 3.333, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 54, Loss: 3.330, Training Accuracy: 21.28%\n",
      "Run 4, Epoch 55, Loss: 3.331, Training Accuracy: 21.21%\n",
      "Run 4, Epoch 56, Loss: 3.332, Training Accuracy: 21.11%\n",
      "Run 4, Epoch 57, Loss: 3.332, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 58, Loss: 3.331, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 59, Loss: 3.332, Training Accuracy: 20.98%\n",
      "Run 4, Epoch 60, Loss: 3.328, Training Accuracy: 21.14%\n",
      "Run 4, Epoch 61, Loss: 3.334, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 62, Loss: 3.331, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 63, Loss: 3.333, Training Accuracy: 20.99%\n",
      "Run 4, Epoch 64, Loss: 3.336, Training Accuracy: 20.91%\n",
      "Run 4, Epoch 65, Loss: 3.330, Training Accuracy: 21.38%\n",
      "Run 4, Epoch 66, Loss: 3.332, Training Accuracy: 21.04%\n",
      "Run 4, Epoch 67, Loss: 3.334, Training Accuracy: 20.87%\n",
      "Run 4, Epoch 68, Loss: 3.331, Training Accuracy: 21.17%\n",
      "Run 4, Epoch 69, Loss: 3.335, Training Accuracy: 21.07%\n",
      "Run 4, Epoch 70, Loss: 3.336, Training Accuracy: 20.83%\n",
      "Run 4, Epoch 71, Loss: 3.335, Training Accuracy: 21.13%\n",
      "Run 4, Epoch 72, Loss: 3.331, Training Accuracy: 21.03%\n",
      "Run 4, Epoch 73, Loss: 3.333, Training Accuracy: 21.02%\n",
      "Run 4, Epoch 74, Loss: 3.334, Training Accuracy: 20.97%\n",
      "Run 4, Epoch 75, Loss: 3.331, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 76, Loss: 3.337, Training Accuracy: 21.01%\n",
      "Run 4, Epoch 77, Loss: 3.329, Training Accuracy: 20.94%\n",
      "Run 4, Epoch 78, Loss: 3.333, Training Accuracy: 21.09%\n",
      "Run 4, Epoch 79, Loss: 3.328, Training Accuracy: 21.29%\n",
      "Run 4, Epoch 80, Loss: 3.335, Training Accuracy: 20.89%\n",
      "Run 4, Epoch 81, Loss: 3.330, Training Accuracy: 21.28%\n",
      "Run 4, Epoch 82, Loss: 3.331, Training Accuracy: 21.01%\n",
      "Run 4, Epoch 83, Loss: 3.333, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 84, Loss: 3.332, Training Accuracy: 20.98%\n",
      "Run 4, Epoch 85, Loss: 3.333, Training Accuracy: 21.01%\n",
      "Run 4, Epoch 86, Loss: 3.331, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 87, Loss: 3.325, Training Accuracy: 21.21%\n",
      "Run 4, Epoch 88, Loss: 3.332, Training Accuracy: 21.10%\n",
      "Run 4, Epoch 89, Loss: 3.332, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 90, Loss: 3.329, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 91, Loss: 3.328, Training Accuracy: 21.22%\n",
      "Run 4, Epoch 92, Loss: 3.335, Training Accuracy: 21.02%\n",
      "Run 4, Epoch 93, Loss: 3.330, Training Accuracy: 21.11%\n",
      "Run 4, Epoch 94, Loss: 3.331, Training Accuracy: 21.14%\n",
      "Run 4, Epoch 95, Loss: 3.334, Training Accuracy: 21.10%\n",
      "Run 4, Epoch 96, Loss: 3.331, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 97, Loss: 3.329, Training Accuracy: 21.05%\n",
      "Run 4, Epoch 98, Loss: 3.325, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 99, Loss: 3.335, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 100, Loss: 3.329, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 101, Loss: 3.332, Training Accuracy: 21.08%\n",
      "Run 4, Epoch 102, Loss: 3.333, Training Accuracy: 21.03%\n",
      "Run 4, Epoch 103, Loss: 3.328, Training Accuracy: 21.27%\n",
      "Run 4, Epoch 104, Loss: 3.334, Training Accuracy: 21.05%\n",
      "Run 4, Epoch 105, Loss: 3.330, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 106, Loss: 3.332, Training Accuracy: 21.23%\n",
      "Run 4, Epoch 107, Loss: 3.332, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 108, Loss: 3.330, Training Accuracy: 21.10%\n",
      "Run 4, Epoch 109, Loss: 3.332, Training Accuracy: 21.11%\n",
      "Run 4, Epoch 110, Loss: 3.335, Training Accuracy: 20.85%\n",
      "Run 4, Epoch 111, Loss: 3.331, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 112, Loss: 3.333, Training Accuracy: 21.05%\n",
      "Run 4, Epoch 113, Loss: 3.335, Training Accuracy: 21.02%\n",
      "Run 4, Epoch 114, Loss: 3.332, Training Accuracy: 21.09%\n",
      "Run 4, Epoch 115, Loss: 3.335, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 116, Loss: 3.333, Training Accuracy: 21.11%\n",
      "Run 4, Epoch 117, Loss: 3.331, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 118, Loss: 3.335, Training Accuracy: 20.81%\n",
      "Run 4, Epoch 119, Loss: 3.337, Training Accuracy: 21.30%\n",
      "Run 4, Epoch 120, Loss: 3.331, Training Accuracy: 21.28%\n",
      "Run 4, Epoch 121, Loss: 3.331, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 122, Loss: 3.330, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 123, Loss: 3.329, Training Accuracy: 21.21%\n",
      "Run 4, Epoch 124, Loss: 3.338, Training Accuracy: 20.98%\n",
      "Run 4, Epoch 125, Loss: 3.332, Training Accuracy: 21.10%\n",
      "Run 4, Epoch 126, Loss: 3.328, Training Accuracy: 21.16%\n",
      "Run 4, Epoch 127, Loss: 3.330, Training Accuracy: 21.17%\n",
      "Run 4, Epoch 128, Loss: 3.331, Training Accuracy: 21.15%\n",
      "Run 4, Epoch 129, Loss: 3.331, Training Accuracy: 21.02%\n",
      "Run 4, Epoch 130, Loss: 3.331, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 131, Loss: 3.331, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 132, Loss: 3.332, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 133, Loss: 3.333, Training Accuracy: 21.30%\n",
      "Run 4, Epoch 134, Loss: 3.335, Training Accuracy: 21.10%\n",
      "Run 4, Epoch 135, Loss: 3.329, Training Accuracy: 21.16%\n",
      "Run 4, Epoch 136, Loss: 3.329, Training Accuracy: 21.10%\n",
      "Run 4, Epoch 137, Loss: 3.332, Training Accuracy: 21.19%\n",
      "Run 4, Epoch 138, Loss: 3.333, Training Accuracy: 21.28%\n",
      "Run 4, Epoch 139, Loss: 3.332, Training Accuracy: 20.98%\n",
      "Run 4, Epoch 140, Loss: 3.333, Training Accuracy: 21.09%\n",
      "Run 4, Epoch 141, Loss: 3.332, Training Accuracy: 21.02%\n",
      "Run 4, Epoch 142, Loss: 3.331, Training Accuracy: 21.04%\n",
      "Run 4, Epoch 143, Loss: 3.334, Training Accuracy: 21.02%\n",
      "Run 4, Epoch 144, Loss: 3.333, Training Accuracy: 21.31%\n",
      "Run 4, Epoch 145, Loss: 3.333, Training Accuracy: 20.94%\n",
      "Run 4, Epoch 146, Loss: 3.336, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 147, Loss: 3.333, Training Accuracy: 21.08%\n",
      "Run 4, Epoch 148, Loss: 3.332, Training Accuracy: 21.05%\n",
      "Run 4, Epoch 149, Loss: 3.331, Training Accuracy: 21.08%\n",
      "Run 4, Epoch 150, Loss: 3.332, Training Accuracy: 21.29%\n",
      "Run 4, Epoch 151, Loss: 3.331, Training Accuracy: 21.11%\n",
      "Run 4, Epoch 152, Loss: 3.326, Training Accuracy: 21.26%\n",
      "Run 4, Epoch 153, Loss: 3.333, Training Accuracy: 21.13%\n",
      "Run 4, Epoch 154, Loss: 3.335, Training Accuracy: 21.09%\n",
      "Run 4, Epoch 155, Loss: 3.329, Training Accuracy: 21.25%\n",
      "Run 4, Epoch 156, Loss: 3.334, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 157, Loss: 3.331, Training Accuracy: 21.23%\n",
      "Run 4, Epoch 158, Loss: 3.334, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 159, Loss: 3.332, Training Accuracy: 21.14%\n",
      "Run 4, Epoch 160, Loss: 3.334, Training Accuracy: 21.22%\n",
      "Run 4, Epoch 161, Loss: 3.332, Training Accuracy: 20.98%\n",
      "Run 4, Epoch 162, Loss: 3.328, Training Accuracy: 21.20%\n",
      "Run 4, Epoch 163, Loss: 3.332, Training Accuracy: 21.10%\n",
      "Run 4, Epoch 164, Loss: 3.334, Training Accuracy: 21.02%\n",
      "Run 4, Epoch 165, Loss: 3.331, Training Accuracy: 21.06%\n",
      "Run 4, Epoch 166, Loss: 3.330, Training Accuracy: 21.13%\n",
      "Run 4, Epoch 167, Loss: 3.333, Training Accuracy: 21.12%\n",
      "Run 4, Epoch 168, Loss: 3.331, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 169, Loss: 3.334, Training Accuracy: 21.00%\n",
      "Run 4, Epoch 170, Loss: 3.333, Training Accuracy: 20.93%\n",
      "Run 4, Epoch 171, Loss: 3.335, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 172, Loss: 3.330, Training Accuracy: 21.18%\n",
      "Run 4, Epoch 173, Loss: 3.326, Training Accuracy: 21.30%\n",
      "Run 4, Epoch 174, Loss: 3.334, Training Accuracy: 20.99%\n",
      "Run 4, Epoch 175, Loss: 3.332, Training Accuracy: 21.19%\n",
      "Run 4, Final Accuracy on test set: 22.33%\n",
      "Results after run 4:\n",
      "Training Accuracies: [6.19, 9.892, 11.792, 13.208, 14.788, 15.692, 16.686, 17.5, 18.396, 18.926, 20.092, 19.98, 20.08, 20.364, 20.296, 20.46, 20.5, 20.726, 20.752, 20.682, 20.774, 21.002, 21.16, 21.144, 21.142, 21.004, 21.064, 20.996, 20.972, 21.168, 21.058, 21.212, 21.178, 21.048, 21.012, 21.032, 21.044, 21.274, 21.114, 21.1, 21.09, 21.166, 21.148, 21.112, 21.064, 21.122, 21.206, 21.15, 21.008, 21.08, 21.178, 21.116, 21.12, 21.282, 21.212, 21.11, 20.998, 21.0, 20.98, 21.136, 21.184, 21.116, 20.988, 20.906, 21.382, 21.042, 20.872, 21.166, 21.07, 20.832, 21.132, 21.034, 21.024, 20.974, 21.064, 21.006, 20.944, 21.094, 21.292, 20.894, 21.278, 21.008, 21.122, 20.978, 21.01, 21.116, 21.214, 21.102, 21.124, 21.0, 21.224, 21.024, 21.108, 21.14, 21.104, 21.064, 21.046, 21.184, 21.064, 21.176, 21.08, 21.028, 21.268, 21.052, 21.116, 21.226, 21.06, 21.102, 21.112, 20.848, 21.062, 21.046, 21.022, 21.09, 20.996, 21.114, 21.062, 20.81, 21.298, 21.278, 21.182, 21.118, 21.21, 20.982, 21.102, 21.16, 21.172, 21.146, 21.022, 21.18, 21.124, 21.002, 21.302, 21.104, 21.164, 21.098, 21.188, 21.278, 20.978, 21.094, 21.018, 21.038, 21.022, 21.312, 20.944, 20.996, 21.078, 21.054, 21.084, 21.288, 21.106, 21.256, 21.132, 21.086, 21.252, 21.058, 21.228, 21.118, 21.142, 21.222, 20.982, 21.2, 21.098, 21.018, 21.056, 21.13, 21.124, 21.004, 21.004, 20.926, 21.178, 21.178, 21.3, 20.988, 21.194]\n",
      "Test Accuracy: 22.33%\n",
      "Losses: [4.2602290094965864, 3.984583441253818, 3.8656749359482085, 3.772688563217592, 3.6954742831647245, 3.632154677530079, 3.582249143239482, 3.5243765009028833, 3.483181454031669, 3.4422867535935033, 3.3911754225221133, 3.3860505450412135, 3.376966683760933, 3.3768996161877958, 3.3732410385785507, 3.3599852192432373, 3.356902876778332, 3.3566318448547205, 3.3442259517776995, 3.344917291875386, 3.341752159016212, 3.3340826168694457, 3.333849795943941, 3.3283974417030353, 3.338389554596923, 3.3330616292441286, 3.33762283276414, 3.3345051488608046, 3.3375410753137924, 3.330945854601653, 3.3323339684235167, 3.32993819524565, 3.3339488945348794, 3.3355983523151758, 3.3329439224184627, 3.3325428121230183, 3.3333874267080557, 3.3291674885908358, 3.3314642357399396, 3.3307061963678932, 3.3324181466456264, 3.3357820334032064, 3.3319164749301606, 3.3311375004556174, 3.332127935136371, 3.328788839032888, 3.331797310153542, 3.3312718441419285, 3.3369300383740983, 3.3320790312784103, 3.3343124273792863, 3.337485846960941, 3.332998764789318, 3.3302673342282816, 3.330598512878808, 3.3318267441771523, 3.3316364757850043, 3.330936390420665, 3.3319262513114363, 3.3281325153682544, 3.3339872201690284, 3.330777228031012, 3.332890027624262, 3.336404859562359, 3.32978750799623, 3.3320859783445784, 3.33351076289516, 3.3307964759104696, 3.334678986493279, 3.3358232657927687, 3.3348673719274418, 3.3305018661577073, 3.3331861514264665, 3.334232840086798, 3.3308376637871002, 3.337100663148534, 3.3294749522148193, 3.333076793519432, 3.327610571366137, 3.335373601645155, 3.3300151605435344, 3.33075467216999, 3.3326662294090252, 3.3322345612908872, 3.3326566646166165, 3.330642053233388, 3.325112784914958, 3.33207269702726, 3.331907138190306, 3.328964182178078, 3.327695888021718, 3.334998510072908, 3.330283953405707, 3.3307989972936527, 3.3340360337815933, 3.330934423924712, 3.329364002818037, 3.324805308485885, 3.33463710592226, 3.329475461369585, 3.3321896026201565, 3.332777676984782, 3.3283773614927328, 3.3336519060842216, 3.329734216260788, 3.3317868166872304, 3.3322368552312827, 3.3304529129086857, 3.3318215916528726, 3.334770708742654, 3.330695351676258, 3.3331649230264335, 3.3349646631714025, 3.3323144845645447, 3.3350800126409896, 3.3333774100789024, 3.331391714418026, 3.335220181423685, 3.3368909237025033, 3.3308392302764345, 3.331049955104623, 3.330458236167498, 3.328983015416528, 3.3375981506484242, 3.332048833217767, 3.3277210951461207, 3.3297002669185627, 3.3308455340392755, 3.330849237759095, 3.3307712828106895, 3.331119708088048, 3.332064435914959, 3.3331891972085703, 3.3350102303887876, 3.328674032255207, 3.329203423941532, 3.3323259640227803, 3.3329384717185175, 3.3324710158131006, 3.3325037480620168, 3.3319186048434517, 3.3310453519796774, 3.333829210847235, 3.3329370137675642, 3.332944328522743, 3.336215872898736, 3.333484808807178, 3.331634916003098, 3.3312952640416373, 3.331916902376258, 3.33136971893213, 3.3263444504164674, 3.332581805451142, 3.335088115823848, 3.3285376732916476, 3.3335254527723697, 3.331402923140075, 3.3337690342417763, 3.33180188400971, 3.333686638366231, 3.3318820749707236, 3.3277691520388473, 3.3322761516132013, 3.334114396663578, 3.3310606388179846, 3.32957418800315, 3.332852103216264, 3.331054270419928, 3.334328098980057, 3.3326481626466715, 3.3354280202285103, 3.329549201614107, 3.326157782083887, 3.3338046311722387, 3.3316957639611284]\n",
      "Starting run 5/5\n",
      "Run 5, Epoch 1, Loss: 4.292, Training Accuracy: 5.68%\n",
      "Run 5, Epoch 2, Loss: 4.018, Training Accuracy: 9.25%\n",
      "Run 5, Epoch 3, Loss: 3.918, Training Accuracy: 10.72%\n",
      "Run 5, Epoch 4, Loss: 3.837, Training Accuracy: 12.05%\n",
      "Run 5, Epoch 5, Loss: 3.769, Training Accuracy: 13.18%\n",
      "Run 5, Epoch 6, Loss: 3.713, Training Accuracy: 14.08%\n",
      "Run 5, Epoch 7, Loss: 3.659, Training Accuracy: 15.24%\n",
      "Run 5, Epoch 8, Loss: 3.608, Training Accuracy: 16.39%\n",
      "Run 5, Epoch 9, Loss: 3.561, Training Accuracy: 17.12%\n",
      "Run 5, Epoch 10, Loss: 3.531, Training Accuracy: 17.34%\n",
      "Run 5, Epoch 11, Loss: 3.474, Training Accuracy: 18.60%\n",
      "Run 5, Epoch 12, Loss: 3.464, Training Accuracy: 18.91%\n",
      "Run 5, Epoch 13, Loss: 3.462, Training Accuracy: 18.93%\n",
      "Run 5, Epoch 14, Loss: 3.453, Training Accuracy: 19.11%\n",
      "Run 5, Epoch 15, Loss: 3.454, Training Accuracy: 19.09%\n",
      "Run 5, Epoch 16, Loss: 3.446, Training Accuracy: 18.96%\n",
      "Run 5, Epoch 17, Loss: 3.443, Training Accuracy: 19.34%\n",
      "Run 5, Epoch 18, Loss: 3.440, Training Accuracy: 19.24%\n",
      "Run 5, Epoch 19, Loss: 3.436, Training Accuracy: 19.19%\n",
      "Run 5, Epoch 20, Loss: 3.432, Training Accuracy: 19.36%\n",
      "Run 5, Epoch 21, Loss: 3.431, Training Accuracy: 19.45%\n",
      "Run 5, Epoch 22, Loss: 3.424, Training Accuracy: 19.69%\n",
      "Run 5, Epoch 23, Loss: 3.424, Training Accuracy: 19.70%\n",
      "Run 5, Epoch 24, Loss: 3.427, Training Accuracy: 19.62%\n",
      "Run 5, Epoch 25, Loss: 3.423, Training Accuracy: 19.57%\n",
      "Run 5, Epoch 26, Loss: 3.426, Training Accuracy: 19.64%\n",
      "Run 5, Epoch 27, Loss: 3.424, Training Accuracy: 19.64%\n",
      "Run 5, Epoch 28, Loss: 3.423, Training Accuracy: 19.49%\n",
      "Run 5, Epoch 29, Loss: 3.422, Training Accuracy: 19.65%\n",
      "Run 5, Epoch 30, Loss: 3.424, Training Accuracy: 19.65%\n",
      "Run 5, Epoch 31, Loss: 3.423, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 32, Loss: 3.423, Training Accuracy: 19.54%\n",
      "Run 5, Epoch 33, Loss: 3.426, Training Accuracy: 19.32%\n",
      "Run 5, Epoch 34, Loss: 3.424, Training Accuracy: 19.70%\n",
      "Run 5, Epoch 35, Loss: 3.420, Training Accuracy: 19.80%\n",
      "Run 5, Epoch 36, Loss: 3.422, Training Accuracy: 19.72%\n",
      "Run 5, Epoch 37, Loss: 3.424, Training Accuracy: 19.69%\n",
      "Run 5, Epoch 38, Loss: 3.426, Training Accuracy: 19.41%\n",
      "Run 5, Epoch 39, Loss: 3.422, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 40, Loss: 3.422, Training Accuracy: 19.58%\n",
      "Run 5, Epoch 41, Loss: 3.423, Training Accuracy: 19.78%\n",
      "Run 5, Epoch 42, Loss: 3.426, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 43, Loss: 3.427, Training Accuracy: 19.61%\n",
      "Run 5, Epoch 44, Loss: 3.421, Training Accuracy: 19.67%\n",
      "Run 5, Epoch 45, Loss: 3.422, Training Accuracy: 19.82%\n",
      "Run 5, Epoch 46, Loss: 3.430, Training Accuracy: 19.65%\n",
      "Run 5, Epoch 47, Loss: 3.425, Training Accuracy: 19.67%\n",
      "Run 5, Epoch 48, Loss: 3.418, Training Accuracy: 19.67%\n",
      "Run 5, Epoch 49, Loss: 3.421, Training Accuracy: 19.76%\n",
      "Run 5, Epoch 50, Loss: 3.425, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 51, Loss: 3.422, Training Accuracy: 19.64%\n",
      "Run 5, Epoch 52, Loss: 3.424, Training Accuracy: 19.57%\n",
      "Run 5, Epoch 53, Loss: 3.420, Training Accuracy: 19.73%\n",
      "Run 5, Epoch 54, Loss: 3.422, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 55, Loss: 3.425, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 56, Loss: 3.422, Training Accuracy: 19.70%\n",
      "Run 5, Epoch 57, Loss: 3.425, Training Accuracy: 19.38%\n",
      "Run 5, Epoch 58, Loss: 3.425, Training Accuracy: 19.56%\n",
      "Run 5, Epoch 59, Loss: 3.426, Training Accuracy: 19.71%\n",
      "Run 5, Epoch 60, Loss: 3.424, Training Accuracy: 19.55%\n",
      "Run 5, Epoch 61, Loss: 3.425, Training Accuracy: 19.75%\n",
      "Run 5, Epoch 62, Loss: 3.424, Training Accuracy: 19.40%\n",
      "Run 5, Epoch 63, Loss: 3.427, Training Accuracy: 19.57%\n",
      "Run 5, Epoch 64, Loss: 3.422, Training Accuracy: 19.68%\n",
      "Run 5, Epoch 65, Loss: 3.419, Training Accuracy: 19.68%\n",
      "Run 5, Epoch 66, Loss: 3.425, Training Accuracy: 19.46%\n",
      "Run 5, Epoch 67, Loss: 3.426, Training Accuracy: 19.77%\n",
      "Run 5, Epoch 68, Loss: 3.425, Training Accuracy: 19.57%\n",
      "Run 5, Epoch 69, Loss: 3.421, Training Accuracy: 19.80%\n",
      "Run 5, Epoch 70, Loss: 3.425, Training Accuracy: 19.70%\n",
      "Run 5, Epoch 71, Loss: 3.426, Training Accuracy: 19.66%\n",
      "Run 5, Epoch 72, Loss: 3.422, Training Accuracy: 19.50%\n",
      "Run 5, Epoch 73, Loss: 3.419, Training Accuracy: 19.72%\n",
      "Run 5, Epoch 74, Loss: 3.423, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 75, Loss: 3.421, Training Accuracy: 19.67%\n",
      "Run 5, Epoch 76, Loss: 3.423, Training Accuracy: 19.59%\n",
      "Run 5, Epoch 77, Loss: 3.423, Training Accuracy: 19.75%\n",
      "Run 5, Epoch 78, Loss: 3.428, Training Accuracy: 19.74%\n",
      "Run 5, Epoch 79, Loss: 3.425, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 80, Loss: 3.421, Training Accuracy: 19.70%\n",
      "Run 5, Epoch 81, Loss: 3.428, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 82, Loss: 3.421, Training Accuracy: 19.92%\n",
      "Run 5, Epoch 83, Loss: 3.425, Training Accuracy: 19.61%\n",
      "Run 5, Epoch 84, Loss: 3.419, Training Accuracy: 19.42%\n",
      "Run 5, Epoch 85, Loss: 3.425, Training Accuracy: 19.65%\n",
      "Run 5, Epoch 86, Loss: 3.421, Training Accuracy: 19.88%\n",
      "Run 5, Epoch 87, Loss: 3.425, Training Accuracy: 19.68%\n",
      "Run 5, Epoch 88, Loss: 3.422, Training Accuracy: 19.75%\n",
      "Run 5, Epoch 89, Loss: 3.421, Training Accuracy: 19.61%\n",
      "Run 5, Epoch 90, Loss: 3.420, Training Accuracy: 19.49%\n",
      "Run 5, Epoch 91, Loss: 3.426, Training Accuracy: 19.55%\n",
      "Run 5, Epoch 92, Loss: 3.422, Training Accuracy: 19.81%\n",
      "Run 5, Epoch 93, Loss: 3.421, Training Accuracy: 19.78%\n",
      "Run 5, Epoch 94, Loss: 3.424, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 95, Loss: 3.423, Training Accuracy: 19.55%\n",
      "Run 5, Epoch 96, Loss: 3.424, Training Accuracy: 19.55%\n",
      "Run 5, Epoch 97, Loss: 3.424, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 98, Loss: 3.422, Training Accuracy: 19.71%\n",
      "Run 5, Epoch 99, Loss: 3.426, Training Accuracy: 19.58%\n",
      "Run 5, Epoch 100, Loss: 3.424, Training Accuracy: 19.48%\n",
      "Run 5, Epoch 101, Loss: 3.421, Training Accuracy: 19.79%\n",
      "Run 5, Epoch 102, Loss: 3.425, Training Accuracy: 19.55%\n",
      "Run 5, Epoch 103, Loss: 3.420, Training Accuracy: 19.75%\n",
      "Run 5, Epoch 104, Loss: 3.423, Training Accuracy: 19.58%\n",
      "Run 5, Epoch 105, Loss: 3.423, Training Accuracy: 19.65%\n",
      "Run 5, Epoch 106, Loss: 3.425, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 107, Loss: 3.424, Training Accuracy: 19.75%\n",
      "Run 5, Epoch 108, Loss: 3.424, Training Accuracy: 19.74%\n",
      "Run 5, Epoch 109, Loss: 3.423, Training Accuracy: 19.68%\n",
      "Run 5, Epoch 110, Loss: 3.423, Training Accuracy: 19.69%\n",
      "Run 5, Epoch 111, Loss: 3.422, Training Accuracy: 19.77%\n",
      "Run 5, Epoch 112, Loss: 3.423, Training Accuracy: 19.53%\n",
      "Run 5, Epoch 113, Loss: 3.423, Training Accuracy: 19.57%\n",
      "Run 5, Epoch 114, Loss: 3.426, Training Accuracy: 19.73%\n",
      "Run 5, Epoch 115, Loss: 3.429, Training Accuracy: 19.72%\n",
      "Run 5, Epoch 116, Loss: 3.426, Training Accuracy: 19.58%\n",
      "Run 5, Epoch 117, Loss: 3.422, Training Accuracy: 19.59%\n",
      "Run 5, Epoch 118, Loss: 3.424, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 119, Loss: 3.426, Training Accuracy: 19.50%\n",
      "Run 5, Epoch 120, Loss: 3.422, Training Accuracy: 19.66%\n",
      "Run 5, Epoch 121, Loss: 3.425, Training Accuracy: 19.77%\n",
      "Run 5, Epoch 122, Loss: 3.420, Training Accuracy: 19.68%\n",
      "Run 5, Epoch 123, Loss: 3.422, Training Accuracy: 19.50%\n",
      "Run 5, Epoch 124, Loss: 3.425, Training Accuracy: 19.73%\n",
      "Run 5, Epoch 125, Loss: 3.421, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 126, Loss: 3.420, Training Accuracy: 19.70%\n",
      "Run 5, Epoch 127, Loss: 3.422, Training Accuracy: 19.65%\n",
      "Run 5, Epoch 128, Loss: 3.423, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 129, Loss: 3.422, Training Accuracy: 19.74%\n",
      "Run 5, Epoch 130, Loss: 3.425, Training Accuracy: 19.64%\n",
      "Run 5, Epoch 131, Loss: 3.422, Training Accuracy: 19.47%\n",
      "Run 5, Epoch 132, Loss: 3.422, Training Accuracy: 19.68%\n",
      "Run 5, Epoch 133, Loss: 3.424, Training Accuracy: 19.70%\n",
      "Run 5, Epoch 134, Loss: 3.423, Training Accuracy: 19.79%\n",
      "Run 5, Epoch 135, Loss: 3.423, Training Accuracy: 19.83%\n",
      "Run 5, Epoch 136, Loss: 3.419, Training Accuracy: 19.75%\n",
      "Run 5, Epoch 137, Loss: 3.420, Training Accuracy: 19.49%\n",
      "Run 5, Epoch 138, Loss: 3.419, Training Accuracy: 19.57%\n",
      "Run 5, Epoch 139, Loss: 3.420, Training Accuracy: 19.83%\n",
      "Run 5, Epoch 140, Loss: 3.425, Training Accuracy: 19.48%\n",
      "Run 5, Epoch 141, Loss: 3.424, Training Accuracy: 19.42%\n",
      "Run 5, Epoch 142, Loss: 3.423, Training Accuracy: 19.55%\n",
      "Run 5, Epoch 143, Loss: 3.427, Training Accuracy: 19.48%\n",
      "Run 5, Epoch 144, Loss: 3.418, Training Accuracy: 19.64%\n",
      "Run 5, Epoch 145, Loss: 3.426, Training Accuracy: 19.48%\n",
      "Run 5, Epoch 146, Loss: 3.423, Training Accuracy: 19.65%\n",
      "Run 5, Epoch 147, Loss: 3.425, Training Accuracy: 19.91%\n",
      "Run 5, Epoch 148, Loss: 3.423, Training Accuracy: 19.74%\n",
      "Run 5, Epoch 149, Loss: 3.423, Training Accuracy: 19.48%\n",
      "Run 5, Epoch 150, Loss: 3.421, Training Accuracy: 19.80%\n",
      "Run 5, Epoch 151, Loss: 3.424, Training Accuracy: 19.77%\n",
      "Run 5, Epoch 152, Loss: 3.425, Training Accuracy: 19.69%\n",
      "Run 5, Epoch 153, Loss: 3.422, Training Accuracy: 19.67%\n",
      "Run 5, Epoch 154, Loss: 3.422, Training Accuracy: 19.65%\n",
      "Run 5, Epoch 155, Loss: 3.420, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 156, Loss: 3.427, Training Accuracy: 19.41%\n",
      "Run 5, Epoch 157, Loss: 3.420, Training Accuracy: 19.60%\n",
      "Run 5, Epoch 158, Loss: 3.422, Training Accuracy: 19.50%\n",
      "Run 5, Epoch 159, Loss: 3.425, Training Accuracy: 19.70%\n",
      "Run 5, Epoch 160, Loss: 3.422, Training Accuracy: 19.72%\n",
      "Run 5, Epoch 161, Loss: 3.425, Training Accuracy: 19.52%\n",
      "Run 5, Epoch 162, Loss: 3.426, Training Accuracy: 19.72%\n",
      "Run 5, Epoch 163, Loss: 3.419, Training Accuracy: 19.68%\n",
      "Run 5, Epoch 164, Loss: 3.422, Training Accuracy: 19.53%\n",
      "Run 5, Epoch 165, Loss: 3.424, Training Accuracy: 19.61%\n",
      "Run 5, Epoch 166, Loss: 3.426, Training Accuracy: 19.76%\n",
      "Run 5, Epoch 167, Loss: 3.425, Training Accuracy: 19.44%\n",
      "Run 5, Epoch 168, Loss: 3.421, Training Accuracy: 19.79%\n",
      "Run 5, Epoch 169, Loss: 3.421, Training Accuracy: 19.77%\n",
      "Run 5, Epoch 170, Loss: 3.426, Training Accuracy: 19.54%\n",
      "Run 5, Epoch 171, Loss: 3.424, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 172, Loss: 3.427, Training Accuracy: 19.44%\n",
      "Run 5, Epoch 173, Loss: 3.422, Training Accuracy: 19.63%\n",
      "Run 5, Epoch 174, Loss: 3.420, Training Accuracy: 19.61%\n",
      "Run 5, Epoch 175, Loss: 3.422, Training Accuracy: 19.56%\n",
      "Run 5, Final Accuracy on test set: 20.81%\n",
      "Results after run 5:\n",
      "Training Accuracies: [5.676, 9.252, 10.718, 12.046, 13.182, 14.08, 15.242, 16.394, 17.124, 17.336, 18.604, 18.91, 18.926, 19.112, 19.088, 18.958, 19.344, 19.236, 19.19, 19.356, 19.448, 19.694, 19.702, 19.622, 19.568, 19.638, 19.638, 19.49, 19.654, 19.652, 19.63, 19.542, 19.318, 19.698, 19.802, 19.718, 19.686, 19.412, 19.602, 19.576, 19.778, 19.632, 19.61, 19.672, 19.82, 19.65, 19.672, 19.668, 19.762, 19.63, 19.636, 19.572, 19.732, 19.598, 19.602, 19.698, 19.378, 19.558, 19.706, 19.55, 19.754, 19.404, 19.574, 19.676, 19.68, 19.462, 19.766, 19.574, 19.8, 19.698, 19.656, 19.502, 19.716, 19.626, 19.666, 19.59, 19.746, 19.742, 19.626, 19.702, 19.602, 19.92, 19.606, 19.422, 19.65, 19.878, 19.676, 19.746, 19.606, 19.494, 19.548, 19.814, 19.782, 19.596, 19.548, 19.546, 19.604, 19.71, 19.582, 19.484, 19.786, 19.548, 19.748, 19.578, 19.652, 19.632, 19.748, 19.744, 19.682, 19.686, 19.768, 19.526, 19.568, 19.734, 19.72, 19.578, 19.594, 19.634, 19.502, 19.664, 19.768, 19.684, 19.502, 19.732, 19.634, 19.7, 19.654, 19.598, 19.74, 19.638, 19.474, 19.68, 19.704, 19.794, 19.826, 19.75, 19.488, 19.568, 19.828, 19.48, 19.424, 19.554, 19.478, 19.644, 19.48, 19.652, 19.906, 19.736, 19.48, 19.804, 19.772, 19.69, 19.666, 19.646, 19.598, 19.414, 19.604, 19.498, 19.696, 19.72, 19.522, 19.718, 19.676, 19.528, 19.606, 19.758, 19.436, 19.788, 19.774, 19.542, 19.628, 19.442, 19.628, 19.608, 19.556]\n",
      "Test Accuracy: 20.81%\n",
      "Losses: [4.2921237988240275, 4.018188113141853, 3.9178274751014417, 3.837462840482707, 3.7691055752737137, 3.7125228579391907, 3.6591282275021837, 3.607776641845703, 3.561238113876499, 3.5312010437021475, 3.4736808480509103, 3.4640631126930646, 3.462464762465728, 3.453027077648036, 3.453922843079433, 3.4460825255459837, 3.443254403750915, 3.440126207478516, 3.4360914925480133, 3.4316145154216406, 3.4306487616370704, 3.424443180908632, 3.423535026857615, 3.4269220902181954, 3.4233780730410914, 3.425570272118844, 3.4240678996991014, 3.4229871775488108, 3.4215349437635574, 3.423957797877319, 3.423198276163672, 3.423250031898089, 3.425552085232552, 3.4241282208191466, 3.42030537768703, 3.4218042409023663, 3.4239205661637095, 3.426201151460028, 3.4217542010499997, 3.4215721669404404, 3.4225062391032344, 3.425772382780109, 3.426556821369454, 3.420766916421368, 3.4223790394375695, 3.4301630310390308, 3.4250732169431797, 3.41848520366737, 3.4210991121618948, 3.4247460407979045, 3.421902863875679, 3.424268221306374, 3.4202187055212154, 3.421849789826766, 3.425170366721385, 3.4223532402301995, 3.424993504648623, 3.424593927915139, 3.4260270107737587, 3.4235480667075233, 3.4248369349847976, 3.423689010503042, 3.427431309009757, 3.4220394155253535, 3.41853933200202, 3.4253228692447437, 3.42565443814563, 3.424603393315659, 3.4208401584869153, 3.425214546720695, 3.426029144955413, 3.422485583273651, 3.4190315169751493, 3.423170562900241, 3.4211841556422242, 3.4225440043622575, 3.4226037602290473, 3.4277919564405672, 3.425349173338517, 3.421253849478329, 3.4276283787339543, 3.421425889520084, 3.425170164888777, 3.419334389059745, 3.4254589727162705, 3.42069359142762, 3.425116761566123, 3.422001585021348, 3.4214505246838036, 3.419577238199961, 3.426298865271956, 3.422168451197007, 3.4212525140903796, 3.424221129063755, 3.423363339870482, 3.4238395977508076, 3.42380138065504, 3.4222153903883132, 3.426294041411651, 3.4239219948458857, 3.4210266951099992, 3.424982241047618, 3.4201370968538174, 3.422817567425311, 3.422940302992721, 3.4252818197850377, 3.4239771390510034, 3.423561526076568, 3.4226844749792154, 3.4233303265193538, 3.4219952263795506, 3.4232635241945077, 3.4231617962917706, 3.4259684762686415, 3.42925454649474, 3.425502625877595, 3.4216092126753628, 3.4235716390487787, 3.425990916883854, 3.422396614728376, 3.4246243363451163, 3.419650468070184, 3.4224937968241895, 3.4254069749046776, 3.421420175401146, 3.419507123625187, 3.422241226181655, 3.4230314154759087, 3.4220686050327234, 3.4253801268994657, 3.421533047390716, 3.421992133035684, 3.424453470713037, 3.422582125724734, 3.423206538495505, 3.418885115162491, 3.4196318086150965, 3.4194066231817843, 3.4200664273918133, 3.425102114372546, 3.424465165418737, 3.422540066492222, 3.427251918846384, 3.418490614732513, 3.4261532963999093, 3.4229505147470536, 3.425451555520372, 3.4227049619035648, 3.4229739986722123, 3.420632527002593, 3.424284346573188, 3.4254208062310965, 3.421609653536316, 3.422330180702307, 3.4198911141251664, 3.426751333124497, 3.4200796038293473, 3.422115595444389, 3.4253911429353994, 3.4215214093932715, 3.425321099703269, 3.426196728825874, 3.4191731729775743, 3.422114388717105, 3.4239090434120745, 3.4255293921741377, 3.425363289425745, 3.420740946479466, 3.420657121921744, 3.4261040236334055, 3.4235651865029886, 3.4269324470968807, 3.422059680494811, 3.4204197320182, 3.4220845735896273]\n",
      "All Training Accuracies over Epochs for each run: [[ 6.012  9.642 11.594 13.278 14.682 15.69  16.442 17.27  17.748 18.506\n",
      "  19.588 19.764 20.108 20.012 19.964 20.092 19.982 20.26  20.14  20.54\n",
      "  20.456 20.416 20.346 20.518 20.39  20.45  20.5   20.418 20.282 20.404\n",
      "  20.294 20.556 20.53  20.538 20.49  20.412 20.624 20.37  20.426 20.496\n",
      "  20.596 20.502 20.434 20.544 20.454 20.668 20.366 20.514 20.366 20.386\n",
      "  20.458 20.772 20.464 20.578 20.664 20.572 20.4   20.576 20.342 20.55\n",
      "  20.428 20.648 20.598 20.528 20.538 20.558 20.266 20.48  20.276 20.486\n",
      "  20.516 20.35  20.564 20.482 20.418 20.66  20.566 20.55  20.364 20.368\n",
      "  20.412 20.48  20.44  20.276 20.326 20.442 20.438 20.634 20.266 20.534\n",
      "  20.588 20.514 20.42  20.296 20.452 20.666 20.624 20.368 20.47  20.408\n",
      "  20.666 20.478 20.38  20.648 20.622 20.484 20.456 20.552 20.434 20.306\n",
      "  20.716 20.586 20.566 20.364 20.478 20.416 20.348 20.252 20.366 20.83\n",
      "  20.69  20.41  20.382 20.36  20.454 20.384 20.71  20.54  20.534 20.44\n",
      "  20.476 20.298 20.474 20.492 20.416 20.526 20.526 20.532 20.64  20.552\n",
      "  20.654 20.52  20.432 20.704 20.47  20.678 20.552 20.456 20.596 20.416\n",
      "  20.34  20.624 20.556 20.32  20.546 20.516 20.56  20.638 20.45  20.534\n",
      "  20.49  20.35  20.642 20.43  20.444 20.448 20.486 20.322 20.56  20.342\n",
      "  20.688 20.738 20.638 20.62  20.614]\n",
      " [ 6.09   9.726 11.376 12.848 14.076 15.428 16.032 16.782 17.57  18.278\n",
      "  19.41  19.726 19.79  19.928 19.808 20.078 20.028 20.112 20.37  20.202\n",
      "  20.598 20.24  20.3   20.6   20.368 20.62  20.414 20.62  20.374 20.486\n",
      "  20.534 20.35  20.486 20.52  20.482 20.44  20.614 20.502 20.65  20.184\n",
      "  20.608 20.364 20.45  20.436 20.668 20.416 20.56  20.498 20.5   20.48\n",
      "  20.382 20.57  20.478 20.588 20.45  20.448 20.62  20.646 20.564 20.548\n",
      "  20.548 20.582 20.514 20.358 20.45  20.536 20.536 20.588 20.506 20.52\n",
      "  20.548 20.586 20.416 20.44  20.392 20.378 20.326 20.756 20.24  20.516\n",
      "  20.344 20.598 20.532 20.574 20.29  20.39  20.6   20.416 20.602 20.22\n",
      "  20.47  20.382 20.572 20.342 20.494 20.634 20.304 20.63  20.55  20.634\n",
      "  20.404 20.462 20.402 20.464 20.388 20.42  20.712 20.502 20.352 20.634\n",
      "  20.526 20.478 20.398 20.65  20.392 20.384 20.628 20.432 20.38  20.572\n",
      "  20.55  20.608 20.338 20.806 20.584 20.54  20.372 20.652 20.426 20.474\n",
      "  20.542 20.512 20.392 20.584 20.33  20.59  20.668 20.768 20.578 20.164\n",
      "  20.65  20.504 20.426 20.564 20.478 20.29  20.666 20.438 20.41  20.442\n",
      "  20.532 20.49  20.372 20.592 20.466 20.26  20.524 20.448 20.572 20.436\n",
      "  20.49  20.718 20.616 20.534 20.61  20.69  20.606 20.254 20.436 20.586\n",
      "  20.7   20.732 20.538 20.526 20.472]\n",
      " [ 6.214  9.708 11.726 13.188 14.244 15.176 16.232 16.844 17.624 18.374\n",
      "  19.404 19.882 19.864 20.02  20.062 20.382 20.21  20.21  20.28  20.496\n",
      "  20.61  20.776 20.632 20.592 20.298 20.59  20.842 20.478 20.646 20.558\n",
      "  20.73  20.72  20.714 20.578 20.552 20.492 20.776 20.786 20.828 20.818\n",
      "  20.764 20.8   20.64  20.62  21.    20.628 20.602 20.514 20.756 20.698\n",
      "  20.856 20.66  20.67  20.456 20.806 20.808 20.854 20.524 20.936 20.524\n",
      "  20.658 20.742 20.604 20.856 20.616 20.732 20.392 20.54  20.594 20.64\n",
      "  20.72  20.666 20.702 20.626 20.628 20.84  20.792 20.806 20.518 20.734\n",
      "  20.678 20.608 20.722 20.568 20.786 20.666 20.592 20.664 20.712 20.98\n",
      "  20.752 20.782 20.706 20.804 20.762 20.67  20.594 20.472 20.838 20.726\n",
      "  20.566 20.776 20.892 20.596 20.656 20.62  20.798 20.676 20.608 20.696\n",
      "  20.712 20.484 20.712 20.756 20.554 20.592 20.714 20.672 20.614 20.768\n",
      "  20.69  20.784 20.708 20.712 20.518 20.598 20.726 20.858 20.768 20.64\n",
      "  20.478 20.66  20.706 20.552 20.846 20.684 20.762 20.764 20.816 20.628\n",
      "  20.666 20.564 20.836 20.67  20.59  20.712 20.794 20.636 20.732 20.594\n",
      "  20.844 20.724 20.65  20.644 20.688 20.712 20.748 20.578 20.66  20.826\n",
      "  20.65  20.814 20.588 20.908 20.878 20.814 20.876 20.678 20.502 20.67\n",
      "  20.934 20.582 20.85  20.62  20.782]\n",
      " [ 6.19   9.892 11.792 13.208 14.788 15.692 16.686 17.5   18.396 18.926\n",
      "  20.092 19.98  20.08  20.364 20.296 20.46  20.5   20.726 20.752 20.682\n",
      "  20.774 21.002 21.16  21.144 21.142 21.004 21.064 20.996 20.972 21.168\n",
      "  21.058 21.212 21.178 21.048 21.012 21.032 21.044 21.274 21.114 21.1\n",
      "  21.09  21.166 21.148 21.112 21.064 21.122 21.206 21.15  21.008 21.08\n",
      "  21.178 21.116 21.12  21.282 21.212 21.11  20.998 21.    20.98  21.136\n",
      "  21.184 21.116 20.988 20.906 21.382 21.042 20.872 21.166 21.07  20.832\n",
      "  21.132 21.034 21.024 20.974 21.064 21.006 20.944 21.094 21.292 20.894\n",
      "  21.278 21.008 21.122 20.978 21.01  21.116 21.214 21.102 21.124 21.\n",
      "  21.224 21.024 21.108 21.14  21.104 21.064 21.046 21.184 21.064 21.176\n",
      "  21.08  21.028 21.268 21.052 21.116 21.226 21.06  21.102 21.112 20.848\n",
      "  21.062 21.046 21.022 21.09  20.996 21.114 21.062 20.81  21.298 21.278\n",
      "  21.182 21.118 21.21  20.982 21.102 21.16  21.172 21.146 21.022 21.18\n",
      "  21.124 21.002 21.302 21.104 21.164 21.098 21.188 21.278 20.978 21.094\n",
      "  21.018 21.038 21.022 21.312 20.944 20.996 21.078 21.054 21.084 21.288\n",
      "  21.106 21.256 21.132 21.086 21.252 21.058 21.228 21.118 21.142 21.222\n",
      "  20.982 21.2   21.098 21.018 21.056 21.13  21.124 21.004 21.004 20.926\n",
      "  21.178 21.178 21.3   20.988 21.194]\n",
      " [ 5.676  9.252 10.718 12.046 13.182 14.08  15.242 16.394 17.124 17.336\n",
      "  18.604 18.91  18.926 19.112 19.088 18.958 19.344 19.236 19.19  19.356\n",
      "  19.448 19.694 19.702 19.622 19.568 19.638 19.638 19.49  19.654 19.652\n",
      "  19.63  19.542 19.318 19.698 19.802 19.718 19.686 19.412 19.602 19.576\n",
      "  19.778 19.632 19.61  19.672 19.82  19.65  19.672 19.668 19.762 19.63\n",
      "  19.636 19.572 19.732 19.598 19.602 19.698 19.378 19.558 19.706 19.55\n",
      "  19.754 19.404 19.574 19.676 19.68  19.462 19.766 19.574 19.8   19.698\n",
      "  19.656 19.502 19.716 19.626 19.666 19.59  19.746 19.742 19.626 19.702\n",
      "  19.602 19.92  19.606 19.422 19.65  19.878 19.676 19.746 19.606 19.494\n",
      "  19.548 19.814 19.782 19.596 19.548 19.546 19.604 19.71  19.582 19.484\n",
      "  19.786 19.548 19.748 19.578 19.652 19.632 19.748 19.744 19.682 19.686\n",
      "  19.768 19.526 19.568 19.734 19.72  19.578 19.594 19.634 19.502 19.664\n",
      "  19.768 19.684 19.502 19.732 19.634 19.7   19.654 19.598 19.74  19.638\n",
      "  19.474 19.68  19.704 19.794 19.826 19.75  19.488 19.568 19.828 19.48\n",
      "  19.424 19.554 19.478 19.644 19.48  19.652 19.906 19.736 19.48  19.804\n",
      "  19.772 19.69  19.666 19.646 19.598 19.414 19.604 19.498 19.696 19.72\n",
      "  19.522 19.718 19.676 19.528 19.606 19.758 19.436 19.788 19.774 19.542\n",
      "  19.628 19.442 19.628 19.608 19.556]]\n",
      "All Test Accuracies after each run: [[22.22]\n",
      " [22.38]\n",
      " [21.77]\n",
      " [22.33]\n",
      " [20.81]]\n",
      "All Losses over Epochs for each run: [[4.22398069 3.98260205 3.86197704 3.75923332 3.68514371 3.62170302\n",
      "  3.57046781 3.53139592 3.49022622 3.45765076 3.40869979 3.40040006\n",
      "  3.38893792 3.38740371 3.38417012 3.38522251 3.38036689 3.37808322\n",
      "  3.37883893 3.36972017 3.36473493 3.36263774 3.36386228 3.36307431\n",
      "  3.36548332 3.36093857 3.36163132 3.36600967 3.36204801 3.3586412\n",
      "  3.36193714 3.35955614 3.35297465 3.3589239  3.35951469 3.3655502\n",
      "  3.35908406 3.3583799  3.35856993 3.36059567 3.35536055 3.35709758\n",
      "  3.3597506  3.36220268 3.36019552 3.3572325  3.35979711 3.36058719\n",
      "  3.35882743 3.3607435  3.35973917 3.35725667 3.35428222 3.3591747\n",
      "  3.35813362 3.35653477 3.36029906 3.35747065 3.36021965 3.35760333\n",
      "  3.36222904 3.35708877 3.35688377 3.35453354 3.36098444 3.35732987\n",
      "  3.35877453 3.3561244  3.36257478 3.36025983 3.35870157 3.36483905\n",
      "  3.35317313 3.35823356 3.35971432 3.36105553 3.36233973 3.35989294\n",
      "  3.36417608 3.36267056 3.36052202 3.35767253 3.36271104 3.35865104\n",
      "  3.36249874 3.35658483 3.35696865 3.35730797 3.36111451 3.35964232\n",
      "  3.35364936 3.3592445  3.35810882 3.35867523 3.36269984 3.35968509\n",
      "  3.35597041 3.35674769 3.36101589 3.35808308 3.35873677 3.36301697\n",
      "  3.3612667  3.35734457 3.35387733 3.36057909 3.36013013 3.35737081\n",
      "  3.35907237 3.36222685 3.35424774 3.36115459 3.35944087 3.36172611\n",
      "  3.3625171  3.36129667 3.35890283 3.35927529 3.35919652 3.36027649\n",
      "  3.35973386 3.36432474 3.35995145 3.36129488 3.36069546 3.35661904\n",
      "  3.35706603 3.35543474 3.35565525 3.35908134 3.35870152 3.35661865\n",
      "  3.36005458 3.35710618 3.3596606  3.36244506 3.36182126 3.35497678\n",
      "  3.3588448  3.35665509 3.35804972 3.35543669 3.36028348 3.36018181\n",
      "  3.35887041 3.35489557 3.36133894 3.36098535 3.36073801 3.35904736\n",
      "  3.36046336 3.3549315  3.36160089 3.36003325 3.35719171 3.36014191\n",
      "  3.35830933 3.35716784 3.35625758 3.36128359 3.35960769 3.35968729\n",
      "  3.35386969 3.35918415 3.36086072 3.35389061 3.36066442 3.36175365\n",
      "  3.35644297 3.36043503 3.35703419 3.35758467 3.35404552 3.3554249\n",
      "  3.36461614]\n",
      " [4.24462442 3.99158258 3.8884118  3.79599355 3.71770144 3.64844098\n",
      "  3.59542198 3.55312162 3.50695848 3.46601337 3.41685024 3.40790187\n",
      "  3.40924861 3.39984752 3.40088405 3.39058224 3.38523748 3.38388116\n",
      "  3.37990044 3.3775011  3.36617271 3.37341378 3.37168802 3.36412867\n",
      "  3.36898328 3.36483701 3.36923458 3.36573547 3.36804207 3.36768999\n",
      "  3.36228781 3.36536712 3.36760426 3.36050088 3.3647559  3.36789576\n",
      "  3.36321644 3.36153303 3.36396593 3.36784409 3.36294534 3.36961336\n",
      "  3.36489975 3.36250209 3.36513126 3.3631786  3.36478959 3.36279063\n",
      "  3.36230007 3.36111713 3.36573946 3.36377741 3.36603524 3.36470506\n",
      "  3.36448403 3.36342201 3.36396158 3.36192902 3.36479376 3.36383152\n",
      "  3.36371167 3.36260142 3.36356903 3.36163196 3.36212051 3.36405027\n",
      "  3.36415168 3.35701964 3.36397638 3.36314436 3.3643407  3.36371907\n",
      "  3.36771424 3.36109944 3.36738434 3.36684193 3.36858893 3.36816235\n",
      "  3.36822207 3.361812   3.366861   3.36733122 3.36649715 3.36217722\n",
      "  3.36937282 3.36458379 3.36316183 3.36785401 3.35943088 3.36689864\n",
      "  3.36804499 3.37014393 3.36276574 3.36698877 3.36537651 3.36556995\n",
      "  3.36810197 3.36505699 3.36224376 3.36380108 3.36393761 3.36520594\n",
      "  3.36363611 3.36101434 3.36507175 3.36679635 3.36331721 3.36465719\n",
      "  3.37042576 3.36338435 3.36755666 3.36155454 3.36392006 3.35944767\n",
      "  3.36506246 3.36762073 3.36285293 3.36250115 3.36526997 3.36558488\n",
      "  3.36206466 3.36294786 3.36279086 3.35817301 3.36353212 3.36296479\n",
      "  3.36473493 3.36182062 3.36778343 3.36201369 3.36284018 3.36349603\n",
      "  3.3624896  3.36209774 3.36560298 3.36187259 3.36304109 3.35832173\n",
      "  3.36476626 3.36720744 3.36194826 3.36437759 3.36859414 3.36246348\n",
      "  3.36718257 3.36118312 3.36504906 3.36616002 3.36344825 3.36166003\n",
      "  3.3657496  3.36666669 3.36438273 3.36444606 3.36749655 3.3647823\n",
      "  3.36117869 3.36723682 3.36348769 3.36366258 3.36630913 3.36264065\n",
      "  3.36015817 3.36618731 3.36468921 3.36666089 3.36463404 3.3642834\n",
      "  3.36043814 3.36481814 3.36380233 3.36061547 3.36242389 3.36574632\n",
      "  3.36462993]\n",
      " [4.23579507 3.98702631 3.86860935 3.78422508 3.71333053 3.65728399\n",
      "  3.60191181 3.55052908 3.50734173 3.46764212 3.41427142 3.40354143\n",
      "  3.39910072 3.39202803 3.39284867 3.3832256  3.38693044 3.37404693\n",
      "  3.37445629 3.3690533  3.36456688 3.35752377 3.36215124 3.36438834\n",
      "  3.36227941 3.36058479 3.35609001 3.36011666 3.35495924 3.36174087\n",
      "  3.35267183 3.35761101 3.35824471 3.35598131 3.36072872 3.36273813\n",
      "  3.35683025 3.35795486 3.35068089 3.35652634 3.35645389 3.3542179\n",
      "  3.35942338 3.36054698 3.35409894 3.35084719 3.35953196 3.35654517\n",
      "  3.35752946 3.35976234 3.35595779 3.35643377 3.36038292 3.36015939\n",
      "  3.35373591 3.36047735 3.35436903 3.36113856 3.35125615 3.35835334\n",
      "  3.35908836 3.35656961 3.35650771 3.35453156 3.35595215 3.36122316\n",
      "  3.35995935 3.36222086 3.36020828 3.35505167 3.3548675  3.35682057\n",
      "  3.35806915 3.35723427 3.35653518 3.35857428 3.36244721 3.3558832\n",
      "  3.36034104 3.35846935 3.35860328 3.35999483 3.35405502 3.35577192\n",
      "  3.35818791 3.35855502 3.35090573 3.35600957 3.35649042 3.35460682\n",
      "  3.35848331 3.35952971 3.35817726 3.35431864 3.35498051 3.35917374\n",
      "  3.35967117 3.35623575 3.35275713 3.35577166 3.35604436 3.35617314\n",
      "  3.35433958 3.35806324 3.35551574 3.35738589 3.35593559 3.35607883\n",
      "  3.35535221 3.35412748 3.3573234  3.35839629 3.35731931 3.35263585\n",
      "  3.35139944 3.36149648 3.35640154 3.35600549 3.35991998 3.35285454\n",
      "  3.36148987 3.352056   3.35470679 3.35768571 3.35535396 3.35746514\n",
      "  3.35805563 3.350547   3.35592686 3.35738631 3.3600439  3.35322678\n",
      "  3.3584789  3.35978396 3.35250841 3.35818601 3.35920383 3.36258042\n",
      "  3.3547582  3.35802056 3.35521983 3.35737972 3.35660571 3.36084439\n",
      "  3.35934918 3.35564769 3.35448165 3.3557094  3.35786735 3.3570602\n",
      "  3.35967772 3.35688713 3.35786562 3.35862397 3.35568686 3.35732259\n",
      "  3.35529398 3.36196335 3.35611414 3.3559337  3.35448104 3.35362938\n",
      "  3.35121208 3.35270212 3.35596583 3.3539804  3.35563857 3.35587663\n",
      "  3.35778481 3.35588398 3.35237877 3.35818938 3.35732712 3.35794047\n",
      "  3.35959991]\n",
      " [4.26022901 3.98458344 3.86567494 3.77268856 3.69547428 3.63215468\n",
      "  3.58224914 3.5243765  3.48318145 3.44228675 3.39117542 3.38605055\n",
      "  3.37696668 3.37689962 3.37324104 3.35998522 3.35690288 3.35663184\n",
      "  3.34422595 3.34491729 3.34175216 3.33408262 3.3338498  3.32839744\n",
      "  3.33838955 3.33306163 3.33762283 3.33450515 3.33754108 3.33094585\n",
      "  3.33233397 3.3299382  3.33394889 3.33559835 3.33294392 3.33254281\n",
      "  3.33338743 3.32916749 3.33146424 3.3307062  3.33241815 3.33578203\n",
      "  3.33191647 3.3311375  3.33212794 3.32878884 3.33179731 3.33127184\n",
      "  3.33693004 3.33207903 3.33431243 3.33748585 3.33299876 3.33026733\n",
      "  3.33059851 3.33182674 3.33163648 3.33093639 3.33192625 3.32813252\n",
      "  3.33398722 3.33077723 3.33289003 3.33640486 3.32978751 3.33208598\n",
      "  3.33351076 3.33079648 3.33467899 3.33582327 3.33486737 3.33050187\n",
      "  3.33318615 3.33423284 3.33083766 3.33710066 3.32947495 3.33307679\n",
      "  3.32761057 3.3353736  3.33001516 3.33075467 3.33266623 3.33223456\n",
      "  3.33265666 3.33064205 3.32511278 3.3320727  3.33190714 3.32896418\n",
      "  3.32769589 3.33499851 3.33028395 3.330799   3.33403603 3.33093442\n",
      "  3.329364   3.32480531 3.33463711 3.32947546 3.3321896  3.33277768\n",
      "  3.32837736 3.33365191 3.32973422 3.33178682 3.33223686 3.33045291\n",
      "  3.33182159 3.33477071 3.33069535 3.33316492 3.33496466 3.33231448\n",
      "  3.33508001 3.33337741 3.33139171 3.33522018 3.33689092 3.33083923\n",
      "  3.33104996 3.33045824 3.32898302 3.33759815 3.33204883 3.3277211\n",
      "  3.32970027 3.33084553 3.33084924 3.33077128 3.33111971 3.33206444\n",
      "  3.3331892  3.33501023 3.32867403 3.32920342 3.33232596 3.33293847\n",
      "  3.33247102 3.33250375 3.3319186  3.33104535 3.33382921 3.33293701\n",
      "  3.33294433 3.33621587 3.33348481 3.33163492 3.33129526 3.3319169\n",
      "  3.33136972 3.32634445 3.33258181 3.33508812 3.32853767 3.33352545\n",
      "  3.33140292 3.33376903 3.33180188 3.33368664 3.33188207 3.32776915\n",
      "  3.33227615 3.3341144  3.33106064 3.32957419 3.3328521  3.33105427\n",
      "  3.3343281  3.33264816 3.33542802 3.3295492  3.32615778 3.33380463\n",
      "  3.33169576]\n",
      " [4.2921238  4.01818811 3.91782748 3.83746284 3.76910558 3.71252286\n",
      "  3.65912823 3.60777664 3.56123811 3.53120104 3.47368085 3.46406311\n",
      "  3.46246476 3.45302708 3.45392284 3.44608253 3.4432544  3.44012621\n",
      "  3.43609149 3.43161452 3.43064876 3.42444318 3.42353503 3.42692209\n",
      "  3.42337807 3.42557027 3.4240679  3.42298718 3.42153494 3.4239578\n",
      "  3.42319828 3.42325003 3.42555209 3.42412822 3.42030538 3.42180424\n",
      "  3.42392057 3.42620115 3.4217542  3.42157217 3.42250624 3.42577238\n",
      "  3.42655682 3.42076692 3.42237904 3.43016303 3.42507322 3.4184852\n",
      "  3.42109911 3.42474604 3.42190286 3.42426822 3.42021871 3.42184979\n",
      "  3.42517037 3.42235324 3.4249935  3.42459393 3.42602701 3.42354807\n",
      "  3.42483693 3.42368901 3.42743131 3.42203942 3.41853933 3.42532287\n",
      "  3.42565444 3.42460339 3.42084016 3.42521455 3.42602914 3.42248558\n",
      "  3.41903152 3.42317056 3.42118416 3.422544   3.42260376 3.42779196\n",
      "  3.42534917 3.42125385 3.42762838 3.42142589 3.42517016 3.41933439\n",
      "  3.42545897 3.42069359 3.42511676 3.42200159 3.42145052 3.41957724\n",
      "  3.42629887 3.42216845 3.42125251 3.42422113 3.42336334 3.4238396\n",
      "  3.42380138 3.42221539 3.42629404 3.42392199 3.4210267  3.42498224\n",
      "  3.4201371  3.42281757 3.4229403  3.42528182 3.42397714 3.42356153\n",
      "  3.42268447 3.42333033 3.42199523 3.42326352 3.4231618  3.42596848\n",
      "  3.42925455 3.42550263 3.42160921 3.42357164 3.42599092 3.42239661\n",
      "  3.42462434 3.41965047 3.4224938  3.42540697 3.42142018 3.41950712\n",
      "  3.42224123 3.42303142 3.42206861 3.42538013 3.42153305 3.42199213\n",
      "  3.42445347 3.42258213 3.42320654 3.41888512 3.41963181 3.41940662\n",
      "  3.42006643 3.42510211 3.42446517 3.42254007 3.42725192 3.41849061\n",
      "  3.4261533  3.42295051 3.42545156 3.42270496 3.422974   3.42063253\n",
      "  3.42428435 3.42542081 3.42160965 3.42233018 3.41989111 3.42675133\n",
      "  3.4200796  3.4221156  3.42539114 3.42152141 3.4253211  3.42619673\n",
      "  3.41917317 3.42211439 3.42390904 3.42552939 3.42536329 3.42074095\n",
      "  3.42065712 3.42610402 3.42356519 3.42693245 3.42205968 3.42041973\n",
      "  3.42208457]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "checkpoint_dir = './checkpoints_cifar100'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 100)  \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                         download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                        download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "num_epochs = 175\n",
    "num_runs = 5\n",
    "\n",
    "\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "all_losses = []\n",
    "\n",
    "def save_checkpoint(run, model, optimizer, scheduler, train_accuracies, losses):\n",
    "    checkpoint = {\n",
    "        'run': run,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'losses': losses\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth'))\n",
    "\n",
    "def load_checkpoint(run):\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth'))\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Starting run {run + 1}/{num_runs}\")\n",
    "\n",
    "    \n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=0.0001)  \n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    \n",
    "    if os.path.exists(os.path.join(checkpoint_dir, f'checkpoint_run_{run}.pth')):\n",
    "        print(f\"Loading checkpoint for run {run + 1}\")\n",
    "        checkpoint = load_checkpoint(run)\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        train_accuracies = checkpoint['train_accuracies']\n",
    "        losses = checkpoint['losses']\n",
    "    else:\n",
    "        train_accuracies = []\n",
    "        losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()  \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        losses.append(running_loss / len(trainloader))\n",
    "        print(f\"Run {run+1}, Epoch {epoch+1}, Loss: {running_loss / len(trainloader):.3f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        scheduler.step()  \n",
    "\n",
    "    net.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f\"Run {run+1}, Final Accuracy on test set: {test_accuracy:.2f}%\")\n",
    "\n",
    "    all_train_accuracies.append(train_accuracies)\n",
    "    all_test_accuracies.append(test_accuracies)\n",
    "    all_losses.append(losses)\n",
    "\n",
    "    save_checkpoint(run, net, optimizer, scheduler, train_accuracies, losses)\n",
    "\n",
    "    print(f\"Results after run {run + 1}:\")\n",
    "    print(f\"Training Accuracies: {train_accuracies}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"Losses: {losses}\")\n",
    "\n",
    "all_train_accuracies = np.array(all_train_accuracies)\n",
    "all_test_accuracies = np.array(all_test_accuracies)\n",
    "all_losses = np.array(all_losses)\n",
    "\n",
    "np.savetxt('train_accuracies_cifar100.txt', all_train_accuracies)\n",
    "np.savetxt('test_accuracies_cifar100.txt', all_test_accuracies)\n",
    "np.savetxt('losses_cifar100.txt', all_losses)\n",
    "\n",
    "\n",
    "print(\"All Training Accuracies over Epochs for each run:\", all_train_accuracies)\n",
    "print(\"All Test Accuracies after each run:\", all_test_accuracies)\n",
    "print(\"All Losses over Epochs for each run:\", all_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27465.417249,
   "end_time": "2024-10-23T20:18:30.918618",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T12:40:45.501369",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
