{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e132fc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T15:51:06.563932Z",
     "iopub.status.busy": "2024-10-23T15:51:06.563578Z",
     "iopub.status.idle": "2024-10-23T15:51:09.727459Z",
     "shell.execute_reply": "2024-10-23T15:51:09.726664Z"
    },
    "papermill": {
     "duration": 3.170166,
     "end_time": "2024-10-23T15:51:09.729718",
     "exception": false,
     "start_time": "2024-10-23T15:51:06.559552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import TYPE_CHECKING, Any, Callable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import pdb\n",
    "import logging\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from torch.optim.optimizer import _params_t\n",
    "else:\n",
    "    _params_t = Any\n",
    "\n",
    "class DAdaptAdam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1.0,\n",
    "                 betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, log_every=0,\n",
    "                 decouple=False,\n",
    "                 use_bias_correction=False,\n",
    "                 d0=1e-6, growth_rate=float('inf'),\n",
    "                 fsdp_in_use=False):\n",
    "        if not 0.0 < d0:\n",
    "            raise ValueError(\"Invalid d0 value: {}\".format(d0))\n",
    "        if not 0.0 < lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 < eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        if decouple:\n",
    "            print(f\"Using decoupled weight decay\")\n",
    "\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        d = d0,\n",
    "                        k=0,\n",
    "                        layer_scale=1.0,\n",
    "                        numerator_weighted=0.0,\n",
    "                        log_every=log_every,\n",
    "                        growth_rate=growth_rate,\n",
    "                        use_bias_correction=use_bias_correction,\n",
    "                        decouple=decouple,\n",
    "                        fsdp_in_use=fsdp_in_use)\n",
    "        self.d0 = d0\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @property\n",
    "    def supports_memory_efficient_fp16(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def supports_flat_params(self):\n",
    "        return True\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        sk_l1 = 0.0\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        use_bias_correction = group['use_bias_correction']\n",
    "        numerator_weighted = group['numerator_weighted']\n",
    "        beta1, beta2 = group['betas']\n",
    "        k = group['k']\n",
    "\n",
    "        d = group['d']\n",
    "        lr = max(group['lr'] for group in self.param_groups)\n",
    "\n",
    "        if use_bias_correction:\n",
    "            bias_correction = ((1-beta2**(k+1))**0.5)/(1-beta1**(k+1))\n",
    "        else:\n",
    "            bias_correction = 1\n",
    "\n",
    "        dlr = d*lr*bias_correction\n",
    "\n",
    "        growth_rate = group['growth_rate']\n",
    "        decouple = group['decouple']\n",
    "        log_every = group['log_every']\n",
    "        fsdp_in_use = group['fsdp_in_use']\n",
    "\n",
    "\n",
    "        sqrt_beta2 = beta2**(0.5)\n",
    "\n",
    "        numerator_acum = 0.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            decay = group['weight_decay']\n",
    "            k = group['k']\n",
    "            eps = group['eps']\n",
    "            group_lr = group['lr']\n",
    "            r = group['layer_scale']\n",
    "\n",
    "            if group_lr not in [lr, 0.0]:\n",
    "                raise RuntimeError(f\"Setting different lr values in different parameter groups \"\n",
    "                                   \"is only supported for values of 0. To scale the learning \"\n",
    "                                   \"rate differently for each layer, set the 'layer_scale' value instead.\")\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if hasattr(p, \"_fsdp_flattened\"):\n",
    "                    fsdp_in_use = True\n",
    "\n",
    "                grad = p.grad.data\n",
    "\n",
    "                if decay != 0 and not decouple:\n",
    "                    grad.add_(p.data, alpha=decay)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if 'step' not in state:\n",
    "                    state['step'] = 0\n",
    "                    state['s'] = torch.zeros_like(p.data).detach()\n",
    "                \n",
    "                    state['exp_avg'] = torch.zeros_like(p.data).detach()\n",
    "               \n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data).detach()\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                s = state['s']\n",
    "\n",
    "                if group_lr > 0.0:\n",
    "                    denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                    numerator_acum += r * dlr * torch.dot(grad.flatten(), s.div(denom).flatten()).item()\n",
    "\n",
    "                 \n",
    "                    exp_avg.mul_(beta1).add_(grad, alpha=r*dlr*(1-beta1))\n",
    "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "\n",
    "                    s.mul_(sqrt_beta2).add_(grad, alpha=dlr*(1-sqrt_beta2))\n",
    "                    sk_l1 += r * s.abs().sum().item()\n",
    "\n",
    "\n",
    "        numerator_weighted = sqrt_beta2*numerator_weighted + (1-sqrt_beta2)*numerator_acum\n",
    "        d_hat = d\n",
    "        if sk_l1 == 0:\n",
    "            return loss\n",
    "\n",
    "        if lr > 0.0:\n",
    "            if fsdp_in_use:\n",
    "                dist_tensor = torch.zeros(2).cuda()\n",
    "                dist_tensor[0] = numerator_weighted\n",
    "                dist_tensor[1] = sk_l1\n",
    "                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)\n",
    "                global_numerator_weighted = dist_tensor[0]\n",
    "                global_sk_l1 = dist_tensor[1]\n",
    "            else:\n",
    "                global_numerator_weighted = numerator_weighted\n",
    "                global_sk_l1 = sk_l1\n",
    "\n",
    "\n",
    "            d_hat = global_numerator_weighted/((1-sqrt_beta2)*global_sk_l1)\n",
    "            d = max(d, min(d_hat, d*growth_rate))\n",
    "\n",
    "        if log_every > 0 and k % log_every == 0:\n",
    "            logging.info(f\"lr: {lr} dlr: {dlr} d_hat: {d_hat}, d: {d}. sk_l1={global_sk_l1:1.1e} numerator_weighted={global_numerator_weighted:1.1e}\")\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            group['numerator_weighted'] = numerator_weighted\n",
    "            group['d'] = d\n",
    "\n",
    "            decay = group['weight_decay']\n",
    "            k = group['k']\n",
    "            eps = group['eps']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "\n",
    "                if decay != 0 and decouple:\n",
    "                    p.data.add_(p.data, alpha=-decay * dlr)\n",
    "\n",
    "\n",
    "              \n",
    "                p.data.addcdiv_(exp_avg, denom, value=-1)\n",
    "\n",
    "            group['k'] = k + 1\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e27cea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T15:51:09.735614Z",
     "iopub.status.busy": "2024-10-23T15:51:09.735191Z",
     "iopub.status.idle": "2024-10-23T23:51:38.604116Z",
     "shell.execute_reply": "2024-10-23T23:51:38.603103Z"
    },
    "papermill": {
     "duration": 28828.874262,
     "end_time": "2024-10-23T23:51:38.606219",
     "exception": false,
     "start_time": "2024-10-23T15:51:09.731957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:08<00:00, 20121779.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-100-python.tar.gz to data\n",
      "Files already downloaded and verified\n",
      "Starting training for hyperparameter configuration 1/3\n",
      "Epoch 1, Loss: 3.4500110987812054, Training Accuracy: 18.12%\n",
      "Epoch 2, Loss: 2.711748654580177, Training Accuracy: 31.87%\n",
      "Epoch 3, Loss: 2.41805437306309, Training Accuracy: 37.94%\n",
      "Epoch 4, Loss: 2.2263208678006516, Training Accuracy: 42.05%\n",
      "Epoch 5, Loss: 2.074692116369067, Training Accuracy: 45.40%\n",
      "Epoch 6, Loss: 1.9364961272920185, Training Accuracy: 48.44%\n",
      "Epoch 7, Loss: 1.809774511458014, Training Accuracy: 51.49%\n",
      "Epoch 8, Loss: 1.7002364009847422, Training Accuracy: 54.26%\n",
      "Epoch 9, Loss: 1.5939034759388555, Training Accuracy: 56.63%\n",
      "Epoch 10, Loss: 1.4940031250876844, Training Accuracy: 59.07%\n",
      "Epoch 11, Loss: 1.3997029525697078, Training Accuracy: 61.34%\n",
      "Epoch 12, Loss: 1.2991076842750735, Training Accuracy: 63.98%\n",
      "Epoch 13, Loss: 1.20836092032435, Training Accuracy: 65.99%\n",
      "Epoch 14, Loss: 1.1157957218644563, Training Accuracy: 68.40%\n",
      "Epoch 15, Loss: 1.0259450510944552, Training Accuracy: 70.85%\n",
      "Epoch 16, Loss: 0.9401349472572736, Training Accuracy: 72.88%\n",
      "Epoch 17, Loss: 0.8653999150866438, Training Accuracy: 74.98%\n",
      "Epoch 18, Loss: 0.7851844836988717, Training Accuracy: 77.32%\n",
      "Epoch 19, Loss: 0.7148718925388268, Training Accuracy: 79.12%\n",
      "Epoch 20, Loss: 0.6486775576687225, Training Accuracy: 81.05%\n",
      "Epoch 21, Loss: 0.5778335401850283, Training Accuracy: 83.00%\n",
      "Epoch 22, Loss: 0.5250661288723921, Training Accuracy: 84.64%\n",
      "Epoch 23, Loss: 0.4692007320387589, Training Accuracy: 86.20%\n",
      "Epoch 24, Loss: 0.4221508759824211, Training Accuracy: 87.37%\n",
      "Epoch 25, Loss: 0.37605389795454264, Training Accuracy: 88.72%\n",
      "Epoch 26, Loss: 0.33668027837258163, Training Accuracy: 89.85%\n",
      "Epoch 27, Loss: 0.31178866882267814, Training Accuracy: 90.39%\n",
      "Epoch 28, Loss: 0.27479713085248036, Training Accuracy: 91.58%\n",
      "Epoch 29, Loss: 0.2525738194165632, Training Accuracy: 92.05%\n",
      "Epoch 30, Loss: 0.22645761002131434, Training Accuracy: 93.00%\n",
      "Epoch 31, Loss: 0.21669856210232086, Training Accuracy: 93.24%\n",
      "Epoch 32, Loss: 0.2011292957607895, Training Accuracy: 93.78%\n",
      "Epoch 33, Loss: 0.18844899920570424, Training Accuracy: 94.07%\n",
      "Epoch 34, Loss: 0.17988398485362073, Training Accuracy: 94.30%\n",
      "Epoch 35, Loss: 0.16310764623858282, Training Accuracy: 94.91%\n",
      "Epoch 36, Loss: 0.16669873431172516, Training Accuracy: 94.55%\n",
      "Epoch 37, Loss: 0.15050510885884694, Training Accuracy: 95.23%\n",
      "Epoch 38, Loss: 0.1513517695571989, Training Accuracy: 95.07%\n",
      "Epoch 39, Loss: 0.14788702886332483, Training Accuracy: 95.19%\n",
      "Epoch 40, Loss: 0.13110892253134715, Training Accuracy: 95.74%\n",
      "Epoch 41, Loss: 0.12658913514655454, Training Accuracy: 95.92%\n",
      "Epoch 42, Loss: 0.1438711757111885, Training Accuracy: 95.28%\n",
      "Epoch 43, Loss: 0.12787814280661322, Training Accuracy: 95.87%\n",
      "Epoch 44, Loss: 0.11535456680866611, Training Accuracy: 96.24%\n",
      "Epoch 45, Loss: 0.13095015373445398, Training Accuracy: 95.71%\n",
      "Epoch 46, Loss: 0.12379527643508732, Training Accuracy: 95.86%\n",
      "Epoch 47, Loss: 0.10949007988266667, Training Accuracy: 96.38%\n",
      "Epoch 48, Loss: 0.11530357358805701, Training Accuracy: 96.31%\n",
      "Epoch 49, Loss: 0.11983970316874859, Training Accuracy: 96.06%\n",
      "Epoch 50, Loss: 0.10040146323835568, Training Accuracy: 96.61%\n",
      "Epoch 51, Loss: 0.11726861161982541, Training Accuracy: 96.03%\n",
      "Epoch 52, Loss: 0.10419059625905379, Training Accuracy: 96.60%\n",
      "Epoch 53, Loss: 0.09400912847660502, Training Accuracy: 96.92%\n",
      "Epoch 54, Loss: 0.11379778946695082, Training Accuracy: 96.28%\n",
      "Epoch 55, Loss: 0.1070016092904236, Training Accuracy: 96.48%\n",
      "Epoch 56, Loss: 0.09808674531267084, Training Accuracy: 96.77%\n",
      "Epoch 57, Loss: 0.10767146230310849, Training Accuracy: 96.38%\n",
      "Epoch 58, Loss: 0.09558874235578986, Training Accuracy: 96.89%\n",
      "Epoch 59, Loss: 0.10042368938974902, Training Accuracy: 96.82%\n",
      "Epoch 60, Loss: 0.09824831380158701, Training Accuracy: 96.86%\n",
      "Epoch 61, Loss: 0.10867827136993713, Training Accuracy: 96.48%\n",
      "Epoch 62, Loss: 0.07880690294172606, Training Accuracy: 97.43%\n",
      "Epoch 63, Loss: 0.10451257546507346, Training Accuracy: 96.55%\n",
      "Epoch 64, Loss: 0.09240645608957619, Training Accuracy: 97.02%\n",
      "Epoch 65, Loss: 0.08153055365229517, Training Accuracy: 97.29%\n",
      "Epoch 66, Loss: 0.0860669371002661, Training Accuracy: 97.21%\n",
      "Epoch 67, Loss: 0.0928090923427678, Training Accuracy: 97.01%\n",
      "Epoch 68, Loss: 0.09657245288576807, Training Accuracy: 96.83%\n",
      "Epoch 69, Loss: 0.08095156316689509, Training Accuracy: 97.44%\n",
      "Epoch 70, Loss: 0.08920410103084224, Training Accuracy: 97.01%\n",
      "Epoch 71, Loss: 0.07787632345117133, Training Accuracy: 97.42%\n",
      "Epoch 72, Loss: 0.09375408297235532, Training Accuracy: 97.05%\n",
      "Epoch 73, Loss: 0.07811671494752825, Training Accuracy: 97.48%\n",
      "Epoch 74, Loss: 0.09422590898747658, Training Accuracy: 97.00%\n",
      "Epoch 75, Loss: 0.09045012182880031, Training Accuracy: 97.09%\n",
      "Epoch 76, Loss: 0.06847609460736147, Training Accuracy: 97.80%\n",
      "Epoch 77, Loss: 0.07952951764047166, Training Accuracy: 97.43%\n",
      "Epoch 78, Loss: 0.09194325938906588, Training Accuracy: 97.09%\n",
      "Epoch 79, Loss: 0.07634625070558056, Training Accuracy: 97.55%\n",
      "Epoch 80, Loss: 0.07771065662813889, Training Accuracy: 97.44%\n",
      "Epoch 81, Loss: 0.08733829473490086, Training Accuracy: 97.21%\n",
      "Epoch 82, Loss: 0.08444369732654747, Training Accuracy: 97.39%\n",
      "Epoch 83, Loss: 0.07580656781965328, Training Accuracy: 97.58%\n",
      "Epoch 84, Loss: 0.08473654270477002, Training Accuracy: 97.31%\n",
      "Epoch 85, Loss: 0.07631968076699926, Training Accuracy: 97.60%\n",
      "Epoch 86, Loss: 0.07212699738347812, Training Accuracy: 97.74%\n",
      "Epoch 87, Loss: 0.07797349890758928, Training Accuracy: 97.53%\n",
      "Epoch 88, Loss: 0.0724889759639459, Training Accuracy: 97.68%\n",
      "Epoch 89, Loss: 0.07571750796993104, Training Accuracy: 97.56%\n",
      "Epoch 90, Loss: 0.07296125211736873, Training Accuracy: 97.61%\n",
      "Epoch 91, Loss: 0.08045549934391705, Training Accuracy: 97.50%\n",
      "Epoch 92, Loss: 0.07068265132475621, Training Accuracy: 97.80%\n",
      "Epoch 93, Loss: 0.08034553669025059, Training Accuracy: 97.48%\n",
      "Epoch 94, Loss: 0.07205242359096094, Training Accuracy: 97.69%\n",
      "Epoch 95, Loss: 0.08286480874027413, Training Accuracy: 97.45%\n",
      "Epoch 96, Loss: 0.06638193929903186, Training Accuracy: 97.91%\n",
      "Epoch 97, Loss: 0.0643391179090873, Training Accuracy: 97.83%\n",
      "Epoch 98, Loss: 0.0777059785163928, Training Accuracy: 97.49%\n",
      "Epoch 99, Loss: 0.0652821872763269, Training Accuracy: 97.89%\n",
      "Epoch 100, Loss: 0.07384501609613624, Training Accuracy: 97.69%\n",
      "Epoch 101, Loss: 0.07736107535497107, Training Accuracy: 97.55%\n",
      "Epoch 102, Loss: 0.07060605226083995, Training Accuracy: 97.88%\n",
      "Epoch 103, Loss: 0.06760102786596262, Training Accuracy: 97.91%\n",
      "Epoch 104, Loss: 0.06943144099733285, Training Accuracy: 97.79%\n",
      "Epoch 105, Loss: 0.07343866063065736, Training Accuracy: 97.68%\n",
      "Epoch 106, Loss: 0.07125922344165588, Training Accuracy: 97.76%\n",
      "Epoch 107, Loss: 0.07224443615705985, Training Accuracy: 97.76%\n",
      "Epoch 108, Loss: 0.06024497442701927, Training Accuracy: 98.11%\n",
      "Epoch 109, Loss: 0.07010195629529493, Training Accuracy: 97.80%\n",
      "Epoch 110, Loss: 0.06743211296099402, Training Accuracy: 97.90%\n",
      "Epoch 111, Loss: 0.05854546807124577, Training Accuracy: 98.18%\n",
      "Epoch 112, Loss: 0.07322587671524743, Training Accuracy: 97.73%\n",
      "Epoch 113, Loss: 0.07311094021546724, Training Accuracy: 97.79%\n",
      "Epoch 114, Loss: 0.07484273940605847, Training Accuracy: 97.66%\n",
      "Epoch 115, Loss: 0.04374600173723872, Training Accuracy: 98.61%\n",
      "Epoch 116, Loss: 0.07115584521090237, Training Accuracy: 97.82%\n",
      "Epoch 117, Loss: 0.06939252928137402, Training Accuracy: 97.94%\n",
      "Epoch 118, Loss: 0.0702877841013617, Training Accuracy: 97.89%\n",
      "Epoch 119, Loss: 0.07020692849490683, Training Accuracy: 97.94%\n",
      "Epoch 120, Loss: 0.062071408937334455, Training Accuracy: 98.06%\n",
      "Epoch 121, Loss: 0.06170515652159122, Training Accuracy: 98.06%\n",
      "Epoch 122, Loss: 0.06533247686572262, Training Accuracy: 97.97%\n",
      "Epoch 123, Loss: 0.05766471339788055, Training Accuracy: 98.17%\n",
      "Epoch 124, Loss: 0.0793245265569539, Training Accuracy: 97.70%\n",
      "Epoch 125, Loss: 0.062016598206139084, Training Accuracy: 98.05%\n",
      "Epoch 126, Loss: 0.05795893780349711, Training Accuracy: 98.22%\n",
      "Epoch 127, Loss: 0.06390915023821854, Training Accuracy: 98.06%\n",
      "Epoch 128, Loss: 0.07135258355397371, Training Accuracy: 97.80%\n",
      "Epoch 129, Loss: 0.04997039415863985, Training Accuracy: 98.43%\n",
      "Epoch 130, Loss: 0.06326328440780139, Training Accuracy: 98.06%\n",
      "Epoch 131, Loss: 0.06841973027647913, Training Accuracy: 97.96%\n",
      "Epoch 132, Loss: 0.06369205485587097, Training Accuracy: 98.06%\n",
      "Epoch 133, Loss: 0.05843126423241001, Training Accuracy: 98.23%\n",
      "Epoch 134, Loss: 0.06750600499933457, Training Accuracy: 98.00%\n",
      "Epoch 135, Loss: 0.05445108029504885, Training Accuracy: 98.32%\n",
      "Epoch 136, Loss: 0.054301636496736236, Training Accuracy: 98.27%\n",
      "Epoch 137, Loss: 0.06705010015985814, Training Accuracy: 97.99%\n",
      "Epoch 138, Loss: 0.06624731192181213, Training Accuracy: 98.07%\n",
      "Epoch 139, Loss: 0.05290951303013897, Training Accuracy: 98.40%\n",
      "Epoch 140, Loss: 0.06733824580062545, Training Accuracy: 97.98%\n",
      "Epoch 141, Loss: 0.057737578447420466, Training Accuracy: 98.22%\n",
      "Epoch 142, Loss: 0.05371129224733378, Training Accuracy: 98.33%\n",
      "Epoch 143, Loss: 0.06452266815427915, Training Accuracy: 98.17%\n",
      "Epoch 144, Loss: 0.05747563929805119, Training Accuracy: 98.27%\n",
      "Epoch 145, Loss: 0.051335873298884116, Training Accuracy: 98.44%\n",
      "Epoch 146, Loss: 0.06160632459595061, Training Accuracy: 98.14%\n",
      "Epoch 147, Loss: 0.05956802390164623, Training Accuracy: 98.21%\n",
      "Epoch 148, Loss: 0.05755310116588157, Training Accuracy: 98.29%\n",
      "Epoch 149, Loss: 0.06089104581071966, Training Accuracy: 98.18%\n",
      "Epoch 150, Loss: 0.05930385529417205, Training Accuracy: 98.25%\n",
      "Epoch 151, Loss: 0.0567658048678141, Training Accuracy: 98.34%\n",
      "Epoch 152, Loss: 0.05544378107644843, Training Accuracy: 98.29%\n",
      "Epoch 153, Loss: 0.05917172565275212, Training Accuracy: 98.18%\n",
      "Epoch 154, Loss: 0.06635639213343851, Training Accuracy: 98.07%\n",
      "Epoch 155, Loss: 0.05335611098835542, Training Accuracy: 98.37%\n",
      "Epoch 156, Loss: 0.054635999368807676, Training Accuracy: 98.43%\n",
      "Epoch 157, Loss: 0.06128040227268333, Training Accuracy: 98.22%\n",
      "Epoch 158, Loss: 0.05565197307709813, Training Accuracy: 98.36%\n",
      "Epoch 159, Loss: 0.05604036399637453, Training Accuracy: 98.36%\n",
      "Epoch 160, Loss: 0.06604210231266722, Training Accuracy: 98.15%\n",
      "Epoch 161, Loss: 0.05166993736431016, Training Accuracy: 98.44%\n",
      "Epoch 162, Loss: 0.05568508341003296, Training Accuracy: 98.46%\n",
      "Epoch 163, Loss: 0.059965877671070804, Training Accuracy: 98.24%\n",
      "Epoch 164, Loss: 0.05296600043950135, Training Accuracy: 98.46%\n",
      "Epoch 165, Loss: 0.06149267819494961, Training Accuracy: 98.32%\n",
      "Epoch 166, Loss: 0.05452738235846919, Training Accuracy: 98.45%\n",
      "Epoch 167, Loss: 0.05729708284840209, Training Accuracy: 98.35%\n",
      "Epoch 168, Loss: 0.05355849587851848, Training Accuracy: 98.52%\n",
      "Epoch 169, Loss: 0.06183782399670854, Training Accuracy: 98.17%\n",
      "Epoch 170, Loss: 0.058423382533069425, Training Accuracy: 98.33%\n",
      "Epoch 171, Loss: 0.0489267323162615, Training Accuracy: 98.54%\n",
      "Epoch 172, Loss: 0.058998684674613854, Training Accuracy: 98.35%\n",
      "Epoch 173, Loss: 0.05452145763236033, Training Accuracy: 98.46%\n",
      "Epoch 174, Loss: 0.05712833916376163, Training Accuracy: 98.37%\n",
      "Epoch 175, Loss: 0.0495852477962005, Training Accuracy: 98.49%\n",
      "Accuracy of the network on the 10000 test images: 32.29%\n",
      "Starting training for hyperparameter configuration 2/3\n",
      "Epoch 1, Loss: 3.9544032336500905, Training Accuracy: 8.90%\n",
      "Epoch 2, Loss: 3.592073708239114, Training Accuracy: 14.61%\n",
      "Epoch 3, Loss: 3.406735704378094, Training Accuracy: 17.95%\n",
      "Epoch 4, Loss: 3.324195061803169, Training Accuracy: 19.46%\n",
      "Epoch 5, Loss: 3.278236992828681, Training Accuracy: 20.43%\n",
      "Epoch 6, Loss: 3.252003452662007, Training Accuracy: 21.02%\n",
      "Epoch 7, Loss: 3.234457494962551, Training Accuracy: 21.12%\n",
      "Epoch 8, Loss: 3.220188661304581, Training Accuracy: 21.61%\n",
      "Epoch 9, Loss: 3.2127716571778593, Training Accuracy: 21.60%\n",
      "Epoch 10, Loss: 3.2033538400669537, Training Accuracy: 21.89%\n",
      "Epoch 11, Loss: 3.197617814059148, Training Accuracy: 22.30%\n",
      "Epoch 12, Loss: 3.1869398078040394, Training Accuracy: 22.23%\n",
      "Epoch 13, Loss: 3.176161022137498, Training Accuracy: 22.33%\n",
      "Epoch 14, Loss: 3.175058029191878, Training Accuracy: 22.57%\n",
      "Epoch 15, Loss: 3.17155654564538, Training Accuracy: 22.68%\n",
      "Epoch 16, Loss: 3.1637385361029975, Training Accuracy: 22.92%\n",
      "Epoch 17, Loss: 3.1574318152864267, Training Accuracy: 22.91%\n",
      "Epoch 18, Loss: 3.150595262532344, Training Accuracy: 22.98%\n",
      "Epoch 19, Loss: 3.144041812023543, Training Accuracy: 23.17%\n",
      "Epoch 20, Loss: 3.1419828200279296, Training Accuracy: 23.34%\n",
      "Epoch 21, Loss: 3.1371260777763696, Training Accuracy: 23.44%\n",
      "Epoch 22, Loss: 3.1274014418692233, Training Accuracy: 23.36%\n",
      "Epoch 23, Loss: 3.127639231779386, Training Accuracy: 23.33%\n",
      "Epoch 24, Loss: 3.119302550850012, Training Accuracy: 23.70%\n",
      "Epoch 25, Loss: 3.116532338549719, Training Accuracy: 23.69%\n",
      "Epoch 26, Loss: 3.1089032162790713, Training Accuracy: 23.79%\n",
      "Epoch 27, Loss: 3.112124736656618, Training Accuracy: 23.52%\n",
      "Epoch 28, Loss: 3.111055189996119, Training Accuracy: 23.62%\n",
      "Epoch 29, Loss: 3.1083300939911163, Training Accuracy: 23.63%\n",
      "Epoch 30, Loss: 3.1073443917057397, Training Accuracy: 24.15%\n",
      "Epoch 31, Loss: 3.0999167834401438, Training Accuracy: 23.95%\n",
      "Epoch 32, Loss: 3.1031858814341944, Training Accuracy: 23.88%\n",
      "Epoch 33, Loss: 3.095969642519646, Training Accuracy: 24.02%\n",
      "Epoch 34, Loss: 3.095927031753618, Training Accuracy: 24.02%\n",
      "Epoch 35, Loss: 3.0933337498198994, Training Accuracy: 24.17%\n",
      "Epoch 36, Loss: 3.095472872104791, Training Accuracy: 23.98%\n",
      "Epoch 37, Loss: 3.0902134774591, Training Accuracy: 24.37%\n",
      "Epoch 38, Loss: 3.0903882239480764, Training Accuracy: 24.16%\n",
      "Epoch 39, Loss: 3.09135274021217, Training Accuracy: 24.21%\n",
      "Epoch 40, Loss: 3.090069909229913, Training Accuracy: 24.21%\n",
      "Epoch 41, Loss: 3.0894424406158953, Training Accuracy: 24.22%\n",
      "Epoch 42, Loss: 3.0877819643606004, Training Accuracy: 24.02%\n",
      "Epoch 43, Loss: 3.087847692277425, Training Accuracy: 24.28%\n",
      "Epoch 44, Loss: 3.0848385586458096, Training Accuracy: 24.23%\n",
      "Epoch 45, Loss: 3.087965305808865, Training Accuracy: 24.13%\n",
      "Epoch 46, Loss: 3.083082170742552, Training Accuracy: 24.29%\n",
      "Epoch 47, Loss: 3.084057702737696, Training Accuracy: 24.29%\n",
      "Epoch 48, Loss: 3.08119082085007, Training Accuracy: 24.20%\n",
      "Epoch 49, Loss: 3.081988671246697, Training Accuracy: 24.09%\n",
      "Epoch 50, Loss: 3.082829106799172, Training Accuracy: 24.36%\n",
      "Epoch 51, Loss: 3.0866757613008895, Training Accuracy: 24.16%\n",
      "Epoch 52, Loss: 3.0785706945697364, Training Accuracy: 24.37%\n",
      "Epoch 53, Loss: 3.0803617306072693, Training Accuracy: 24.33%\n",
      "Epoch 54, Loss: 3.0785953358311176, Training Accuracy: 24.23%\n",
      "Epoch 55, Loss: 3.081813707680958, Training Accuracy: 24.27%\n",
      "Epoch 56, Loss: 3.076381019009349, Training Accuracy: 24.47%\n",
      "Epoch 57, Loss: 3.0756631403627908, Training Accuracy: 24.35%\n",
      "Epoch 58, Loss: 3.0818386837039764, Training Accuracy: 24.29%\n",
      "Epoch 59, Loss: 3.074116052873909, Training Accuracy: 24.60%\n",
      "Epoch 60, Loss: 3.0802337226965237, Training Accuracy: 24.37%\n",
      "Epoch 61, Loss: 3.0795072353709383, Training Accuracy: 24.27%\n",
      "Epoch 62, Loss: 3.0799550278412413, Training Accuracy: 24.55%\n",
      "Epoch 63, Loss: 3.0745119679614406, Training Accuracy: 24.42%\n",
      "Epoch 64, Loss: 3.072836458225689, Training Accuracy: 24.46%\n",
      "Epoch 65, Loss: 3.071885958047169, Training Accuracy: 24.51%\n",
      "Epoch 66, Loss: 3.0761778787578766, Training Accuracy: 24.16%\n",
      "Epoch 67, Loss: 3.0681299526063377, Training Accuracy: 24.32%\n",
      "Epoch 68, Loss: 3.074958463763947, Training Accuracy: 24.37%\n",
      "Epoch 69, Loss: 3.0689239733664277, Training Accuracy: 24.30%\n",
      "Epoch 70, Loss: 3.0692064661504057, Training Accuracy: 24.69%\n",
      "Epoch 71, Loss: 3.0700686910878057, Training Accuracy: 24.44%\n",
      "Epoch 72, Loss: 3.0683924871332504, Training Accuracy: 24.51%\n",
      "Epoch 73, Loss: 3.0704831558725107, Training Accuracy: 24.38%\n",
      "Epoch 74, Loss: 3.0693490535706816, Training Accuracy: 24.53%\n",
      "Epoch 75, Loss: 3.067487844115938, Training Accuracy: 24.45%\n",
      "Epoch 76, Loss: 3.0642338902749064, Training Accuracy: 24.60%\n",
      "Epoch 77, Loss: 3.0651097614746874, Training Accuracy: 24.46%\n",
      "Epoch 78, Loss: 3.066094806432114, Training Accuracy: 24.52%\n",
      "Epoch 79, Loss: 3.066329125553141, Training Accuracy: 24.52%\n",
      "Epoch 80, Loss: 3.062846312742404, Training Accuracy: 24.34%\n",
      "Epoch 81, Loss: 3.0610757919833484, Training Accuracy: 24.49%\n",
      "Epoch 82, Loss: 3.0599365020956832, Training Accuracy: 24.54%\n",
      "Epoch 83, Loss: 3.0566955338353696, Training Accuracy: 24.44%\n",
      "Epoch 84, Loss: 3.0547438012364574, Training Accuracy: 24.60%\n",
      "Epoch 85, Loss: 3.056688405058878, Training Accuracy: 24.60%\n",
      "Epoch 86, Loss: 3.0559256823776324, Training Accuracy: 24.82%\n",
      "Epoch 87, Loss: 3.0505192798116934, Training Accuracy: 24.94%\n",
      "Epoch 88, Loss: 3.0501816919087754, Training Accuracy: 24.96%\n",
      "Epoch 89, Loss: 3.050180225725979, Training Accuracy: 24.87%\n",
      "Epoch 90, Loss: 3.050014567192253, Training Accuracy: 24.72%\n",
      "Epoch 91, Loss: 3.046514959286546, Training Accuracy: 24.85%\n",
      "Epoch 92, Loss: 3.0452246327534356, Training Accuracy: 24.82%\n",
      "Epoch 93, Loss: 3.04903326772363, Training Accuracy: 24.78%\n",
      "Epoch 94, Loss: 3.0493854098307813, Training Accuracy: 24.80%\n",
      "Epoch 95, Loss: 3.04431211277652, Training Accuracy: 24.85%\n",
      "Epoch 96, Loss: 3.0430839375766645, Training Accuracy: 24.94%\n",
      "Epoch 97, Loss: 3.041027379157903, Training Accuracy: 24.89%\n",
      "Epoch 98, Loss: 3.0447609857525055, Training Accuracy: 24.84%\n",
      "Epoch 99, Loss: 3.0397703181142393, Training Accuracy: 25.01%\n",
      "Epoch 100, Loss: 3.042303918572643, Training Accuracy: 24.93%\n",
      "Epoch 101, Loss: 3.0446673177392283, Training Accuracy: 24.75%\n",
      "Epoch 102, Loss: 3.0388421283658507, Training Accuracy: 25.03%\n",
      "Epoch 103, Loss: 3.034470920062736, Training Accuracy: 24.95%\n",
      "Epoch 104, Loss: 3.044628622281887, Training Accuracy: 24.88%\n",
      "Epoch 105, Loss: 3.039363373880801, Training Accuracy: 24.98%\n",
      "Epoch 106, Loss: 3.0358421439709873, Training Accuracy: 25.03%\n",
      "Epoch 107, Loss: 3.0390679156383893, Training Accuracy: 24.94%\n",
      "Epoch 108, Loss: 3.04015834069313, Training Accuracy: 25.11%\n",
      "Epoch 109, Loss: 3.0338375830589355, Training Accuracy: 25.13%\n",
      "Epoch 110, Loss: 3.0317452963050977, Training Accuracy: 25.01%\n",
      "Epoch 111, Loss: 3.037990418236579, Training Accuracy: 24.99%\n",
      "Epoch 112, Loss: 3.0325242259618266, Training Accuracy: 25.09%\n",
      "Epoch 113, Loss: 3.031912936274048, Training Accuracy: 25.18%\n",
      "Epoch 114, Loss: 3.0319888414934164, Training Accuracy: 25.01%\n",
      "Epoch 115, Loss: 3.0345161485550043, Training Accuracy: 25.32%\n",
      "Epoch 116, Loss: 3.0349634074798937, Training Accuracy: 25.11%\n",
      "Epoch 117, Loss: 3.0369381523498182, Training Accuracy: 25.03%\n",
      "Epoch 118, Loss: 3.0309731801757422, Training Accuracy: 24.91%\n",
      "Epoch 119, Loss: 3.032906621618344, Training Accuracy: 25.09%\n",
      "Epoch 120, Loss: 3.0309174091309843, Training Accuracy: 25.09%\n",
      "Epoch 121, Loss: 3.026779211695542, Training Accuracy: 25.22%\n",
      "Epoch 122, Loss: 3.02825609985215, Training Accuracy: 25.14%\n",
      "Epoch 123, Loss: 3.0277063654511784, Training Accuracy: 25.04%\n",
      "Epoch 124, Loss: 3.031095778850643, Training Accuracy: 25.17%\n",
      "Epoch 125, Loss: 3.030811664698374, Training Accuracy: 25.27%\n",
      "Epoch 126, Loss: 3.028627764233543, Training Accuracy: 25.24%\n",
      "Epoch 127, Loss: 3.024922288287326, Training Accuracy: 25.20%\n",
      "Epoch 128, Loss: 3.032453616561792, Training Accuracy: 25.02%\n",
      "Epoch 129, Loss: 3.0292264406028613, Training Accuracy: 25.26%\n",
      "Epoch 130, Loss: 3.029310716997327, Training Accuracy: 25.02%\n",
      "Epoch 131, Loss: 3.028830700823108, Training Accuracy: 25.19%\n",
      "Epoch 132, Loss: 3.030268408758256, Training Accuracy: 25.17%\n",
      "Epoch 133, Loss: 3.0272202135047035, Training Accuracy: 24.88%\n",
      "Epoch 134, Loss: 3.029469331816944, Training Accuracy: 25.09%\n",
      "Epoch 135, Loss: 3.024080478931632, Training Accuracy: 24.96%\n",
      "Epoch 136, Loss: 3.0350716034774585, Training Accuracy: 25.15%\n",
      "Epoch 137, Loss: 3.0252748017420856, Training Accuracy: 25.13%\n",
      "Epoch 138, Loss: 3.0269561457207135, Training Accuracy: 25.39%\n",
      "Epoch 139, Loss: 3.0312752446250233, Training Accuracy: 25.31%\n",
      "Epoch 140, Loss: 3.029985136693091, Training Accuracy: 24.93%\n",
      "Epoch 141, Loss: 3.023819029788532, Training Accuracy: 25.38%\n",
      "Epoch 142, Loss: 3.027802123438062, Training Accuracy: 25.23%\n",
      "Epoch 143, Loss: 3.023800151732267, Training Accuracy: 25.13%\n",
      "Epoch 144, Loss: 3.0244828316256824, Training Accuracy: 25.15%\n",
      "Epoch 145, Loss: 3.0241274044031985, Training Accuracy: 25.19%\n",
      "Epoch 146, Loss: 3.026327064884898, Training Accuracy: 25.35%\n",
      "Epoch 147, Loss: 3.02840269404604, Training Accuracy: 24.95%\n",
      "Epoch 148, Loss: 3.0258471697492673, Training Accuracy: 25.25%\n",
      "Epoch 149, Loss: 3.0253283163470686, Training Accuracy: 25.06%\n",
      "Epoch 150, Loss: 3.0300277986794786, Training Accuracy: 25.10%\n",
      "Epoch 151, Loss: 3.02222440645213, Training Accuracy: 25.15%\n",
      "Epoch 152, Loss: 3.0243430643740212, Training Accuracy: 25.33%\n",
      "Epoch 153, Loss: 3.025261556096089, Training Accuracy: 25.35%\n",
      "Epoch 154, Loss: 3.0219514653505875, Training Accuracy: 25.33%\n",
      "Epoch 155, Loss: 3.0236030006042833, Training Accuracy: 25.35%\n",
      "Epoch 156, Loss: 3.0239591311920635, Training Accuracy: 25.36%\n",
      "Epoch 157, Loss: 3.0211666276692735, Training Accuracy: 25.44%\n",
      "Epoch 158, Loss: 3.0261777118038946, Training Accuracy: 25.11%\n",
      "Epoch 159, Loss: 3.0251872585252726, Training Accuracy: 25.11%\n",
      "Epoch 160, Loss: 3.0241320249064803, Training Accuracy: 25.13%\n",
      "Epoch 161, Loss: 3.0277824978084515, Training Accuracy: 25.11%\n",
      "Epoch 162, Loss: 3.028698580649198, Training Accuracy: 25.26%\n",
      "Epoch 163, Loss: 3.0262848428448144, Training Accuracy: 25.29%\n",
      "Epoch 164, Loss: 3.0251424358324015, Training Accuracy: 25.49%\n",
      "Epoch 165, Loss: 3.0276930688897057, Training Accuracy: 25.45%\n",
      "Epoch 166, Loss: 3.028322219848633, Training Accuracy: 25.42%\n",
      "Epoch 167, Loss: 3.0297447558863997, Training Accuracy: 25.33%\n",
      "Epoch 168, Loss: 3.0293018531311504, Training Accuracy: 25.19%\n",
      "Epoch 169, Loss: 3.0289683741376834, Training Accuracy: 25.30%\n",
      "Epoch 170, Loss: 3.031510780229593, Training Accuracy: 25.23%\n",
      "Epoch 171, Loss: 3.023948591688405, Training Accuracy: 25.47%\n",
      "Epoch 172, Loss: 3.0246928308321084, Training Accuracy: 25.19%\n",
      "Epoch 173, Loss: 3.0264096476537796, Training Accuracy: 25.16%\n",
      "Epoch 174, Loss: 3.022887830844011, Training Accuracy: 25.26%\n",
      "Epoch 175, Loss: 3.025431833608681, Training Accuracy: 25.39%\n",
      "Accuracy of the network on the 10000 test images: 25.41%\n",
      "Starting training for hyperparameter configuration 3/3\n",
      "Epoch 1, Loss: 3.584235273358767, Training Accuracy: 15.69%\n",
      "Epoch 2, Loss: 2.97578722467203, Training Accuracy: 26.49%\n",
      "Epoch 3, Loss: 2.717910797699638, Training Accuracy: 31.55%\n",
      "Epoch 4, Loss: 2.542881492306204, Training Accuracy: 35.06%\n",
      "Epoch 5, Loss: 2.408835997057083, Training Accuracy: 38.08%\n",
      "Epoch 6, Loss: 2.3006280027996855, Training Accuracy: 40.60%\n",
      "Epoch 7, Loss: 2.206301683812495, Training Accuracy: 42.65%\n",
      "Epoch 8, Loss: 2.1219120458568757, Training Accuracy: 44.53%\n",
      "Epoch 9, Loss: 2.046581220901226, Training Accuracy: 46.06%\n",
      "Epoch 10, Loss: 1.9743445151297332, Training Accuracy: 47.79%\n",
      "Epoch 11, Loss: 1.906678474315292, Training Accuracy: 49.28%\n",
      "Epoch 12, Loss: 1.8455666300585813, Training Accuracy: 50.78%\n",
      "Epoch 13, Loss: 1.7846243488209328, Training Accuracy: 52.21%\n",
      "Epoch 14, Loss: 1.7256310448012389, Training Accuracy: 53.75%\n",
      "Epoch 15, Loss: 1.666770374607247, Training Accuracy: 54.89%\n",
      "Epoch 16, Loss: 1.6140015611563192, Training Accuracy: 56.24%\n",
      "Epoch 17, Loss: 1.5616520098255724, Training Accuracy: 57.42%\n",
      "Epoch 18, Loss: 1.5126517881517825, Training Accuracy: 58.80%\n",
      "Epoch 19, Loss: 1.463934478247562, Training Accuracy: 59.75%\n",
      "Epoch 20, Loss: 1.4191152778885248, Training Accuracy: 60.82%\n",
      "Epoch 21, Loss: 1.3765905940014382, Training Accuracy: 61.88%\n",
      "Epoch 22, Loss: 1.331501003421481, Training Accuracy: 62.81%\n",
      "Epoch 23, Loss: 1.2915822407778572, Training Accuracy: 63.91%\n",
      "Epoch 24, Loss: 1.2506730413955192, Training Accuracy: 64.77%\n",
      "Epoch 25, Loss: 1.2160608548947307, Training Accuracy: 65.65%\n",
      "Epoch 26, Loss: 1.175287388093636, Training Accuracy: 66.87%\n",
      "Epoch 27, Loss: 1.1411219047920784, Training Accuracy: 67.61%\n",
      "Epoch 28, Loss: 1.1104466433415328, Training Accuracy: 68.34%\n",
      "Epoch 29, Loss: 1.0760262827281757, Training Accuracy: 68.99%\n",
      "Epoch 30, Loss: 1.035780449962372, Training Accuracy: 70.38%\n",
      "Epoch 31, Loss: 1.005673704778447, Training Accuracy: 71.07%\n",
      "Epoch 32, Loss: 0.982150364105049, Training Accuracy: 71.60%\n",
      "Epoch 33, Loss: 0.9475854382947888, Training Accuracy: 72.31%\n",
      "Epoch 34, Loss: 0.9223108333928506, Training Accuracy: 73.28%\n",
      "Epoch 35, Loss: 0.8999286225765867, Training Accuracy: 73.61%\n",
      "Epoch 36, Loss: 0.8734052219354284, Training Accuracy: 74.47%\n",
      "Epoch 37, Loss: 0.8519320269984663, Training Accuracy: 74.94%\n",
      "Epoch 38, Loss: 0.8210863983996993, Training Accuracy: 76.02%\n",
      "Epoch 39, Loss: 0.8021099538449437, Training Accuracy: 76.43%\n",
      "Epoch 40, Loss: 0.782224856983975, Training Accuracy: 76.92%\n",
      "Epoch 41, Loss: 0.7589424902673267, Training Accuracy: 77.44%\n",
      "Epoch 42, Loss: 0.7354540657966643, Training Accuracy: 77.95%\n",
      "Epoch 43, Loss: 0.7228099794110374, Training Accuracy: 78.17%\n",
      "Epoch 44, Loss: 0.700831387544532, Training Accuracy: 79.03%\n",
      "Epoch 45, Loss: 0.690036228398228, Training Accuracy: 79.33%\n",
      "Epoch 46, Loss: 0.6681862547422004, Training Accuracy: 79.84%\n",
      "Epoch 47, Loss: 0.6584501819080099, Training Accuracy: 80.20%\n",
      "Epoch 48, Loss: 0.635022306076401, Training Accuracy: 80.85%\n",
      "Epoch 49, Loss: 0.6237272799319928, Training Accuracy: 80.89%\n",
      "Epoch 50, Loss: 0.6216742387779838, Training Accuracy: 81.01%\n",
      "Epoch 51, Loss: 0.5915935464832179, Training Accuracy: 82.06%\n",
      "Epoch 52, Loss: 0.5848167499008081, Training Accuracy: 82.04%\n",
      "Epoch 53, Loss: 0.5818053418603699, Training Accuracy: 82.14%\n",
      "Epoch 54, Loss: 0.5639894886890335, Training Accuracy: 82.91%\n",
      "Epoch 55, Loss: 0.5486520144259533, Training Accuracy: 83.17%\n",
      "Epoch 56, Loss: 0.5457307738263893, Training Accuracy: 83.26%\n",
      "Epoch 57, Loss: 0.5403425998966712, Training Accuracy: 83.40%\n",
      "Epoch 58, Loss: 0.5198006371555426, Training Accuracy: 84.02%\n",
      "Epoch 59, Loss: 0.5155198752613324, Training Accuracy: 84.10%\n",
      "Epoch 60, Loss: 0.5107471026918468, Training Accuracy: 84.52%\n",
      "Epoch 61, Loss: 0.49827063337082755, Training Accuracy: 84.72%\n",
      "Epoch 62, Loss: 0.4939682909556667, Training Accuracy: 84.95%\n",
      "Epoch 63, Loss: 0.48596810545686564, Training Accuracy: 85.00%\n",
      "Epoch 64, Loss: 0.4785227565204396, Training Accuracy: 85.20%\n",
      "Epoch 65, Loss: 0.4619734129675514, Training Accuracy: 85.76%\n",
      "Epoch 66, Loss: 0.4672875235528897, Training Accuracy: 85.49%\n",
      "Epoch 67, Loss: 0.4570861378746569, Training Accuracy: 85.87%\n",
      "Epoch 68, Loss: 0.45516895224599885, Training Accuracy: 85.82%\n",
      "Epoch 69, Loss: 0.4426889305605608, Training Accuracy: 86.34%\n",
      "Epoch 70, Loss: 0.443515579528211, Training Accuracy: 86.42%\n",
      "Epoch 71, Loss: 0.439809761846157, Training Accuracy: 86.40%\n",
      "Epoch 72, Loss: 0.4359281541555739, Training Accuracy: 86.44%\n",
      "Epoch 73, Loss: 0.4282032993557813, Training Accuracy: 86.86%\n",
      "Epoch 74, Loss: 0.42567252262931343, Training Accuracy: 86.69%\n",
      "Epoch 75, Loss: 0.41609384967466756, Training Accuracy: 87.16%\n",
      "Epoch 76, Loss: 0.4191068442695586, Training Accuracy: 87.02%\n",
      "Epoch 77, Loss: 0.4140258358167413, Training Accuracy: 87.25%\n",
      "Epoch 78, Loss: 0.39809045014555194, Training Accuracy: 87.69%\n",
      "Epoch 79, Loss: 0.4056321560116985, Training Accuracy: 87.32%\n",
      "Epoch 80, Loss: 0.4028814935775669, Training Accuracy: 87.35%\n",
      "Epoch 81, Loss: 0.38792121719064004, Training Accuracy: 88.23%\n",
      "Epoch 82, Loss: 0.40385310909213007, Training Accuracy: 87.44%\n",
      "Epoch 83, Loss: 0.3950145464114216, Training Accuracy: 87.71%\n",
      "Epoch 84, Loss: 0.38945681361667334, Training Accuracy: 87.87%\n",
      "Epoch 85, Loss: 0.3812237715591555, Training Accuracy: 88.13%\n",
      "Epoch 86, Loss: 0.3863247391551047, Training Accuracy: 88.04%\n",
      "Epoch 87, Loss: 0.38954385693954385, Training Accuracy: 87.88%\n",
      "Epoch 88, Loss: 0.3777445444213155, Training Accuracy: 88.38%\n",
      "Epoch 89, Loss: 0.3747189457878432, Training Accuracy: 88.40%\n",
      "Epoch 90, Loss: 0.3689560875525255, Training Accuracy: 88.67%\n",
      "Epoch 91, Loss: 0.3784398381095713, Training Accuracy: 88.17%\n",
      "Epoch 92, Loss: 0.36558508683386665, Training Accuracy: 88.69%\n",
      "Epoch 93, Loss: 0.36953844187204793, Training Accuracy: 88.56%\n",
      "Epoch 94, Loss: 0.3690609478813303, Training Accuracy: 88.56%\n",
      "Epoch 95, Loss: 0.3667310480685795, Training Accuracy: 88.70%\n",
      "Epoch 96, Loss: 0.365861337746272, Training Accuracy: 88.77%\n",
      "Epoch 97, Loss: 0.3636695063480026, Training Accuracy: 88.72%\n",
      "Epoch 98, Loss: 0.3506992224632474, Training Accuracy: 89.20%\n",
      "Epoch 99, Loss: 0.3646985605797347, Training Accuracy: 88.68%\n",
      "Epoch 100, Loss: 0.359904236862879, Training Accuracy: 88.82%\n",
      "Epoch 101, Loss: 0.35048734270931814, Training Accuracy: 89.33%\n",
      "Epoch 102, Loss: 0.3558428753500857, Training Accuracy: 88.92%\n",
      "Epoch 103, Loss: 0.3344853419877227, Training Accuracy: 89.96%\n",
      "Epoch 104, Loss: 0.3522553919716869, Training Accuracy: 89.20%\n",
      "Epoch 105, Loss: 0.34840117051930686, Training Accuracy: 89.50%\n",
      "Epoch 106, Loss: 0.3457019311444991, Training Accuracy: 89.40%\n",
      "Epoch 107, Loss: 0.35394175727006116, Training Accuracy: 89.10%\n",
      "Epoch 108, Loss: 0.3351206697542649, Training Accuracy: 89.76%\n",
      "Epoch 109, Loss: 0.35048982074193635, Training Accuracy: 89.17%\n",
      "Epoch 110, Loss: 0.33756921231708564, Training Accuracy: 89.70%\n",
      "Epoch 111, Loss: 0.33521403556170365, Training Accuracy: 89.79%\n",
      "Epoch 112, Loss: 0.34094121999790905, Training Accuracy: 89.52%\n",
      "Epoch 113, Loss: 0.3385755228512275, Training Accuracy: 89.65%\n",
      "Epoch 114, Loss: 0.33925310578530704, Training Accuracy: 89.52%\n",
      "Epoch 115, Loss: 0.33121249345524234, Training Accuracy: 89.89%\n",
      "Epoch 116, Loss: 0.33077148792079036, Training Accuracy: 89.96%\n",
      "Epoch 117, Loss: 0.32902063368379003, Training Accuracy: 89.82%\n",
      "Epoch 118, Loss: 0.3332474455999596, Training Accuracy: 89.77%\n",
      "Epoch 119, Loss: 0.3362088388456103, Training Accuracy: 89.48%\n",
      "Epoch 120, Loss: 0.31718389508898, Training Accuracy: 90.40%\n",
      "Epoch 121, Loss: 0.33240819770052, Training Accuracy: 89.77%\n",
      "Epoch 122, Loss: 0.3306978635604272, Training Accuracy: 89.79%\n",
      "Epoch 123, Loss: 0.31654895355215157, Training Accuracy: 90.40%\n",
      "Epoch 124, Loss: 0.3217403996268959, Training Accuracy: 90.24%\n",
      "Epoch 125, Loss: 0.33411185357652967, Training Accuracy: 89.75%\n",
      "Epoch 126, Loss: 0.31408771708645783, Training Accuracy: 90.51%\n",
      "Epoch 127, Loss: 0.32472227125064185, Training Accuracy: 90.10%\n",
      "Epoch 128, Loss: 0.3192085523510833, Training Accuracy: 90.35%\n",
      "Epoch 129, Loss: 0.32443432872900574, Training Accuracy: 90.03%\n",
      "Epoch 130, Loss: 0.3139422770465731, Training Accuracy: 90.40%\n",
      "Epoch 131, Loss: 0.3182826966542722, Training Accuracy: 90.38%\n",
      "Epoch 132, Loss: 0.3160603852737743, Training Accuracy: 90.39%\n",
      "Epoch 133, Loss: 0.31896979733348807, Training Accuracy: 90.34%\n",
      "Epoch 134, Loss: 0.3192008750422684, Training Accuracy: 90.37%\n",
      "Epoch 135, Loss: 0.31238640944861695, Training Accuracy: 90.50%\n",
      "Epoch 136, Loss: 0.3142081940894389, Training Accuracy: 90.48%\n",
      "Epoch 137, Loss: 0.3091497694230293, Training Accuracy: 90.50%\n",
      "Epoch 138, Loss: 0.3227725842457903, Training Accuracy: 90.18%\n",
      "Epoch 139, Loss: 0.2974318983914602, Training Accuracy: 91.00%\n",
      "Epoch 140, Loss: 0.31220740305683803, Training Accuracy: 90.44%\n",
      "Epoch 141, Loss: 0.3017213097809221, Training Accuracy: 90.96%\n",
      "Epoch 142, Loss: 0.3171228283296918, Training Accuracy: 90.35%\n",
      "Epoch 143, Loss: 0.3183825601206716, Training Accuracy: 90.29%\n",
      "Epoch 144, Loss: 0.29919654410094254, Training Accuracy: 91.01%\n",
      "Epoch 145, Loss: 0.300896701052823, Training Accuracy: 91.10%\n",
      "Epoch 146, Loss: 0.3044341198166313, Training Accuracy: 90.78%\n",
      "Epoch 147, Loss: 0.30131265422915254, Training Accuracy: 90.92%\n",
      "Epoch 148, Loss: 0.30861937241328646, Training Accuracy: 90.70%\n",
      "Epoch 149, Loss: 0.31079607794199454, Training Accuracy: 90.60%\n",
      "Epoch 150, Loss: 0.3003197096173873, Training Accuracy: 90.96%\n",
      "Epoch 151, Loss: 0.3073754960008899, Training Accuracy: 90.65%\n",
      "Epoch 152, Loss: 0.31062240309803685, Training Accuracy: 90.66%\n",
      "Epoch 153, Loss: 0.3008226449494167, Training Accuracy: 90.90%\n",
      "Epoch 154, Loss: 0.28773509199395203, Training Accuracy: 91.43%\n",
      "Epoch 155, Loss: 0.3055829636781188, Training Accuracy: 90.87%\n",
      "Epoch 156, Loss: 0.30456260501233207, Training Accuracy: 90.74%\n",
      "Epoch 157, Loss: 0.29412026652861434, Training Accuracy: 91.25%\n",
      "Epoch 158, Loss: 0.3009635527020373, Training Accuracy: 90.93%\n",
      "Epoch 159, Loss: 0.29588344154874685, Training Accuracy: 91.14%\n",
      "Epoch 160, Loss: 0.3048287834638677, Training Accuracy: 90.76%\n",
      "Epoch 161, Loss: 0.29619062691926956, Training Accuracy: 91.02%\n",
      "Epoch 162, Loss: 0.2905657601345073, Training Accuracy: 91.43%\n",
      "Epoch 163, Loss: 0.29483217160072167, Training Accuracy: 91.04%\n",
      "Epoch 164, Loss: 0.2976844718827463, Training Accuracy: 91.04%\n",
      "Epoch 165, Loss: 0.29329915954481306, Training Accuracy: 91.17%\n",
      "Epoch 166, Loss: 0.2792841639855634, Training Accuracy: 91.72%\n",
      "Epoch 167, Loss: 0.304290513858161, Training Accuracy: 90.95%\n",
      "Epoch 168, Loss: 0.2984132743285745, Training Accuracy: 91.15%\n",
      "Epoch 169, Loss: 0.2854068861116686, Training Accuracy: 91.46%\n",
      "Epoch 170, Loss: 0.2947630803851063, Training Accuracy: 91.01%\n",
      "Epoch 171, Loss: 0.29629889681287436, Training Accuracy: 91.07%\n",
      "Epoch 172, Loss: 0.287976842764241, Training Accuracy: 91.48%\n",
      "Epoch 173, Loss: 0.2907826693066398, Training Accuracy: 91.26%\n",
      "Epoch 174, Loss: 0.2858869593180811, Training Accuracy: 91.53%\n",
      "Epoch 175, Loss: 0.2871130227166064, Training Accuracy: 91.40%\n",
      "Accuracy of the network on the 10000 test images: 31.85%\n",
      "All Training Accuracies over Epochs for each config: [[18.124, 31.87, 37.94, 42.05, 45.402, 48.436, 51.494, 54.258, 56.626, 59.074, 61.338, 63.98, 65.99, 68.398, 70.85, 72.884, 74.982, 77.316, 79.122, 81.048, 83.0, 84.642, 86.204, 87.368, 88.72, 89.854, 90.392, 91.582, 92.05, 93.002, 93.242, 93.784, 94.072, 94.296, 94.912, 94.546, 95.23, 95.066, 95.188, 95.736, 95.916, 95.284, 95.868, 96.244, 95.706, 95.864, 96.38, 96.312, 96.06, 96.614, 96.034, 96.596, 96.92, 96.276, 96.48, 96.772, 96.382, 96.888, 96.822, 96.862, 96.476, 97.43, 96.546, 97.016, 97.292, 97.206, 97.012, 96.834, 97.436, 97.014, 97.416, 97.048, 97.478, 96.996, 97.092, 97.8, 97.432, 97.09, 97.546, 97.442, 97.208, 97.39, 97.58, 97.306, 97.596, 97.744, 97.532, 97.682, 97.556, 97.612, 97.502, 97.8, 97.484, 97.688, 97.452, 97.906, 97.828, 97.486, 97.888, 97.69, 97.554, 97.884, 97.906, 97.788, 97.684, 97.756, 97.756, 98.106, 97.8, 97.896, 98.178, 97.734, 97.788, 97.664, 98.61, 97.816, 97.94, 97.892, 97.938, 98.056, 98.062, 97.972, 98.166, 97.696, 98.054, 98.216, 98.056, 97.796, 98.428, 98.056, 97.96, 98.062, 98.226, 98.0, 98.318, 98.274, 97.986, 98.074, 98.402, 97.976, 98.224, 98.326, 98.172, 98.272, 98.444, 98.144, 98.21, 98.29, 98.178, 98.254, 98.338, 98.286, 98.178, 98.068, 98.372, 98.43, 98.224, 98.358, 98.358, 98.15, 98.444, 98.462, 98.236, 98.456, 98.318, 98.446, 98.346, 98.52, 98.174, 98.326, 98.544, 98.348, 98.456, 98.374, 98.486], [8.904, 14.61, 17.95, 19.464, 20.428, 21.02, 21.12, 21.612, 21.602, 21.888, 22.296, 22.226, 22.326, 22.566, 22.676, 22.918, 22.914, 22.984, 23.17, 23.344, 23.436, 23.358, 23.328, 23.7, 23.69, 23.788, 23.52, 23.622, 23.634, 24.152, 23.95, 23.878, 24.024, 24.02, 24.168, 23.978, 24.37, 24.158, 24.208, 24.212, 24.224, 24.016, 24.278, 24.23, 24.126, 24.288, 24.292, 24.198, 24.088, 24.36, 24.158, 24.372, 24.33, 24.228, 24.274, 24.474, 24.35, 24.294, 24.598, 24.368, 24.274, 24.546, 24.42, 24.464, 24.506, 24.16, 24.324, 24.368, 24.302, 24.694, 24.44, 24.508, 24.382, 24.526, 24.454, 24.6, 24.458, 24.518, 24.524, 24.344, 24.492, 24.538, 24.436, 24.598, 24.604, 24.816, 24.944, 24.956, 24.866, 24.716, 24.848, 24.824, 24.784, 24.802, 24.852, 24.944, 24.892, 24.844, 25.012, 24.928, 24.752, 25.026, 24.954, 24.876, 24.978, 25.032, 24.944, 25.112, 25.126, 25.008, 24.992, 25.092, 25.184, 25.014, 25.324, 25.11, 25.028, 24.914, 25.088, 25.092, 25.216, 25.14, 25.044, 25.17, 25.266, 25.242, 25.202, 25.022, 25.256, 25.016, 25.192, 25.172, 24.876, 25.092, 24.958, 25.15, 25.128, 25.392, 25.306, 24.926, 25.376, 25.226, 25.128, 25.148, 25.192, 25.348, 24.95, 25.25, 25.062, 25.104, 25.146, 25.326, 25.354, 25.332, 25.35, 25.358, 25.44, 25.106, 25.108, 25.126, 25.11, 25.258, 25.294, 25.494, 25.45, 25.42, 25.334, 25.188, 25.296, 25.234, 25.47, 25.186, 25.158, 25.256, 25.392], [15.688, 26.49, 31.554, 35.06, 38.082, 40.598, 42.654, 44.534, 46.06, 47.788, 49.282, 50.776, 52.21, 53.748, 54.892, 56.238, 57.424, 58.798, 59.746, 60.82, 61.876, 62.814, 63.908, 64.774, 65.65, 66.87, 67.614, 68.344, 68.994, 70.382, 71.072, 71.6, 72.308, 73.276, 73.614, 74.47, 74.936, 76.018, 76.426, 76.918, 77.44, 77.954, 78.172, 79.032, 79.33, 79.838, 80.202, 80.85, 80.894, 81.008, 82.06, 82.04, 82.14, 82.91, 83.168, 83.258, 83.396, 84.016, 84.104, 84.522, 84.718, 84.946, 84.996, 85.204, 85.758, 85.486, 85.87, 85.818, 86.338, 86.42, 86.398, 86.436, 86.86, 86.69, 87.156, 87.024, 87.248, 87.694, 87.322, 87.348, 88.23, 87.436, 87.708, 87.866, 88.128, 88.04, 87.882, 88.384, 88.398, 88.668, 88.174, 88.692, 88.564, 88.56, 88.698, 88.772, 88.724, 89.198, 88.684, 88.818, 89.332, 88.92, 89.962, 89.202, 89.496, 89.396, 89.098, 89.762, 89.174, 89.704, 89.79, 89.52, 89.652, 89.524, 89.888, 89.956, 89.816, 89.768, 89.484, 90.402, 89.774, 89.794, 90.402, 90.24, 89.75, 90.508, 90.098, 90.346, 90.026, 90.396, 90.376, 90.388, 90.34, 90.368, 90.504, 90.484, 90.5, 90.18, 90.996, 90.438, 90.96, 90.352, 90.294, 91.014, 91.1, 90.782, 90.922, 90.7, 90.602, 90.962, 90.65, 90.664, 90.898, 91.432, 90.872, 90.738, 91.248, 90.926, 91.14, 90.758, 91.016, 91.434, 91.042, 91.036, 91.172, 91.722, 90.954, 91.152, 91.464, 91.008, 91.07, 91.482, 91.26, 91.53, 91.402]]\n",
      "All Test Accuracies after each config: [[32.29], [25.41], [31.85]]\n",
      "All Losses over Epochs for each config: [[3.4500110987812054, 2.711748654580177, 2.41805437306309, 2.2263208678006516, 2.074692116369067, 1.9364961272920185, 1.809774511458014, 1.7002364009847422, 1.5939034759388555, 1.4940031250876844, 1.3997029525697078, 1.2991076842750735, 1.20836092032435, 1.1157957218644563, 1.0259450510944552, 0.9401349472572736, 0.8653999150866438, 0.7851844836988717, 0.7148718925388268, 0.6486775576687225, 0.5778335401850283, 0.5250661288723921, 0.4692007320387589, 0.4221508759824211, 0.37605389795454264, 0.33668027837258163, 0.31178866882267814, 0.27479713085248036, 0.2525738194165632, 0.22645761002131434, 0.21669856210232086, 0.2011292957607895, 0.18844899920570424, 0.17988398485362073, 0.16310764623858282, 0.16669873431172516, 0.15050510885884694, 0.1513517695571989, 0.14788702886332483, 0.13110892253134715, 0.12658913514655454, 0.1438711757111885, 0.12787814280661322, 0.11535456680866611, 0.13095015373445398, 0.12379527643508732, 0.10949007988266667, 0.11530357358805701, 0.11983970316874859, 0.10040146323835568, 0.11726861161982541, 0.10419059625905379, 0.09400912847660502, 0.11379778946695082, 0.1070016092904236, 0.09808674531267084, 0.10767146230310849, 0.09558874235578986, 0.10042368938974902, 0.09824831380158701, 0.10867827136993713, 0.07880690294172606, 0.10451257546507346, 0.09240645608957619, 0.08153055365229517, 0.0860669371002661, 0.0928090923427678, 0.09657245288576807, 0.08095156316689509, 0.08920410103084224, 0.07787632345117133, 0.09375408297235532, 0.07811671494752825, 0.09422590898747658, 0.09045012182880031, 0.06847609460736147, 0.07952951764047166, 0.09194325938906588, 0.07634625070558056, 0.07771065662813889, 0.08733829473490086, 0.08444369732654747, 0.07580656781965328, 0.08473654270477002, 0.07631968076699926, 0.07212699738347812, 0.07797349890758928, 0.0724889759639459, 0.07571750796993104, 0.07296125211736873, 0.08045549934391705, 0.07068265132475621, 0.08034553669025059, 0.07205242359096094, 0.08286480874027413, 0.06638193929903186, 0.0643391179090873, 0.0777059785163928, 0.0652821872763269, 0.07384501609613624, 0.07736107535497107, 0.07060605226083995, 0.06760102786596262, 0.06943144099733285, 0.07343866063065736, 0.07125922344165588, 0.07224443615705985, 0.06024497442701927, 0.07010195629529493, 0.06743211296099402, 0.05854546807124577, 0.07322587671524743, 0.07311094021546724, 0.07484273940605847, 0.04374600173723872, 0.07115584521090237, 0.06939252928137402, 0.0702877841013617, 0.07020692849490683, 0.062071408937334455, 0.06170515652159122, 0.06533247686572262, 0.05766471339788055, 0.0793245265569539, 0.062016598206139084, 0.05795893780349711, 0.06390915023821854, 0.07135258355397371, 0.04997039415863985, 0.06326328440780139, 0.06841973027647913, 0.06369205485587097, 0.05843126423241001, 0.06750600499933457, 0.05445108029504885, 0.054301636496736236, 0.06705010015985814, 0.06624731192181213, 0.05290951303013897, 0.06733824580062545, 0.057737578447420466, 0.05371129224733378, 0.06452266815427915, 0.05747563929805119, 0.051335873298884116, 0.06160632459595061, 0.05956802390164623, 0.05755310116588157, 0.06089104581071966, 0.05930385529417205, 0.0567658048678141, 0.05544378107644843, 0.05917172565275212, 0.06635639213343851, 0.05335611098835542, 0.054635999368807676, 0.06128040227268333, 0.05565197307709813, 0.05604036399637453, 0.06604210231266722, 0.05166993736431016, 0.05568508341003296, 0.059965877671070804, 0.05296600043950135, 0.06149267819494961, 0.05452738235846919, 0.05729708284840209, 0.05355849587851848, 0.06183782399670854, 0.058423382533069425, 0.0489267323162615, 0.058998684674613854, 0.05452145763236033, 0.05712833916376163, 0.0495852477962005], [3.9544032336500905, 3.592073708239114, 3.406735704378094, 3.324195061803169, 3.278236992828681, 3.252003452662007, 3.234457494962551, 3.220188661304581, 3.2127716571778593, 3.2033538400669537, 3.197617814059148, 3.1869398078040394, 3.176161022137498, 3.175058029191878, 3.17155654564538, 3.1637385361029975, 3.1574318152864267, 3.150595262532344, 3.144041812023543, 3.1419828200279296, 3.1371260777763696, 3.1274014418692233, 3.127639231779386, 3.119302550850012, 3.116532338549719, 3.1089032162790713, 3.112124736656618, 3.111055189996119, 3.1083300939911163, 3.1073443917057397, 3.0999167834401438, 3.1031858814341944, 3.095969642519646, 3.095927031753618, 3.0933337498198994, 3.095472872104791, 3.0902134774591, 3.0903882239480764, 3.09135274021217, 3.090069909229913, 3.0894424406158953, 3.0877819643606004, 3.087847692277425, 3.0848385586458096, 3.087965305808865, 3.083082170742552, 3.084057702737696, 3.08119082085007, 3.081988671246697, 3.082829106799172, 3.0866757613008895, 3.0785706945697364, 3.0803617306072693, 3.0785953358311176, 3.081813707680958, 3.076381019009349, 3.0756631403627908, 3.0818386837039764, 3.074116052873909, 3.0802337226965237, 3.0795072353709383, 3.0799550278412413, 3.0745119679614406, 3.072836458225689, 3.071885958047169, 3.0761778787578766, 3.0681299526063377, 3.074958463763947, 3.0689239733664277, 3.0692064661504057, 3.0700686910878057, 3.0683924871332504, 3.0704831558725107, 3.0693490535706816, 3.067487844115938, 3.0642338902749064, 3.0651097614746874, 3.066094806432114, 3.066329125553141, 3.062846312742404, 3.0610757919833484, 3.0599365020956832, 3.0566955338353696, 3.0547438012364574, 3.056688405058878, 3.0559256823776324, 3.0505192798116934, 3.0501816919087754, 3.050180225725979, 3.050014567192253, 3.046514959286546, 3.0452246327534356, 3.04903326772363, 3.0493854098307813, 3.04431211277652, 3.0430839375766645, 3.041027379157903, 3.0447609857525055, 3.0397703181142393, 3.042303918572643, 3.0446673177392283, 3.0388421283658507, 3.034470920062736, 3.044628622281887, 3.039363373880801, 3.0358421439709873, 3.0390679156383893, 3.04015834069313, 3.0338375830589355, 3.0317452963050977, 3.037990418236579, 3.0325242259618266, 3.031912936274048, 3.0319888414934164, 3.0345161485550043, 3.0349634074798937, 3.0369381523498182, 3.0309731801757422, 3.032906621618344, 3.0309174091309843, 3.026779211695542, 3.02825609985215, 3.0277063654511784, 3.031095778850643, 3.030811664698374, 3.028627764233543, 3.024922288287326, 3.032453616561792, 3.0292264406028613, 3.029310716997327, 3.028830700823108, 3.030268408758256, 3.0272202135047035, 3.029469331816944, 3.024080478931632, 3.0350716034774585, 3.0252748017420856, 3.0269561457207135, 3.0312752446250233, 3.029985136693091, 3.023819029788532, 3.027802123438062, 3.023800151732267, 3.0244828316256824, 3.0241274044031985, 3.026327064884898, 3.02840269404604, 3.0258471697492673, 3.0253283163470686, 3.0300277986794786, 3.02222440645213, 3.0243430643740212, 3.025261556096089, 3.0219514653505875, 3.0236030006042833, 3.0239591311920635, 3.0211666276692735, 3.0261777118038946, 3.0251872585252726, 3.0241320249064803, 3.0277824978084515, 3.028698580649198, 3.0262848428448144, 3.0251424358324015, 3.0276930688897057, 3.028322219848633, 3.0297447558863997, 3.0293018531311504, 3.0289683741376834, 3.031510780229593, 3.023948591688405, 3.0246928308321084, 3.0264096476537796, 3.022887830844011, 3.025431833608681], [3.584235273358767, 2.97578722467203, 2.717910797699638, 2.542881492306204, 2.408835997057083, 2.3006280027996855, 2.206301683812495, 2.1219120458568757, 2.046581220901226, 1.9743445151297332, 1.906678474315292, 1.8455666300585813, 1.7846243488209328, 1.7256310448012389, 1.666770374607247, 1.6140015611563192, 1.5616520098255724, 1.5126517881517825, 1.463934478247562, 1.4191152778885248, 1.3765905940014382, 1.331501003421481, 1.2915822407778572, 1.2506730413955192, 1.2160608548947307, 1.175287388093636, 1.1411219047920784, 1.1104466433415328, 1.0760262827281757, 1.035780449962372, 1.005673704778447, 0.982150364105049, 0.9475854382947888, 0.9223108333928506, 0.8999286225765867, 0.8734052219354284, 0.8519320269984663, 0.8210863983996993, 0.8021099538449437, 0.782224856983975, 0.7589424902673267, 0.7354540657966643, 0.7228099794110374, 0.700831387544532, 0.690036228398228, 0.6681862547422004, 0.6584501819080099, 0.635022306076401, 0.6237272799319928, 0.6216742387779838, 0.5915935464832179, 0.5848167499008081, 0.5818053418603699, 0.5639894886890335, 0.5486520144259533, 0.5457307738263893, 0.5403425998966712, 0.5198006371555426, 0.5155198752613324, 0.5107471026918468, 0.49827063337082755, 0.4939682909556667, 0.48596810545686564, 0.4785227565204396, 0.4619734129675514, 0.4672875235528897, 0.4570861378746569, 0.45516895224599885, 0.4426889305605608, 0.443515579528211, 0.439809761846157, 0.4359281541555739, 0.4282032993557813, 0.42567252262931343, 0.41609384967466756, 0.4191068442695586, 0.4140258358167413, 0.39809045014555194, 0.4056321560116985, 0.4028814935775669, 0.38792121719064004, 0.40385310909213007, 0.3950145464114216, 0.38945681361667334, 0.3812237715591555, 0.3863247391551047, 0.38954385693954385, 0.3777445444213155, 0.3747189457878432, 0.3689560875525255, 0.3784398381095713, 0.36558508683386665, 0.36953844187204793, 0.3690609478813303, 0.3667310480685795, 0.365861337746272, 0.3636695063480026, 0.3506992224632474, 0.3646985605797347, 0.359904236862879, 0.35048734270931814, 0.3558428753500857, 0.3344853419877227, 0.3522553919716869, 0.34840117051930686, 0.3457019311444991, 0.35394175727006116, 0.3351206697542649, 0.35048982074193635, 0.33756921231708564, 0.33521403556170365, 0.34094121999790905, 0.3385755228512275, 0.33925310578530704, 0.33121249345524234, 0.33077148792079036, 0.32902063368379003, 0.3332474455999596, 0.3362088388456103, 0.31718389508898, 0.33240819770052, 0.3306978635604272, 0.31654895355215157, 0.3217403996268959, 0.33411185357652967, 0.31408771708645783, 0.32472227125064185, 0.3192085523510833, 0.32443432872900574, 0.3139422770465731, 0.3182826966542722, 0.3160603852737743, 0.31896979733348807, 0.3192008750422684, 0.31238640944861695, 0.3142081940894389, 0.3091497694230293, 0.3227725842457903, 0.2974318983914602, 0.31220740305683803, 0.3017213097809221, 0.3171228283296918, 0.3183825601206716, 0.29919654410094254, 0.300896701052823, 0.3044341198166313, 0.30131265422915254, 0.30861937241328646, 0.31079607794199454, 0.3003197096173873, 0.3073754960008899, 0.31062240309803685, 0.3008226449494167, 0.28773509199395203, 0.3055829636781188, 0.30456260501233207, 0.29412026652861434, 0.3009635527020373, 0.29588344154874685, 0.3048287834638677, 0.29619062691926956, 0.2905657601345073, 0.29483217160072167, 0.2976844718827463, 0.29329915954481306, 0.2792841639855634, 0.304290513858161, 0.2984132743285745, 0.2854068861116686, 0.2947630803851063, 0.29629889681287436, 0.287976842764241, 0.2907826693066398, 0.2858869593180811, 0.2871130227166064]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 100)  \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.CIFAR100(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR100(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "def save_checkpoint(config, state, filename='checkpoint.pth.tar'):\n",
    "    os.makedirs(f'checkpoints/config_{config}', exist_ok=True)\n",
    "    filepath = os.path.join(f'checkpoints/config_{config}', filename)\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "def load_checkpoint(config, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(f'checkpoints/config_{config}', filename)\n",
    "    if os.path.exists(filepath):\n",
    "        return torch.load(filepath)\n",
    "    return None\n",
    "\n",
    "num_epochs = 175\n",
    "\n",
    "\n",
    "hyperparams = [\n",
    "    {'lr': 1.0, 'betas': (0.9, 0.999), 'weight_decay': 0, 'd0': 1e-6},\n",
    "    {'lr': 0.5, 'betas': (0.9, 0.999), 'weight_decay': 0.01, 'd0': 1e-5},\n",
    "    {'lr': 1.0, 'betas': (0.85, 0.999), 'weight_decay': 0.001, 'd0': 1e-7}\n",
    "]\n",
    "\n",
    "\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "for config_id, hparams in enumerate(hyperparams):\n",
    "    print(f\"Starting training for hyperparameter configuration {config_id + 1}/{len(hyperparams)}\")\n",
    "\n",
    "    model = SimpleCNN()\n",
    "    optimizer = DAdaptAdam(\n",
    "        model.parameters(),\n",
    "        lr=hparams['lr'],\n",
    "        betas=hparams['betas'],\n",
    "        weight_decay=hparams['weight_decay'],\n",
    "        d0=hparams['d0']\n",
    "    )\n",
    "\n",
    "   \n",
    "    start_epoch = 0\n",
    "    checkpoint = load_checkpoint(config_id)\n",
    "    if checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        train_accuracies = checkpoint['train_accuracies']\n",
    "        losses = checkpoint['losses']\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    else:\n",
    "        train_accuracies = []\n",
    "        losses = []\n",
    "\n",
    " \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "           \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        losses.append(running_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "     \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            save_checkpoint(config_id, {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'losses': losses,\n",
    "            })\n",
    "\n",
    "   \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    all_test_accuracies.append([test_accuracy]) \n",
    "    print(f'Accuracy of the network on the 10000 test images: {test_accuracy:.2f}%')\n",
    "\n",
    "    all_train_accuracies.append(train_accuracies)\n",
    "    all_losses.append(losses)\n",
    "\n",
    "    \n",
    "    save_checkpoint(config_id, {\n",
    "        'epoch': num_epochs - 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'losses': losses,\n",
    "        'test_accuracy': test_accuracy\n",
    "    })\n",
    "\n",
    "all_train_accuracies = [list(train_accuracies) for train_accuracies in all_train_accuracies]\n",
    "all_test_accuracies = [list(test_accuracies) for test_accuracies in all_test_accuracies]\n",
    "all_losses = [list(losses) for losses in all_losses]\n",
    "\n",
    "print(\"All Training Accuracies over Epochs for each config:\", all_train_accuracies)\n",
    "print(\"All Test Accuracies after each config:\", all_test_accuracies)\n",
    "print(\"All Losses over Epochs for each config:\", all_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28836.256615,
   "end_time": "2024-10-23T23:51:40.175944",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T15:51:03.919329",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
