{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9a47a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T05:16:42.422053Z",
     "iopub.status.busy": "2024-08-28T05:16:42.421339Z",
     "iopub.status.idle": "2024-08-28T05:16:45.680282Z",
     "shell.execute_reply": "2024-08-28T05:16:45.679481Z"
    },
    "papermill": {
     "duration": 3.265596,
     "end_time": "2024-08-28T05:16:45.682506",
     "exception": false,
     "start_time": "2024-08-28T05:16:42.416910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import TYPE_CHECKING, Any, Callable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import pdb\n",
    "import logging\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from torch.optim.optimizer import _params_t\n",
    "else:\n",
    "    _params_t = Any\n",
    "\n",
    "class DAdaptAdam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1.0,\n",
    "                betas=(0.9, 0.999), eps=1e-8,\n",
    "                weight_decay=0, log_every=0,\n",
    "                decouple=False,\n",
    "                use_bias_correction=False,\n",
    "                d0=1e-6, growth_rate=float('inf'),\n",
    "                fsdp_in_use=False):\n",
    "        if not 0.0 < d0:\n",
    "            raise ValueError(\"Invalid d0 value: {}\".format(d0))\n",
    "        if not 0.0 < lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 < eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        if decouple:\n",
    "            print(f\"Using decoupled weight decay\")\n",
    "\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        d = d0,\n",
    "                        k=0,\n",
    "                        layer_scale=1.0,\n",
    "                        numerator_weighted=0.0,\n",
    "                        log_every=log_every,\n",
    "                        growth_rate=growth_rate,\n",
    "                        use_bias_correction=use_bias_correction,\n",
    "                        decouple=decouple,\n",
    "                        fsdp_in_use=fsdp_in_use)\n",
    "        self.d0 = d0\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @property\n",
    "    def supports_memory_efficient_fp16(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def supports_flat_params(self):\n",
    "        return True\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        sk_l1 = 0.0\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        use_bias_correction = group['use_bias_correction']\n",
    "        numerator_weighted = group['numerator_weighted']\n",
    "        beta1, beta2 = group['betas']\n",
    "        k = group['k']\n",
    "\n",
    "        d = group['d']\n",
    "        lr = max(group['lr'] for group in self.param_groups)\n",
    "\n",
    "        if use_bias_correction:\n",
    "            bias_correction = ((1-beta2**(k+1))**0.5)/(1-beta1**(k+1))\n",
    "        else:\n",
    "            bias_correction = 1\n",
    "\n",
    "        dlr = d*lr*bias_correction\n",
    "\n",
    "        growth_rate = group['growth_rate']\n",
    "        decouple = group['decouple']\n",
    "        log_every = group['log_every']\n",
    "        fsdp_in_use = group['fsdp_in_use']\n",
    "\n",
    "\n",
    "        sqrt_beta2 = beta2**(0.5)\n",
    "\n",
    "        numerator_acum = 0.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            decay = group['weight_decay']\n",
    "            k = group['k']\n",
    "            eps = group['eps']\n",
    "            group_lr = group['lr']\n",
    "            r = group['layer_scale']\n",
    "\n",
    "            if group_lr not in [lr, 0.0]:\n",
    "                raise RuntimeError(f\"Setting different lr values in different parameter groups \"\n",
    "                                \"is only supported for values of 0. To scale the learning \"\n",
    "                                \"rate differently for each layer, set the 'layer_scale' value instead.\")\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if hasattr(p, \"_fsdp_flattened\"):\n",
    "                    fsdp_in_use = True\n",
    "\n",
    "                grad = p.grad.data\n",
    "\n",
    "                if decay != 0 and not decouple:\n",
    "                    grad.add_(p.data, alpha=decay)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if 'step' not in state:\n",
    "                    state['step'] = 0\n",
    "                    state['s'] = torch.zeros_like(p.data).detach()\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data).detach()\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data).detach()\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                s = state['s']\n",
    "\n",
    "                if group_lr > 0.0:\n",
    "                    denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                    numerator_acum += r * dlr * torch.dot(grad.flatten(), s.div(denom).flatten()).item()\n",
    "\n",
    "                    exp_avg.mul_(beta1).add_(grad, alpha=r*dlr*(1-beta1))\n",
    "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "\n",
    "                    s.mul_(sqrt_beta2).add_(grad, alpha=dlr*(1-sqrt_beta2))\n",
    "                    sk_l1 += r * s.abs().sum().item()\n",
    "\n",
    "        numerator_weighted = sqrt_beta2*numerator_weighted + (1-sqrt_beta2)*numerator_acum\n",
    "        d_hat = d\n",
    "        if sk_l1 == 0:\n",
    "            return loss\n",
    "\n",
    "        if lr > 0.0:\n",
    "            if fsdp_in_use:\n",
    "                dist_tensor = torch.zeros(2).cuda()\n",
    "                dist_tensor[0] = numerator_weighted\n",
    "                dist_tensor[1] = sk_l1\n",
    "                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)\n",
    "                global_numerator_weighted = dist_tensor[0]\n",
    "                global_sk_l1 = dist_tensor[1]\n",
    "            else:\n",
    "                global_numerator_weighted = numerator_weighted\n",
    "                global_sk_l1 = sk_l1\n",
    "\n",
    "\n",
    "            d_hat = global_numerator_weighted/((1-sqrt_beta2)*global_sk_l1)\n",
    "            d = max(d, min(d_hat, d*growth_rate))\n",
    "\n",
    "        if log_every > 0 and k % log_every == 0:\n",
    "            logging.info(f\"lr: {lr} dlr: {dlr} d_hat: {d_hat}, d: {d}. sk_l1={global_sk_l1:1.1e} numerator_weighted={global_numerator_weighted:1.1e}\")\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            group['numerator_weighted'] = numerator_weighted\n",
    "            group['d'] = d\n",
    "\n",
    "            decay = group['weight_decay']\n",
    "            k = group['k']\n",
    "            eps = group['eps']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "\n",
    "                if decay != 0 and decouple:\n",
    "                    p.data.add_(p.data, alpha=-decay * dlr)\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-1)\n",
    "\n",
    "            group['k'] = k + 1\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e3ce0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T05:16:45.687930Z",
     "iopub.status.busy": "2024-08-28T05:16:45.687509Z",
     "iopub.status.idle": "2024-08-28T12:53:24.383880Z",
     "shell.execute_reply": "2024-08-28T12:53:24.382870Z"
    },
    "papermill": {
     "duration": 27398.703102,
     "end_time": "2024-08-28T12:53:24.387798",
     "exception": false,
     "start_time": "2024-08-28T05:16:45.684696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:04<00:00, 36545698.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n",
      "Starting run 1/5\n",
      "Epoch 1, Loss: 1.3494219994148635, Training Accuracy: 51.17%\n",
      "Epoch 2, Loss: 0.9814208072926992, Training Accuracy: 65.30%\n",
      "Epoch 3, Loss: 0.8356298074088133, Training Accuracy: 70.54%\n",
      "Epoch 4, Loss: 0.7299919880717002, Training Accuracy: 74.15%\n",
      "Epoch 5, Loss: 0.63582071055994, Training Accuracy: 77.66%\n",
      "Epoch 6, Loss: 0.5552120241134063, Training Accuracy: 80.50%\n",
      "Epoch 7, Loss: 0.4778876421815904, Training Accuracy: 83.20%\n",
      "Epoch 8, Loss: 0.41430928741041045, Training Accuracy: 85.45%\n",
      "Epoch 9, Loss: 0.34762262654921894, Training Accuracy: 87.76%\n",
      "Epoch 10, Loss: 0.29292867616619295, Training Accuracy: 89.71%\n",
      "Epoch 11, Loss: 0.24321787699561595, Training Accuracy: 91.55%\n",
      "Epoch 12, Loss: 0.21076269257727945, Training Accuracy: 92.54%\n",
      "Epoch 13, Loss: 0.18039121651007314, Training Accuracy: 93.75%\n",
      "Epoch 14, Loss: 0.15494676549797473, Training Accuracy: 94.65%\n",
      "Epoch 15, Loss: 0.13247217021434737, Training Accuracy: 95.31%\n",
      "Epoch 16, Loss: 0.12202590903567384, Training Accuracy: 95.80%\n",
      "Epoch 17, Loss: 0.1198809984779638, Training Accuracy: 95.83%\n",
      "Epoch 18, Loss: 0.09875401328472644, Training Accuracy: 96.61%\n",
      "Epoch 19, Loss: 0.10753591237422984, Training Accuracy: 96.23%\n",
      "Epoch 20, Loss: 0.08315114609306426, Training Accuracy: 97.16%\n",
      "Epoch 21, Loss: 0.09783185840747498, Training Accuracy: 96.63%\n",
      "Epoch 22, Loss: 0.08821104312151232, Training Accuracy: 96.85%\n",
      "Epoch 23, Loss: 0.08245842212327587, Training Accuracy: 97.13%\n",
      "Epoch 24, Loss: 0.07404946474372731, Training Accuracy: 97.49%\n",
      "Epoch 25, Loss: 0.07822781097253456, Training Accuracy: 97.41%\n",
      "Epoch 26, Loss: 0.07994936078982166, Training Accuracy: 97.32%\n",
      "Epoch 27, Loss: 0.07658994300207576, Training Accuracy: 97.43%\n",
      "Epoch 28, Loss: 0.07201124009881121, Training Accuracy: 97.58%\n",
      "Epoch 29, Loss: 0.07617712487785808, Training Accuracy: 97.43%\n",
      "Epoch 30, Loss: 0.06500426902229686, Training Accuracy: 97.86%\n",
      "Epoch 31, Loss: 0.06862043977990184, Training Accuracy: 97.62%\n",
      "Epoch 32, Loss: 0.07154680112057635, Training Accuracy: 97.62%\n",
      "Epoch 33, Loss: 0.05802399053594426, Training Accuracy: 98.02%\n",
      "Epoch 34, Loss: 0.07347977895091962, Training Accuracy: 97.66%\n",
      "Epoch 35, Loss: 0.06673880804108594, Training Accuracy: 97.88%\n",
      "Epoch 36, Loss: 0.053280079646198, Training Accuracy: 98.21%\n",
      "Epoch 37, Loss: 0.0630197105713396, Training Accuracy: 97.91%\n",
      "Epoch 38, Loss: 0.07292321335394507, Training Accuracy: 97.75%\n",
      "Epoch 39, Loss: 0.06310431856630846, Training Accuracy: 97.93%\n",
      "Epoch 40, Loss: 0.055643964284623655, Training Accuracy: 98.16%\n",
      "Epoch 41, Loss: 0.07402469538276672, Training Accuracy: 97.69%\n",
      "Epoch 42, Loss: 0.0495981971043154, Training Accuracy: 98.40%\n",
      "Epoch 43, Loss: 0.05888521254954078, Training Accuracy: 98.19%\n",
      "Epoch 44, Loss: 0.06940366407794415, Training Accuracy: 97.83%\n",
      "Epoch 45, Loss: 0.04867410723185774, Training Accuracy: 98.46%\n",
      "Epoch 46, Loss: 0.06341840127558784, Training Accuracy: 98.00%\n",
      "Epoch 47, Loss: 0.050416885934564074, Training Accuracy: 98.38%\n",
      "Epoch 48, Loss: 0.05312141296926804, Training Accuracy: 98.33%\n",
      "Epoch 49, Loss: 0.05580737756639201, Training Accuracy: 98.30%\n",
      "Epoch 50, Loss: 0.060791817370529286, Training Accuracy: 98.13%\n",
      "Epoch 51, Loss: 0.054809361604223825, Training Accuracy: 98.31%\n",
      "Epoch 52, Loss: 0.045900860011441835, Training Accuracy: 98.51%\n",
      "Epoch 53, Loss: 0.06751536052708891, Training Accuracy: 97.95%\n",
      "Epoch 54, Loss: 0.05123775020071878, Training Accuracy: 98.48%\n",
      "Epoch 55, Loss: 0.05433858845372062, Training Accuracy: 98.39%\n",
      "Epoch 56, Loss: 0.055119402584960134, Training Accuracy: 98.25%\n",
      "Epoch 57, Loss: 0.054167409221332036, Training Accuracy: 98.37%\n",
      "Epoch 58, Loss: 0.04501873624921271, Training Accuracy: 98.62%\n",
      "Epoch 59, Loss: 0.06143221241915348, Training Accuracy: 98.20%\n",
      "Epoch 60, Loss: 0.04901857899380666, Training Accuracy: 98.51%\n",
      "Epoch 61, Loss: 0.056822762858578506, Training Accuracy: 98.36%\n",
      "Epoch 62, Loss: 0.05124758591782875, Training Accuracy: 98.51%\n",
      "Epoch 63, Loss: 0.05191148485548992, Training Accuracy: 98.40%\n",
      "Epoch 64, Loss: 0.041650983319832774, Training Accuracy: 98.70%\n",
      "Epoch 65, Loss: 0.05077781228107743, Training Accuracy: 98.56%\n",
      "Epoch 66, Loss: 0.05294440287796957, Training Accuracy: 98.47%\n",
      "Epoch 67, Loss: 0.05372902190807424, Training Accuracy: 98.41%\n",
      "Epoch 68, Loss: 0.052165133042877024, Training Accuracy: 98.56%\n",
      "Epoch 69, Loss: 0.04629737349334573, Training Accuracy: 98.62%\n",
      "Epoch 70, Loss: 0.03654165578396235, Training Accuracy: 98.88%\n",
      "Epoch 71, Loss: 0.04924253311936114, Training Accuracy: 98.56%\n",
      "Epoch 72, Loss: 0.05517445135378659, Training Accuracy: 98.48%\n",
      "Epoch 73, Loss: 0.056883226097924566, Training Accuracy: 98.41%\n",
      "Epoch 74, Loss: 0.04834863272042784, Training Accuracy: 98.58%\n",
      "Epoch 75, Loss: 0.04582109974942754, Training Accuracy: 98.66%\n",
      "Epoch 76, Loss: 0.048549537841221686, Training Accuracy: 98.74%\n",
      "Epoch 77, Loss: 0.051910899737802656, Training Accuracy: 98.61%\n",
      "Epoch 78, Loss: 0.03785377249165866, Training Accuracy: 98.93%\n",
      "Epoch 79, Loss: 0.054603333208577104, Training Accuracy: 98.50%\n",
      "Epoch 80, Loss: 0.049816382440391216, Training Accuracy: 98.68%\n",
      "Epoch 81, Loss: 0.04403444061154237, Training Accuracy: 98.75%\n",
      "Epoch 82, Loss: 0.04504884399036533, Training Accuracy: 98.70%\n",
      "Epoch 83, Loss: 0.042707194856274146, Training Accuracy: 98.88%\n",
      "Epoch 84, Loss: 0.05077088317994307, Training Accuracy: 98.65%\n",
      "Epoch 85, Loss: 0.05036678125257762, Training Accuracy: 98.71%\n",
      "Epoch 86, Loss: 0.05036345093802204, Training Accuracy: 98.68%\n",
      "Epoch 87, Loss: 0.041722474577847814, Training Accuracy: 98.84%\n",
      "Epoch 88, Loss: 0.045735524100952815, Training Accuracy: 98.74%\n",
      "Epoch 89, Loss: 0.047661107325566866, Training Accuracy: 98.80%\n",
      "Epoch 90, Loss: 0.05574325227122091, Training Accuracy: 98.59%\n",
      "Epoch 91, Loss: 0.047344699102895224, Training Accuracy: 98.76%\n",
      "Epoch 92, Loss: 0.04078968727437937, Training Accuracy: 98.89%\n",
      "Epoch 93, Loss: 0.04569780654152745, Training Accuracy: 98.76%\n",
      "Epoch 94, Loss: 0.04622830067842478, Training Accuracy: 98.73%\n",
      "Epoch 95, Loss: 0.05022568720300326, Training Accuracy: 98.68%\n",
      "Epoch 96, Loss: 0.044480796817972935, Training Accuracy: 98.87%\n",
      "Epoch 97, Loss: 0.03507042633827776, Training Accuracy: 99.06%\n",
      "Epoch 98, Loss: 0.05688012767765994, Training Accuracy: 98.55%\n",
      "Epoch 99, Loss: 0.04616609441460064, Training Accuracy: 98.86%\n",
      "Epoch 100, Loss: 0.05168688289116748, Training Accuracy: 98.70%\n",
      "Epoch 101, Loss: 0.03915028337912453, Training Accuracy: 99.02%\n",
      "Epoch 102, Loss: 0.04404620183644943, Training Accuracy: 98.88%\n",
      "Epoch 103, Loss: 0.04073709795069825, Training Accuracy: 98.98%\n",
      "Epoch 104, Loss: 0.05074440307964226, Training Accuracy: 98.75%\n",
      "Epoch 105, Loss: 0.05150251386214125, Training Accuracy: 98.68%\n",
      "Epoch 106, Loss: 0.035365775348268735, Training Accuracy: 99.15%\n",
      "Epoch 107, Loss: 0.043002464255330775, Training Accuracy: 98.87%\n",
      "Epoch 108, Loss: 0.03960087072548335, Training Accuracy: 98.99%\n",
      "Epoch 109, Loss: 0.05426415149696697, Training Accuracy: 98.68%\n",
      "Epoch 110, Loss: 0.04598342836254571, Training Accuracy: 98.89%\n",
      "Epoch 111, Loss: 0.0496999714639055, Training Accuracy: 98.80%\n",
      "Epoch 112, Loss: 0.03940297967022567, Training Accuracy: 99.03%\n",
      "Epoch 113, Loss: 0.051378208403043295, Training Accuracy: 98.77%\n",
      "Epoch 114, Loss: 0.04368006373202157, Training Accuracy: 98.93%\n",
      "Epoch 115, Loss: 0.050380697261953045, Training Accuracy: 98.79%\n",
      "Epoch 116, Loss: 0.042830194948190084, Training Accuracy: 98.96%\n",
      "Epoch 117, Loss: 0.04137004320743181, Training Accuracy: 99.05%\n",
      "Epoch 118, Loss: 0.0381052736089797, Training Accuracy: 99.04%\n",
      "Epoch 119, Loss: 0.04517117261841201, Training Accuracy: 98.91%\n",
      "Epoch 120, Loss: 0.05465890958130629, Training Accuracy: 98.78%\n",
      "Epoch 121, Loss: 0.03047620848274327, Training Accuracy: 99.18%\n",
      "Epoch 122, Loss: 0.033518462584727314, Training Accuracy: 99.14%\n",
      "Epoch 123, Loss: 0.053905554202913646, Training Accuracy: 98.85%\n",
      "Epoch 124, Loss: 0.04100624394074123, Training Accuracy: 99.03%\n",
      "Epoch 125, Loss: 0.05580198179528944, Training Accuracy: 98.72%\n",
      "Epoch 126, Loss: 0.030493243830219202, Training Accuracy: 99.24%\n",
      "Epoch 127, Loss: 0.05214900130612922, Training Accuracy: 98.89%\n",
      "Epoch 128, Loss: 0.05349613634146744, Training Accuracy: 98.81%\n",
      "Epoch 129, Loss: 0.029411291167528164, Training Accuracy: 99.28%\n",
      "Epoch 130, Loss: 0.04281478731810892, Training Accuracy: 99.00%\n",
      "Epoch 131, Loss: 0.050433070322318384, Training Accuracy: 98.91%\n",
      "Epoch 132, Loss: 0.04798261029488682, Training Accuracy: 98.96%\n",
      "Epoch 133, Loss: 0.0409423986805132, Training Accuracy: 99.02%\n",
      "Epoch 134, Loss: 0.030127974398957785, Training Accuracy: 99.26%\n",
      "Epoch 135, Loss: 0.04733844258057082, Training Accuracy: 98.94%\n",
      "Epoch 136, Loss: 0.04390253207491608, Training Accuracy: 99.01%\n",
      "Epoch 137, Loss: 0.047692129427635156, Training Accuracy: 98.94%\n",
      "Epoch 138, Loss: 0.038005713739234885, Training Accuracy: 99.12%\n",
      "Epoch 139, Loss: 0.046226860148944025, Training Accuracy: 99.02%\n",
      "Epoch 140, Loss: 0.040523126328002715, Training Accuracy: 99.07%\n",
      "Epoch 141, Loss: 0.033968313123323976, Training Accuracy: 99.22%\n",
      "Epoch 142, Loss: 0.053194880481167287, Training Accuracy: 98.89%\n",
      "Epoch 143, Loss: 0.037532860260077, Training Accuracy: 99.15%\n",
      "Epoch 144, Loss: 0.043130733284628235, Training Accuracy: 99.12%\n",
      "Epoch 145, Loss: 0.04990667610783326, Training Accuracy: 99.00%\n",
      "Epoch 146, Loss: 0.0329915511816425, Training Accuracy: 99.20%\n",
      "Epoch 147, Loss: 0.04895344995968478, Training Accuracy: 98.98%\n",
      "Epoch 148, Loss: 0.03688568733232712, Training Accuracy: 99.13%\n",
      "Epoch 149, Loss: 0.043079762681339506, Training Accuracy: 99.04%\n",
      "Epoch 150, Loss: 0.05515014837203762, Training Accuracy: 98.94%\n",
      "Epoch 151, Loss: 0.04091230605537674, Training Accuracy: 99.18%\n",
      "Epoch 152, Loss: 0.04618779872338444, Training Accuracy: 99.07%\n",
      "Epoch 153, Loss: 0.0410433281078481, Training Accuracy: 99.13%\n",
      "Epoch 154, Loss: 0.04679280152276303, Training Accuracy: 99.05%\n",
      "Epoch 155, Loss: 0.037200092000636974, Training Accuracy: 99.22%\n",
      "Epoch 156, Loss: 0.048536063764177, Training Accuracy: 99.03%\n",
      "Epoch 157, Loss: 0.041194989453205315, Training Accuracy: 99.16%\n",
      "Epoch 158, Loss: 0.03897624429923384, Training Accuracy: 99.17%\n",
      "Epoch 159, Loss: 0.04283737533389572, Training Accuracy: 99.07%\n",
      "Epoch 160, Loss: 0.046002068745033646, Training Accuracy: 99.07%\n",
      "Epoch 161, Loss: 0.044457281379452734, Training Accuracy: 99.12%\n",
      "Epoch 162, Loss: 0.0435650420652791, Training Accuracy: 99.08%\n",
      "Epoch 163, Loss: 0.041342044808043044, Training Accuracy: 99.09%\n",
      "Epoch 164, Loss: 0.04322917366229957, Training Accuracy: 99.14%\n",
      "Epoch 165, Loss: 0.04461348667124198, Training Accuracy: 99.15%\n",
      "Epoch 166, Loss: 0.045515491471811685, Training Accuracy: 99.10%\n",
      "Epoch 167, Loss: 0.05452500201545295, Training Accuracy: 98.99%\n",
      "Epoch 168, Loss: 0.03732733280612245, Training Accuracy: 99.23%\n",
      "Epoch 169, Loss: 0.05219769632380703, Training Accuracy: 99.01%\n",
      "Epoch 170, Loss: 0.037968611398123846, Training Accuracy: 99.28%\n",
      "Epoch 171, Loss: 0.03636969861942209, Training Accuracy: 99.25%\n",
      "Epoch 172, Loss: 0.03992017833636889, Training Accuracy: 99.21%\n",
      "Epoch 173, Loss: 0.047498089535642586, Training Accuracy: 99.10%\n",
      "Epoch 174, Loss: 0.04600494168642561, Training Accuracy: 99.14%\n",
      "Epoch 175, Loss: 0.041717335035293304, Training Accuracy: 99.20%\n",
      "Accuracy of the network on the 10000 test images: 63.66%\n",
      "Run 1 Summary:\n",
      "Training Accuracies: [51.17, 65.302, 70.542, 74.152, 77.658, 80.496, 83.198, 85.448, 87.762, 89.708, 91.552, 92.538, 93.748, 94.654, 95.31, 95.796, 95.826, 96.61, 96.226, 97.162, 96.634, 96.848, 97.128, 97.494, 97.406, 97.322, 97.426, 97.584, 97.43, 97.856, 97.624, 97.616, 98.018, 97.66, 97.878, 98.214, 97.906, 97.748, 97.926, 98.162, 97.692, 98.404, 98.188, 97.826, 98.462, 98.002, 98.376, 98.328, 98.298, 98.132, 98.308, 98.514, 97.948, 98.476, 98.394, 98.254, 98.37, 98.618, 98.204, 98.51, 98.362, 98.514, 98.404, 98.702, 98.556, 98.466, 98.41, 98.564, 98.616, 98.884, 98.556, 98.476, 98.41, 98.578, 98.66, 98.74, 98.61, 98.928, 98.5, 98.678, 98.748, 98.696, 98.88, 98.652, 98.708, 98.682, 98.844, 98.742, 98.804, 98.59, 98.762, 98.89, 98.764, 98.734, 98.68, 98.87, 99.058, 98.554, 98.856, 98.704, 99.018, 98.88, 98.982, 98.754, 98.682, 99.146, 98.874, 98.992, 98.684, 98.886, 98.796, 99.026, 98.772, 98.932, 98.788, 98.958, 99.046, 99.042, 98.908, 98.782, 99.182, 99.138, 98.846, 99.032, 98.722, 99.244, 98.886, 98.81, 99.282, 99.002, 98.906, 98.958, 99.024, 99.262, 98.936, 99.014, 98.938, 99.124, 99.018, 99.072, 99.218, 98.886, 99.15, 99.12, 99.004, 99.2, 98.98, 99.134, 99.04, 98.94, 99.182, 99.068, 99.13, 99.048, 99.22, 99.034, 99.156, 99.166, 99.068, 99.066, 99.122, 99.082, 99.09, 99.142, 99.146, 99.104, 98.99, 99.23, 99.01, 99.278, 99.246, 99.206, 99.1, 99.144, 99.202]\n",
      "Test Accuracy: 63.66%\n",
      "Losses: [1.3494219994148635, 0.9814208072926992, 0.8356298074088133, 0.7299919880717002, 0.63582071055994, 0.5552120241134063, 0.4778876421815904, 0.41430928741041045, 0.34762262654921894, 0.29292867616619295, 0.24321787699561595, 0.21076269257727945, 0.18039121651007314, 0.15494676549797473, 0.13247217021434737, 0.12202590903567384, 0.1198809984779638, 0.09875401328472644, 0.10753591237422984, 0.08315114609306426, 0.09783185840747498, 0.08821104312151232, 0.08245842212327587, 0.07404946474372731, 0.07822781097253456, 0.07994936078982166, 0.07658994300207576, 0.07201124009881121, 0.07617712487785808, 0.06500426902229686, 0.06862043977990184, 0.07154680112057635, 0.05802399053594426, 0.07347977895091962, 0.06673880804108594, 0.053280079646198, 0.0630197105713396, 0.07292321335394507, 0.06310431856630846, 0.055643964284623655, 0.07402469538276672, 0.0495981971043154, 0.05888521254954078, 0.06940366407794415, 0.04867410723185774, 0.06341840127558784, 0.050416885934564074, 0.05312141296926804, 0.05580737756639201, 0.060791817370529286, 0.054809361604223825, 0.045900860011441835, 0.06751536052708891, 0.05123775020071878, 0.05433858845372062, 0.055119402584960134, 0.054167409221332036, 0.04501873624921271, 0.06143221241915348, 0.04901857899380666, 0.056822762858578506, 0.05124758591782875, 0.05191148485548992, 0.041650983319832774, 0.05077781228107743, 0.05294440287796957, 0.05372902190807424, 0.052165133042877024, 0.04629737349334573, 0.03654165578396235, 0.04924253311936114, 0.05517445135378659, 0.056883226097924566, 0.04834863272042784, 0.04582109974942754, 0.048549537841221686, 0.051910899737802656, 0.03785377249165866, 0.054603333208577104, 0.049816382440391216, 0.04403444061154237, 0.04504884399036533, 0.042707194856274146, 0.05077088317994307, 0.05036678125257762, 0.05036345093802204, 0.041722474577847814, 0.045735524100952815, 0.047661107325566866, 0.05574325227122091, 0.047344699102895224, 0.04078968727437937, 0.04569780654152745, 0.04622830067842478, 0.05022568720300326, 0.044480796817972935, 0.03507042633827776, 0.05688012767765994, 0.04616609441460064, 0.05168688289116748, 0.03915028337912453, 0.04404620183644943, 0.04073709795069825, 0.05074440307964226, 0.05150251386214125, 0.035365775348268735, 0.043002464255330775, 0.03960087072548335, 0.05426415149696697, 0.04598342836254571, 0.0496999714639055, 0.03940297967022567, 0.051378208403043295, 0.04368006373202157, 0.050380697261953045, 0.042830194948190084, 0.04137004320743181, 0.0381052736089797, 0.04517117261841201, 0.05465890958130629, 0.03047620848274327, 0.033518462584727314, 0.053905554202913646, 0.04100624394074123, 0.05580198179528944, 0.030493243830219202, 0.05214900130612922, 0.05349613634146744, 0.029411291167528164, 0.04281478731810892, 0.050433070322318384, 0.04798261029488682, 0.0409423986805132, 0.030127974398957785, 0.04733844258057082, 0.04390253207491608, 0.047692129427635156, 0.038005713739234885, 0.046226860148944025, 0.040523126328002715, 0.033968313123323976, 0.053194880481167287, 0.037532860260077, 0.043130733284628235, 0.04990667610783326, 0.0329915511816425, 0.04895344995968478, 0.03688568733232712, 0.043079762681339506, 0.05515014837203762, 0.04091230605537674, 0.04618779872338444, 0.0410433281078481, 0.04679280152276303, 0.037200092000636974, 0.048536063764177, 0.041194989453205315, 0.03897624429923384, 0.04283737533389572, 0.046002068745033646, 0.044457281379452734, 0.0435650420652791, 0.041342044808043044, 0.04322917366229957, 0.04461348667124198, 0.045515491471811685, 0.05452500201545295, 0.03732733280612245, 0.05219769632380703, 0.037968611398123846, 0.03636969861942209, 0.03992017833636889, 0.047498089535642586, 0.04600494168642561, 0.041717335035293304]\n",
      "Starting run 2/5\n",
      "Epoch 1, Loss: 1.4163293008456754, Training Accuracy: 48.56%\n",
      "Epoch 2, Loss: 1.0791452498844518, Training Accuracy: 61.38%\n",
      "Epoch 3, Loss: 0.9499881208857612, Training Accuracy: 65.95%\n",
      "Epoch 4, Loss: 0.868969599228076, Training Accuracy: 69.14%\n",
      "Epoch 5, Loss: 0.8077892848216665, Training Accuracy: 71.21%\n",
      "Epoch 6, Loss: 0.7574323995034103, Training Accuracy: 73.10%\n",
      "Epoch 7, Loss: 0.7050806714217072, Training Accuracy: 74.81%\n",
      "Epoch 8, Loss: 0.6605011337934552, Training Accuracy: 76.51%\n",
      "Epoch 9, Loss: 0.6157228853696447, Training Accuracy: 77.98%\n",
      "Epoch 10, Loss: 0.5787174860610986, Training Accuracy: 79.44%\n",
      "Epoch 11, Loss: 0.5358124918606885, Training Accuracy: 81.04%\n",
      "Epoch 12, Loss: 0.498826170821324, Training Accuracy: 82.28%\n",
      "Epoch 13, Loss: 0.4627329172266414, Training Accuracy: 83.47%\n",
      "Epoch 14, Loss: 0.4293824739354041, Training Accuracy: 84.78%\n",
      "Epoch 15, Loss: 0.3976521817848201, Training Accuracy: 85.88%\n",
      "Epoch 16, Loss: 0.36425261054654867, Training Accuracy: 87.23%\n",
      "Epoch 17, Loss: 0.3365084996728031, Training Accuracy: 88.08%\n",
      "Epoch 18, Loss: 0.3098334405676026, Training Accuracy: 89.04%\n",
      "Epoch 19, Loss: 0.2775965527538448, Training Accuracy: 90.31%\n",
      "Epoch 20, Loss: 0.2477849342233842, Training Accuracy: 91.38%\n",
      "Epoch 21, Loss: 0.23135414888696446, Training Accuracy: 91.85%\n",
      "Epoch 22, Loss: 0.20851584242852142, Training Accuracy: 92.78%\n",
      "Epoch 23, Loss: 0.18813242553673742, Training Accuracy: 93.38%\n",
      "Epoch 24, Loss: 0.17103497764510114, Training Accuracy: 94.01%\n",
      "Epoch 25, Loss: 0.15695629473251607, Training Accuracy: 94.57%\n",
      "Epoch 26, Loss: 0.13975263739009494, Training Accuracy: 95.15%\n",
      "Epoch 27, Loss: 0.13659694797271277, Training Accuracy: 95.22%\n",
      "Epoch 28, Loss: 0.12389370628997036, Training Accuracy: 95.66%\n",
      "Epoch 29, Loss: 0.11745399211073661, Training Accuracy: 95.89%\n",
      "Epoch 30, Loss: 0.10892996952161574, Training Accuracy: 96.27%\n",
      "Epoch 31, Loss: 0.10495186207430138, Training Accuracy: 96.30%\n",
      "Epoch 32, Loss: 0.09722707879579509, Training Accuracy: 96.65%\n",
      "Epoch 33, Loss: 0.09675757439159181, Training Accuracy: 96.64%\n",
      "Epoch 34, Loss: 0.09243296950047983, Training Accuracy: 96.69%\n",
      "Epoch 35, Loss: 0.07664667776264154, Training Accuracy: 97.30%\n",
      "Epoch 36, Loss: 0.09028068110656441, Training Accuracy: 96.79%\n",
      "Epoch 37, Loss: 0.0931002419332371, Training Accuracy: 96.73%\n",
      "Epoch 38, Loss: 0.07434952324923233, Training Accuracy: 97.45%\n",
      "Epoch 39, Loss: 0.07567173732763818, Training Accuracy: 97.44%\n",
      "Epoch 40, Loss: 0.08145833805309433, Training Accuracy: 97.11%\n",
      "Epoch 41, Loss: 0.06371177329986696, Training Accuracy: 97.87%\n",
      "Epoch 42, Loss: 0.07705045293997545, Training Accuracy: 97.38%\n",
      "Epoch 43, Loss: 0.0853294745939748, Training Accuracy: 97.12%\n",
      "Epoch 44, Loss: 0.05961264045748269, Training Accuracy: 97.93%\n",
      "Epoch 45, Loss: 0.0745487960801005, Training Accuracy: 97.44%\n",
      "Epoch 46, Loss: 0.06392456093342151, Training Accuracy: 97.78%\n",
      "Epoch 47, Loss: 0.06041531441265793, Training Accuracy: 97.85%\n",
      "Epoch 48, Loss: 0.07132906127733576, Training Accuracy: 97.67%\n",
      "Epoch 49, Loss: 0.06737334943403873, Training Accuracy: 97.78%\n",
      "Epoch 50, Loss: 0.05797548827273376, Training Accuracy: 98.00%\n",
      "Epoch 51, Loss: 0.06361876758169192, Training Accuracy: 97.78%\n",
      "Epoch 52, Loss: 0.06254735159809174, Training Accuracy: 97.84%\n",
      "Epoch 53, Loss: 0.0504921095058433, Training Accuracy: 98.26%\n",
      "Epoch 54, Loss: 0.07000680948274753, Training Accuracy: 97.67%\n",
      "Epoch 55, Loss: 0.06617411371568203, Training Accuracy: 97.87%\n",
      "Epoch 56, Loss: 0.05537199717819812, Training Accuracy: 98.11%\n",
      "Epoch 57, Loss: 0.05658712785002058, Training Accuracy: 98.08%\n",
      "Epoch 58, Loss: 0.06270839341143813, Training Accuracy: 97.78%\n",
      "Epoch 59, Loss: 0.05405417049546009, Training Accuracy: 98.16%\n",
      "Epoch 60, Loss: 0.0650167510459346, Training Accuracy: 97.81%\n",
      "Epoch 61, Loss: 0.06341174769285551, Training Accuracy: 97.82%\n",
      "Epoch 62, Loss: 0.044876873402012504, Training Accuracy: 98.46%\n",
      "Epoch 63, Loss: 0.06036586427892608, Training Accuracy: 97.96%\n",
      "Epoch 64, Loss: 0.05902245019949899, Training Accuracy: 98.03%\n",
      "Epoch 65, Loss: 0.06466951696275104, Training Accuracy: 97.84%\n",
      "Epoch 66, Loss: 0.0499129203686942, Training Accuracy: 98.28%\n",
      "Epoch 67, Loss: 0.052434447090565155, Training Accuracy: 98.27%\n",
      "Epoch 68, Loss: 0.049594364790207186, Training Accuracy: 98.29%\n",
      "Epoch 69, Loss: 0.04531761277054646, Training Accuracy: 98.48%\n",
      "Epoch 70, Loss: 0.05630567309281733, Training Accuracy: 98.15%\n",
      "Epoch 71, Loss: 0.04999071792752989, Training Accuracy: 98.30%\n",
      "Epoch 72, Loss: 0.05138066350927522, Training Accuracy: 98.25%\n",
      "Epoch 73, Loss: 0.055802647567540525, Training Accuracy: 98.21%\n",
      "Epoch 74, Loss: 0.05858141850565335, Training Accuracy: 98.12%\n",
      "Epoch 75, Loss: 0.043502983307707674, Training Accuracy: 98.54%\n",
      "Epoch 76, Loss: 0.04576144999676675, Training Accuracy: 98.49%\n",
      "Epoch 77, Loss: 0.04778124669568537, Training Accuracy: 98.41%\n",
      "Epoch 78, Loss: 0.05541332448971734, Training Accuracy: 98.21%\n",
      "Epoch 79, Loss: 0.04751408734643782, Training Accuracy: 98.49%\n",
      "Epoch 80, Loss: 0.05138750859805906, Training Accuracy: 98.28%\n",
      "Epoch 81, Loss: 0.055274037081636446, Training Accuracy: 98.30%\n",
      "Epoch 82, Loss: 0.05234093030689426, Training Accuracy: 98.32%\n",
      "Epoch 83, Loss: 0.0386231567542342, Training Accuracy: 98.69%\n",
      "Epoch 84, Loss: 0.04664620223527779, Training Accuracy: 98.44%\n",
      "Epoch 85, Loss: 0.05445765020825086, Training Accuracy: 98.29%\n",
      "Epoch 86, Loss: 0.04238017354695414, Training Accuracy: 98.60%\n",
      "Epoch 87, Loss: 0.0514793188672985, Training Accuracy: 98.31%\n",
      "Epoch 88, Loss: 0.03879212535398356, Training Accuracy: 98.74%\n",
      "Epoch 89, Loss: 0.03735892436507455, Training Accuracy: 98.79%\n",
      "Epoch 90, Loss: 0.059592189058787104, Training Accuracy: 98.15%\n",
      "Epoch 91, Loss: 0.04389915318083381, Training Accuracy: 98.61%\n",
      "Epoch 92, Loss: 0.04991928593446801, Training Accuracy: 98.44%\n",
      "Epoch 93, Loss: 0.04213213166644019, Training Accuracy: 98.62%\n",
      "Epoch 94, Loss: 0.043571923487264304, Training Accuracy: 98.59%\n",
      "Epoch 95, Loss: 0.05017843400567348, Training Accuracy: 98.41%\n",
      "Epoch 96, Loss: 0.034666008810885114, Training Accuracy: 98.90%\n",
      "Epoch 97, Loss: 0.050166034081299035, Training Accuracy: 98.47%\n",
      "Epoch 98, Loss: 0.0543355359221705, Training Accuracy: 98.36%\n",
      "Epoch 99, Loss: 0.03693477928487375, Training Accuracy: 98.85%\n",
      "Epoch 100, Loss: 0.043181417188933924, Training Accuracy: 98.64%\n",
      "Epoch 101, Loss: 0.035304972659730975, Training Accuracy: 98.83%\n",
      "Epoch 102, Loss: 0.05326986756999224, Training Accuracy: 98.47%\n",
      "Epoch 103, Loss: 0.044394671768055755, Training Accuracy: 98.64%\n",
      "Epoch 104, Loss: 0.045180758569695355, Training Accuracy: 98.65%\n",
      "Epoch 105, Loss: 0.047971284665587734, Training Accuracy: 98.55%\n",
      "Epoch 106, Loss: 0.042366809311785554, Training Accuracy: 98.73%\n",
      "Epoch 107, Loss: 0.03938667629023499, Training Accuracy: 98.74%\n",
      "Epoch 108, Loss: 0.04477480155504093, Training Accuracy: 98.59%\n",
      "Epoch 109, Loss: 0.04266280805495161, Training Accuracy: 98.73%\n",
      "Epoch 110, Loss: 0.039037532774275456, Training Accuracy: 98.72%\n",
      "Epoch 111, Loss: 0.05160979400946131, Training Accuracy: 98.49%\n",
      "Epoch 112, Loss: 0.048253731686044146, Training Accuracy: 98.52%\n",
      "Epoch 113, Loss: 0.03414438625910567, Training Accuracy: 98.91%\n",
      "Epoch 114, Loss: 0.04752236085585402, Training Accuracy: 98.57%\n",
      "Epoch 115, Loss: 0.03767401121774554, Training Accuracy: 98.82%\n",
      "Epoch 116, Loss: 0.03534612648160485, Training Accuracy: 98.82%\n",
      "Epoch 117, Loss: 0.039830699953713716, Training Accuracy: 98.80%\n",
      "Epoch 118, Loss: 0.03753279759352935, Training Accuracy: 98.78%\n",
      "Epoch 119, Loss: 0.03505877554129061, Training Accuracy: 98.89%\n",
      "Epoch 120, Loss: 0.04060544746192803, Training Accuracy: 98.83%\n",
      "Epoch 121, Loss: 0.04466851417607505, Training Accuracy: 98.63%\n",
      "Epoch 122, Loss: 0.0411111660783407, Training Accuracy: 98.80%\n",
      "Epoch 123, Loss: 0.04004462841247312, Training Accuracy: 98.80%\n",
      "Epoch 124, Loss: 0.04791447764197408, Training Accuracy: 98.61%\n",
      "Epoch 125, Loss: 0.03493305637011942, Training Accuracy: 98.96%\n",
      "Epoch 126, Loss: 0.0468950165355506, Training Accuracy: 98.57%\n",
      "Epoch 127, Loss: 0.03438200550948857, Training Accuracy: 98.96%\n",
      "Epoch 128, Loss: 0.054467279060535036, Training Accuracy: 98.47%\n",
      "Epoch 129, Loss: 0.02987112625594846, Training Accuracy: 99.08%\n",
      "Epoch 130, Loss: 0.04547413409274012, Training Accuracy: 98.72%\n",
      "Epoch 131, Loss: 0.03536734345585173, Training Accuracy: 98.92%\n",
      "Epoch 132, Loss: 0.046829287604748485, Training Accuracy: 98.64%\n",
      "Epoch 133, Loss: 0.042889015587183554, Training Accuracy: 98.73%\n",
      "Epoch 134, Loss: 0.03435178659946571, Training Accuracy: 98.91%\n",
      "Epoch 135, Loss: 0.03381763690519277, Training Accuracy: 98.95%\n",
      "Epoch 136, Loss: 0.0473215097202733, Training Accuracy: 98.61%\n",
      "Epoch 137, Loss: 0.034706393702368436, Training Accuracy: 98.97%\n",
      "Epoch 138, Loss: 0.03899141936759276, Training Accuracy: 98.91%\n",
      "Epoch 139, Loss: 0.03961983401675594, Training Accuracy: 98.83%\n",
      "Epoch 140, Loss: 0.04302003032631905, Training Accuracy: 98.78%\n",
      "Epoch 141, Loss: 0.04348831962336641, Training Accuracy: 98.78%\n",
      "Epoch 142, Loss: 0.03873149558020558, Training Accuracy: 98.87%\n",
      "Epoch 143, Loss: 0.04025840557810078, Training Accuracy: 98.84%\n",
      "Epoch 144, Loss: 0.03533157960380944, Training Accuracy: 98.95%\n",
      "Epoch 145, Loss: 0.03999934721299435, Training Accuracy: 98.86%\n",
      "Epoch 146, Loss: 0.03260983229039547, Training Accuracy: 99.07%\n",
      "Epoch 147, Loss: 0.04191858359141683, Training Accuracy: 98.78%\n",
      "Epoch 148, Loss: 0.045293002135561894, Training Accuracy: 98.73%\n",
      "Epoch 149, Loss: 0.03982300975916159, Training Accuracy: 98.92%\n",
      "Epoch 150, Loss: 0.03841242400575657, Training Accuracy: 98.95%\n",
      "Epoch 151, Loss: 0.032461455211866434, Training Accuracy: 99.06%\n",
      "Epoch 152, Loss: 0.037656720992971945, Training Accuracy: 98.95%\n",
      "Epoch 153, Loss: 0.036769942569743065, Training Accuracy: 98.99%\n",
      "Epoch 154, Loss: 0.04465324396672949, Training Accuracy: 98.79%\n",
      "Epoch 155, Loss: 0.038218144282829894, Training Accuracy: 98.94%\n",
      "Epoch 156, Loss: 0.036383810001277134, Training Accuracy: 98.96%\n",
      "Epoch 157, Loss: 0.03883087651674615, Training Accuracy: 98.95%\n",
      "Epoch 158, Loss: 0.03121050567612663, Training Accuracy: 99.09%\n",
      "Epoch 159, Loss: 0.03786874829163544, Training Accuracy: 98.90%\n",
      "Epoch 160, Loss: 0.031818090446034585, Training Accuracy: 99.06%\n",
      "Epoch 161, Loss: 0.036533488055817506, Training Accuracy: 98.97%\n",
      "Epoch 162, Loss: 0.03459691003997745, Training Accuracy: 99.00%\n",
      "Epoch 163, Loss: 0.04403996599559176, Training Accuracy: 98.78%\n",
      "Epoch 164, Loss: 0.03606286131402567, Training Accuracy: 98.97%\n",
      "Epoch 165, Loss: 0.04028042828364734, Training Accuracy: 98.89%\n",
      "Epoch 166, Loss: 0.028572325140481795, Training Accuracy: 99.13%\n",
      "Epoch 167, Loss: 0.03734133949141059, Training Accuracy: 99.00%\n",
      "Epoch 168, Loss: 0.0444232622853133, Training Accuracy: 98.80%\n",
      "Epoch 169, Loss: 0.027211332173911078, Training Accuracy: 99.21%\n",
      "Epoch 170, Loss: 0.03625570303484099, Training Accuracy: 99.05%\n",
      "Epoch 171, Loss: 0.03733791235099375, Training Accuracy: 98.95%\n",
      "Epoch 172, Loss: 0.03846575647578197, Training Accuracy: 98.93%\n",
      "Epoch 173, Loss: 0.03805623113144449, Training Accuracy: 98.99%\n",
      "Epoch 174, Loss: 0.03350715759998225, Training Accuracy: 99.08%\n",
      "Epoch 175, Loss: 0.049480960629195624, Training Accuracy: 98.76%\n",
      "Accuracy of the network on the 10000 test images: 64.02%\n",
      "Run 2 Summary:\n",
      "Training Accuracies: [48.564, 61.384, 65.95, 69.138, 71.214, 73.1, 74.812, 76.508, 77.978, 79.44, 81.044, 82.284, 83.472, 84.776, 85.88, 87.234, 88.084, 89.042, 90.306, 91.38, 91.846, 92.78, 93.378, 94.014, 94.566, 95.152, 95.22, 95.656, 95.89, 96.272, 96.298, 96.648, 96.642, 96.688, 97.302, 96.79, 96.73, 97.452, 97.442, 97.114, 97.872, 97.378, 97.124, 97.928, 97.442, 97.778, 97.85, 97.672, 97.78, 98.0, 97.778, 97.84, 98.264, 97.668, 97.872, 98.112, 98.08, 97.782, 98.156, 97.814, 97.818, 98.462, 97.964, 98.028, 97.84, 98.276, 98.268, 98.29, 98.476, 98.148, 98.296, 98.246, 98.206, 98.118, 98.544, 98.486, 98.414, 98.214, 98.494, 98.278, 98.304, 98.322, 98.688, 98.444, 98.294, 98.598, 98.306, 98.74, 98.79, 98.146, 98.614, 98.444, 98.62, 98.594, 98.406, 98.896, 98.468, 98.358, 98.846, 98.64, 98.834, 98.47, 98.644, 98.648, 98.552, 98.726, 98.738, 98.586, 98.726, 98.724, 98.488, 98.522, 98.912, 98.572, 98.822, 98.82, 98.798, 98.78, 98.894, 98.834, 98.628, 98.804, 98.798, 98.606, 98.958, 98.574, 98.96, 98.466, 99.078, 98.722, 98.916, 98.644, 98.728, 98.906, 98.952, 98.614, 98.966, 98.908, 98.832, 98.784, 98.776, 98.874, 98.838, 98.952, 98.862, 99.074, 98.778, 98.728, 98.916, 98.946, 99.056, 98.948, 98.988, 98.788, 98.936, 98.964, 98.948, 99.086, 98.898, 99.056, 98.97, 98.996, 98.782, 98.974, 98.886, 99.13, 99.0, 98.802, 99.21, 99.046, 98.95, 98.932, 98.99, 99.084, 98.76]\n",
      "Test Accuracy: 64.02%\n",
      "Losses: [1.4163293008456754, 1.0791452498844518, 0.9499881208857612, 0.868969599228076, 0.8077892848216665, 0.7574323995034103, 0.7050806714217072, 0.6605011337934552, 0.6157228853696447, 0.5787174860610986, 0.5358124918606885, 0.498826170821324, 0.4627329172266414, 0.4293824739354041, 0.3976521817848201, 0.36425261054654867, 0.3365084996728031, 0.3098334405676026, 0.2775965527538448, 0.2477849342233842, 0.23135414888696446, 0.20851584242852142, 0.18813242553673742, 0.17103497764510114, 0.15695629473251607, 0.13975263739009494, 0.13659694797271277, 0.12389370628997036, 0.11745399211073661, 0.10892996952161574, 0.10495186207430138, 0.09722707879579509, 0.09675757439159181, 0.09243296950047983, 0.07664667776264154, 0.09028068110656441, 0.0931002419332371, 0.07434952324923233, 0.07567173732763818, 0.08145833805309433, 0.06371177329986696, 0.07705045293997545, 0.0853294745939748, 0.05961264045748269, 0.0745487960801005, 0.06392456093342151, 0.06041531441265793, 0.07132906127733576, 0.06737334943403873, 0.05797548827273376, 0.06361876758169192, 0.06254735159809174, 0.0504921095058433, 0.07000680948274753, 0.06617411371568203, 0.05537199717819812, 0.05658712785002058, 0.06270839341143813, 0.05405417049546009, 0.0650167510459346, 0.06341174769285551, 0.044876873402012504, 0.06036586427892608, 0.05902245019949899, 0.06466951696275104, 0.0499129203686942, 0.052434447090565155, 0.049594364790207186, 0.04531761277054646, 0.05630567309281733, 0.04999071792752989, 0.05138066350927522, 0.055802647567540525, 0.05858141850565335, 0.043502983307707674, 0.04576144999676675, 0.04778124669568537, 0.05541332448971734, 0.04751408734643782, 0.05138750859805906, 0.055274037081636446, 0.05234093030689426, 0.0386231567542342, 0.04664620223527779, 0.05445765020825086, 0.04238017354695414, 0.0514793188672985, 0.03879212535398356, 0.03735892436507455, 0.059592189058787104, 0.04389915318083381, 0.04991928593446801, 0.04213213166644019, 0.043571923487264304, 0.05017843400567348, 0.034666008810885114, 0.050166034081299035, 0.0543355359221705, 0.03693477928487375, 0.043181417188933924, 0.035304972659730975, 0.05326986756999224, 0.044394671768055755, 0.045180758569695355, 0.047971284665587734, 0.042366809311785554, 0.03938667629023499, 0.04477480155504093, 0.04266280805495161, 0.039037532774275456, 0.05160979400946131, 0.048253731686044146, 0.03414438625910567, 0.04752236085585402, 0.03767401121774554, 0.03534612648160485, 0.039830699953713716, 0.03753279759352935, 0.03505877554129061, 0.04060544746192803, 0.04466851417607505, 0.0411111660783407, 0.04004462841247312, 0.04791447764197408, 0.03493305637011942, 0.0468950165355506, 0.03438200550948857, 0.054467279060535036, 0.02987112625594846, 0.04547413409274012, 0.03536734345585173, 0.046829287604748485, 0.042889015587183554, 0.03435178659946571, 0.03381763690519277, 0.0473215097202733, 0.034706393702368436, 0.03899141936759276, 0.03961983401675594, 0.04302003032631905, 0.04348831962336641, 0.03873149558020558, 0.04025840557810078, 0.03533157960380944, 0.03999934721299435, 0.03260983229039547, 0.04191858359141683, 0.045293002135561894, 0.03982300975916159, 0.03841242400575657, 0.032461455211866434, 0.037656720992971945, 0.036769942569743065, 0.04465324396672949, 0.038218144282829894, 0.036383810001277134, 0.03883087651674615, 0.03121050567612663, 0.03786874829163544, 0.031818090446034585, 0.036533488055817506, 0.03459691003997745, 0.04403996599559176, 0.03606286131402567, 0.04028042828364734, 0.028572325140481795, 0.03734133949141059, 0.0444232622853133, 0.027211332173911078, 0.03625570303484099, 0.03733791235099375, 0.03846575647578197, 0.03805623113144449, 0.03350715759998225, 0.049480960629195624]\n",
      "Starting run 3/5\n",
      "Epoch 1, Loss: 1.334589092475374, Training Accuracy: 51.65%\n",
      "Epoch 2, Loss: 0.9749450300965468, Training Accuracy: 65.37%\n",
      "Epoch 3, Loss: 0.8254432051306795, Training Accuracy: 70.83%\n",
      "Epoch 4, Loss: 0.7169686791003512, Training Accuracy: 74.67%\n",
      "Epoch 5, Loss: 0.6189110497836872, Training Accuracy: 78.24%\n",
      "Epoch 6, Loss: 0.5339433452509859, Training Accuracy: 81.09%\n",
      "Epoch 7, Loss: 0.45064001166454665, Training Accuracy: 84.20%\n",
      "Epoch 8, Loss: 0.37651557890731663, Training Accuracy: 86.96%\n",
      "Epoch 9, Loss: 0.3125065455256063, Training Accuracy: 88.98%\n",
      "Epoch 10, Loss: 0.24673931753200948, Training Accuracy: 91.51%\n",
      "Epoch 11, Loss: 0.20392838422961704, Training Accuracy: 92.79%\n",
      "Epoch 12, Loss: 0.15580585751863543, Training Accuracy: 94.76%\n",
      "Epoch 13, Loss: 0.13771913779418335, Training Accuracy: 95.17%\n",
      "Epoch 14, Loss: 0.10989751358685629, Training Accuracy: 96.29%\n",
      "Epoch 15, Loss: 0.10131781926149945, Training Accuracy: 96.48%\n",
      "Epoch 16, Loss: 0.0900929072309676, Training Accuracy: 96.83%\n",
      "Epoch 17, Loss: 0.08630223718145505, Training Accuracy: 96.99%\n",
      "Epoch 18, Loss: 0.07109347003502557, Training Accuracy: 97.47%\n",
      "Epoch 19, Loss: 0.07759925648790386, Training Accuracy: 97.35%\n",
      "Epoch 20, Loss: 0.07894099634879119, Training Accuracy: 97.26%\n",
      "Epoch 21, Loss: 0.059652544614022876, Training Accuracy: 97.90%\n",
      "Epoch 22, Loss: 0.06979044966007847, Training Accuracy: 97.50%\n",
      "Epoch 23, Loss: 0.060014518483689584, Training Accuracy: 97.88%\n",
      "Epoch 24, Loss: 0.07043072672517937, Training Accuracy: 97.57%\n",
      "Epoch 25, Loss: 0.05767723985383754, Training Accuracy: 98.09%\n",
      "Epoch 26, Loss: 0.05815388454118972, Training Accuracy: 98.00%\n",
      "Epoch 27, Loss: 0.06473925867629454, Training Accuracy: 97.79%\n",
      "Epoch 28, Loss: 0.0556794028372511, Training Accuracy: 98.14%\n",
      "Epoch 29, Loss: 0.05708471546485506, Training Accuracy: 98.05%\n",
      "Epoch 30, Loss: 0.04925561297501502, Training Accuracy: 98.35%\n",
      "Epoch 31, Loss: 0.06162466817235341, Training Accuracy: 97.99%\n",
      "Epoch 32, Loss: 0.05709614791318446, Training Accuracy: 98.08%\n",
      "Epoch 33, Loss: 0.04648693205032181, Training Accuracy: 98.45%\n",
      "Epoch 34, Loss: 0.05743846246172362, Training Accuracy: 98.13%\n",
      "Epoch 35, Loss: 0.04594403502744306, Training Accuracy: 98.44%\n",
      "Epoch 36, Loss: 0.05078570663464813, Training Accuracy: 98.32%\n",
      "Epoch 37, Loss: 0.05313572195383733, Training Accuracy: 98.25%\n",
      "Epoch 38, Loss: 0.04398207311280896, Training Accuracy: 98.53%\n",
      "Epoch 39, Loss: 0.04763752605725566, Training Accuracy: 98.46%\n",
      "Epoch 40, Loss: 0.04493362677196571, Training Accuracy: 98.52%\n",
      "Epoch 41, Loss: 0.04484021431876278, Training Accuracy: 98.55%\n",
      "Epoch 42, Loss: 0.044183579594333944, Training Accuracy: 98.57%\n",
      "Epoch 43, Loss: 0.04889711515304795, Training Accuracy: 98.43%\n",
      "Epoch 44, Loss: 0.04683627094183971, Training Accuracy: 98.53%\n",
      "Epoch 45, Loss: 0.04326125550968245, Training Accuracy: 98.64%\n",
      "Epoch 46, Loss: 0.055358322966444724, Training Accuracy: 98.32%\n",
      "Epoch 47, Loss: 0.04568549724826601, Training Accuracy: 98.59%\n",
      "Epoch 48, Loss: 0.040140624467991805, Training Accuracy: 98.77%\n",
      "Epoch 49, Loss: 0.03840590122609634, Training Accuracy: 98.81%\n",
      "Epoch 50, Loss: 0.053584007793990605, Training Accuracy: 98.40%\n",
      "Epoch 51, Loss: 0.03929476146203274, Training Accuracy: 98.80%\n",
      "Epoch 52, Loss: 0.04739670632361106, Training Accuracy: 98.57%\n",
      "Epoch 53, Loss: 0.04765509391121496, Training Accuracy: 98.54%\n",
      "Epoch 54, Loss: 0.03358992473844028, Training Accuracy: 98.98%\n",
      "Epoch 55, Loss: 0.04707975177980258, Training Accuracy: 98.61%\n",
      "Epoch 56, Loss: 0.03849361576447713, Training Accuracy: 98.80%\n",
      "Epoch 57, Loss: 0.04580956300712161, Training Accuracy: 98.59%\n",
      "Epoch 58, Loss: 0.05208675653434011, Training Accuracy: 98.51%\n",
      "Epoch 59, Loss: 0.042020576877703227, Training Accuracy: 98.82%\n",
      "Epoch 60, Loss: 0.029307750959965243, Training Accuracy: 99.06%\n",
      "Epoch 61, Loss: 0.044151141814978485, Training Accuracy: 98.73%\n",
      "Epoch 62, Loss: 0.035710669139425916, Training Accuracy: 98.92%\n",
      "Epoch 63, Loss: 0.046483689276436604, Training Accuracy: 98.67%\n",
      "Epoch 64, Loss: 0.04731927136051227, Training Accuracy: 98.73%\n",
      "Epoch 65, Loss: 0.03495536197810081, Training Accuracy: 98.99%\n",
      "Epoch 66, Loss: 0.04145735319283412, Training Accuracy: 98.77%\n",
      "Epoch 67, Loss: 0.04023817718876332, Training Accuracy: 98.87%\n",
      "Epoch 68, Loss: 0.0462213355752148, Training Accuracy: 98.73%\n",
      "Epoch 69, Loss: 0.036110142148975184, Training Accuracy: 99.00%\n",
      "Epoch 70, Loss: 0.04146637422304489, Training Accuracy: 98.86%\n",
      "Epoch 71, Loss: 0.044589179201533775, Training Accuracy: 98.77%\n",
      "Epoch 72, Loss: 0.04125126561668409, Training Accuracy: 98.81%\n",
      "Epoch 73, Loss: 0.029268305674213246, Training Accuracy: 99.12%\n",
      "Epoch 74, Loss: 0.031547221616967686, Training Accuracy: 99.00%\n",
      "Epoch 75, Loss: 0.054141148939260864, Training Accuracy: 98.57%\n",
      "Epoch 76, Loss: 0.029601577160684962, Training Accuracy: 99.12%\n",
      "Epoch 77, Loss: 0.0422806017775463, Training Accuracy: 98.87%\n",
      "Epoch 78, Loss: 0.04016990247279336, Training Accuracy: 98.85%\n",
      "Epoch 79, Loss: 0.03687381221934693, Training Accuracy: 98.95%\n",
      "Epoch 80, Loss: 0.03789400523253468, Training Accuracy: 99.00%\n",
      "Epoch 81, Loss: 0.03357993843323957, Training Accuracy: 99.10%\n",
      "Epoch 82, Loss: 0.0399331979604748, Training Accuracy: 98.94%\n",
      "Epoch 83, Loss: 0.04333878042548593, Training Accuracy: 98.89%\n",
      "Epoch 84, Loss: 0.0404920931345029, Training Accuracy: 98.87%\n",
      "Epoch 85, Loss: 0.033536393110041834, Training Accuracy: 99.11%\n",
      "Epoch 86, Loss: 0.03689722970165383, Training Accuracy: 98.99%\n",
      "Epoch 87, Loss: 0.043093080563088325, Training Accuracy: 98.85%\n",
      "Epoch 88, Loss: 0.033008244359934276, Training Accuracy: 99.10%\n",
      "Epoch 89, Loss: 0.03607754541027792, Training Accuracy: 99.03%\n",
      "Epoch 90, Loss: 0.028206142852867395, Training Accuracy: 99.27%\n",
      "Epoch 91, Loss: 0.04327622973403439, Training Accuracy: 98.89%\n",
      "Epoch 92, Loss: 0.03225828847272489, Training Accuracy: 99.11%\n",
      "Epoch 93, Loss: 0.0367656214506173, Training Accuracy: 99.00%\n",
      "Epoch 94, Loss: 0.04361213465135547, Training Accuracy: 98.92%\n",
      "Epoch 95, Loss: 0.04303866882710304, Training Accuracy: 98.91%\n",
      "Epoch 96, Loss: 0.02909428643704083, Training Accuracy: 99.21%\n",
      "Epoch 97, Loss: 0.02929968204206491, Training Accuracy: 99.20%\n",
      "Epoch 98, Loss: 0.04499189914304457, Training Accuracy: 98.90%\n",
      "Epoch 99, Loss: 0.037635077545543835, Training Accuracy: 99.07%\n",
      "Epoch 100, Loss: 0.02977045131344368, Training Accuracy: 99.19%\n",
      "Epoch 101, Loss: 0.043093462400884686, Training Accuracy: 98.93%\n",
      "Epoch 102, Loss: 0.036014517782442815, Training Accuracy: 99.06%\n",
      "Epoch 103, Loss: 0.03297480061911383, Training Accuracy: 99.15%\n",
      "Epoch 104, Loss: 0.036296472272245583, Training Accuracy: 99.11%\n",
      "Epoch 105, Loss: 0.032960147814139064, Training Accuracy: 99.17%\n",
      "Epoch 106, Loss: 0.025274147994165753, Training Accuracy: 99.32%\n",
      "Epoch 107, Loss: 0.049161286004248, Training Accuracy: 98.91%\n",
      "Epoch 108, Loss: 0.043875292445769584, Training Accuracy: 98.94%\n",
      "Epoch 109, Loss: 0.03404716463642834, Training Accuracy: 99.18%\n",
      "Epoch 110, Loss: 0.027775726351183867, Training Accuracy: 99.33%\n",
      "Epoch 111, Loss: 0.04253179481806439, Training Accuracy: 99.08%\n",
      "Epoch 112, Loss: 0.04284357498829417, Training Accuracy: 98.98%\n",
      "Epoch 113, Loss: 0.03496712074400097, Training Accuracy: 99.19%\n",
      "Epoch 114, Loss: 0.033402873233939305, Training Accuracy: 99.20%\n",
      "Epoch 115, Loss: 0.0346747573513131, Training Accuracy: 99.18%\n",
      "Epoch 116, Loss: 0.04145772318613909, Training Accuracy: 99.00%\n",
      "Epoch 117, Loss: 0.030067536123660286, Training Accuracy: 99.28%\n",
      "Epoch 118, Loss: 0.034974306897538915, Training Accuracy: 99.18%\n",
      "Epoch 119, Loss: 0.03375468372998219, Training Accuracy: 99.17%\n",
      "Epoch 120, Loss: 0.037959138235536795, Training Accuracy: 99.13%\n",
      "Epoch 121, Loss: 0.03368195285090678, Training Accuracy: 99.24%\n",
      "Epoch 122, Loss: 0.030605811146323957, Training Accuracy: 99.27%\n",
      "Epoch 123, Loss: 0.041014853893384155, Training Accuracy: 99.12%\n",
      "Epoch 124, Loss: 0.032670779090247314, Training Accuracy: 99.25%\n",
      "Epoch 125, Loss: 0.03260672624044012, Training Accuracy: 99.16%\n",
      "Epoch 126, Loss: 0.027754936356919543, Training Accuracy: 99.34%\n",
      "Epoch 127, Loss: 0.04330607079506564, Training Accuracy: 99.04%\n",
      "Epoch 128, Loss: 0.040654419513383555, Training Accuracy: 99.06%\n",
      "Epoch 129, Loss: 0.030322671349603664, Training Accuracy: 99.34%\n",
      "Epoch 130, Loss: 0.03189427854555434, Training Accuracy: 99.21%\n",
      "Epoch 131, Loss: 0.038858121217352826, Training Accuracy: 99.15%\n",
      "Epoch 132, Loss: 0.03551454013844226, Training Accuracy: 99.23%\n",
      "Epoch 133, Loss: 0.03873485607990691, Training Accuracy: 99.17%\n",
      "Epoch 134, Loss: 0.04057666656511157, Training Accuracy: 99.12%\n",
      "Epoch 135, Loss: 0.03458614366830274, Training Accuracy: 99.22%\n",
      "Epoch 136, Loss: 0.026335754118458354, Training Accuracy: 99.34%\n",
      "Epoch 137, Loss: 0.03641098875723612, Training Accuracy: 99.19%\n",
      "Epoch 138, Loss: 0.03809516050873214, Training Accuracy: 99.22%\n",
      "Epoch 139, Loss: 0.030115818295853653, Training Accuracy: 99.31%\n",
      "Epoch 140, Loss: 0.03739423294300166, Training Accuracy: 99.16%\n",
      "Epoch 141, Loss: 0.03489176236395174, Training Accuracy: 99.26%\n",
      "Epoch 142, Loss: 0.027823867873070205, Training Accuracy: 99.34%\n",
      "Epoch 143, Loss: 0.03931179529998888, Training Accuracy: 99.11%\n",
      "Epoch 144, Loss: 0.03143593369108075, Training Accuracy: 99.29%\n",
      "Epoch 145, Loss: 0.032761324691998696, Training Accuracy: 99.26%\n",
      "Epoch 146, Loss: 0.03639638179059, Training Accuracy: 99.22%\n",
      "Epoch 147, Loss: 0.03504647819875764, Training Accuracy: 99.24%\n",
      "Epoch 148, Loss: 0.033867980351081024, Training Accuracy: 99.25%\n",
      "Epoch 149, Loss: 0.03300839384360785, Training Accuracy: 99.29%\n",
      "Epoch 150, Loss: 0.05201517774660328, Training Accuracy: 98.98%\n",
      "Epoch 151, Loss: 0.031230885386752442, Training Accuracy: 99.33%\n",
      "Epoch 152, Loss: 0.03242123318139725, Training Accuracy: 99.30%\n",
      "Epoch 153, Loss: 0.028091454350253196, Training Accuracy: 99.42%\n",
      "Epoch 154, Loss: 0.026952538805528104, Training Accuracy: 99.45%\n",
      "Epoch 155, Loss: 0.04578718071686281, Training Accuracy: 99.09%\n",
      "Epoch 156, Loss: 0.03637733910527948, Training Accuracy: 99.25%\n",
      "Epoch 157, Loss: 0.0323983312821905, Training Accuracy: 99.40%\n",
      "Epoch 158, Loss: 0.03798065337968581, Training Accuracy: 99.23%\n",
      "Epoch 159, Loss: 0.035563550931934154, Training Accuracy: 99.30%\n",
      "Epoch 160, Loss: 0.038287182951094755, Training Accuracy: 99.26%\n",
      "Epoch 161, Loss: 0.04027047514967669, Training Accuracy: 99.23%\n",
      "Epoch 162, Loss: 0.037277919464527194, Training Accuracy: 99.29%\n",
      "Epoch 163, Loss: 0.03285580544508642, Training Accuracy: 99.33%\n",
      "Epoch 164, Loss: 0.03714195569150075, Training Accuracy: 99.24%\n",
      "Epoch 165, Loss: 0.03898513096599157, Training Accuracy: 99.23%\n",
      "Epoch 166, Loss: 0.03410393150718347, Training Accuracy: 99.34%\n",
      "Epoch 167, Loss: 0.024774205876429905, Training Accuracy: 99.49%\n",
      "Epoch 168, Loss: 0.04215243476839697, Training Accuracy: 99.16%\n",
      "Epoch 169, Loss: 0.034495131309721365, Training Accuracy: 99.34%\n",
      "Epoch 170, Loss: 0.033938274973021085, Training Accuracy: 99.34%\n",
      "Epoch 171, Loss: 0.030340070578827063, Training Accuracy: 99.38%\n",
      "Epoch 172, Loss: 0.03773543751643165, Training Accuracy: 99.30%\n",
      "Epoch 173, Loss: 0.04011085868076404, Training Accuracy: 99.26%\n",
      "Epoch 174, Loss: 0.03600967619459342, Training Accuracy: 99.28%\n",
      "Epoch 175, Loss: 0.026279430327250265, Training Accuracy: 99.44%\n",
      "Accuracy of the network on the 10000 test images: 65.98%\n",
      "Run 3 Summary:\n",
      "Training Accuracies: [51.652, 65.374, 70.834, 74.672, 78.244, 81.092, 84.202, 86.96, 88.978, 91.512, 92.786, 94.758, 95.172, 96.288, 96.48, 96.826, 96.992, 97.472, 97.35, 97.258, 97.898, 97.502, 97.88, 97.574, 98.094, 97.998, 97.786, 98.142, 98.046, 98.35, 97.992, 98.084, 98.446, 98.126, 98.444, 98.32, 98.254, 98.528, 98.458, 98.518, 98.548, 98.572, 98.434, 98.528, 98.642, 98.322, 98.588, 98.772, 98.814, 98.4, 98.802, 98.572, 98.544, 98.984, 98.608, 98.802, 98.592, 98.506, 98.818, 99.064, 98.734, 98.916, 98.674, 98.732, 98.988, 98.768, 98.87, 98.732, 99.0, 98.862, 98.766, 98.808, 99.12, 99.004, 98.568, 99.118, 98.87, 98.848, 98.952, 98.996, 99.1, 98.944, 98.894, 98.87, 99.106, 98.988, 98.846, 99.1, 99.03, 99.268, 98.894, 99.114, 99.004, 98.918, 98.91, 99.208, 99.2, 98.9, 99.07, 99.194, 98.934, 99.064, 99.148, 99.106, 99.166, 99.324, 98.906, 98.938, 99.176, 99.326, 99.078, 98.976, 99.186, 99.204, 99.176, 98.998, 99.278, 99.184, 99.166, 99.128, 99.236, 99.27, 99.118, 99.248, 99.158, 99.338, 99.042, 99.06, 99.336, 99.21, 99.146, 99.228, 99.166, 99.116, 99.216, 99.336, 99.186, 99.224, 99.31, 99.156, 99.256, 99.342, 99.106, 99.286, 99.256, 99.216, 99.242, 99.25, 99.294, 98.976, 99.332, 99.304, 99.422, 99.454, 99.09, 99.252, 99.396, 99.23, 99.302, 99.264, 99.226, 99.288, 99.334, 99.242, 99.226, 99.338, 99.492, 99.164, 99.342, 99.336, 99.384, 99.3, 99.26, 99.276, 99.444]\n",
      "Test Accuracy: 65.98%\n",
      "Losses: [1.334589092475374, 0.9749450300965468, 0.8254432051306795, 0.7169686791003512, 0.6189110497836872, 0.5339433452509859, 0.45064001166454665, 0.37651557890731663, 0.3125065455256063, 0.24673931753200948, 0.20392838422961704, 0.15580585751863543, 0.13771913779418335, 0.10989751358685629, 0.10131781926149945, 0.0900929072309676, 0.08630223718145505, 0.07109347003502557, 0.07759925648790386, 0.07894099634879119, 0.059652544614022876, 0.06979044966007847, 0.060014518483689584, 0.07043072672517937, 0.05767723985383754, 0.05815388454118972, 0.06473925867629454, 0.0556794028372511, 0.05708471546485506, 0.04925561297501502, 0.06162466817235341, 0.05709614791318446, 0.04648693205032181, 0.05743846246172362, 0.04594403502744306, 0.05078570663464813, 0.05313572195383733, 0.04398207311280896, 0.04763752605725566, 0.04493362677196571, 0.04484021431876278, 0.044183579594333944, 0.04889711515304795, 0.04683627094183971, 0.04326125550968245, 0.055358322966444724, 0.04568549724826601, 0.040140624467991805, 0.03840590122609634, 0.053584007793990605, 0.03929476146203274, 0.04739670632361106, 0.04765509391121496, 0.03358992473844028, 0.04707975177980258, 0.03849361576447713, 0.04580956300712161, 0.05208675653434011, 0.042020576877703227, 0.029307750959965243, 0.044151141814978485, 0.035710669139425916, 0.046483689276436604, 0.04731927136051227, 0.03495536197810081, 0.04145735319283412, 0.04023817718876332, 0.0462213355752148, 0.036110142148975184, 0.04146637422304489, 0.044589179201533775, 0.04125126561668409, 0.029268305674213246, 0.031547221616967686, 0.054141148939260864, 0.029601577160684962, 0.0422806017775463, 0.04016990247279336, 0.03687381221934693, 0.03789400523253468, 0.03357993843323957, 0.0399331979604748, 0.04333878042548593, 0.0404920931345029, 0.033536393110041834, 0.03689722970165383, 0.043093080563088325, 0.033008244359934276, 0.03607754541027792, 0.028206142852867395, 0.04327622973403439, 0.03225828847272489, 0.0367656214506173, 0.04361213465135547, 0.04303866882710304, 0.02909428643704083, 0.02929968204206491, 0.04499189914304457, 0.037635077545543835, 0.02977045131344368, 0.043093462400884686, 0.036014517782442815, 0.03297480061911383, 0.036296472272245583, 0.032960147814139064, 0.025274147994165753, 0.049161286004248, 0.043875292445769584, 0.03404716463642834, 0.027775726351183867, 0.04253179481806439, 0.04284357498829417, 0.03496712074400097, 0.033402873233939305, 0.0346747573513131, 0.04145772318613909, 0.030067536123660286, 0.034974306897538915, 0.03375468372998219, 0.037959138235536795, 0.03368195285090678, 0.030605811146323957, 0.041014853893384155, 0.032670779090247314, 0.03260672624044012, 0.027754936356919543, 0.04330607079506564, 0.040654419513383555, 0.030322671349603664, 0.03189427854555434, 0.038858121217352826, 0.03551454013844226, 0.03873485607990691, 0.04057666656511157, 0.03458614366830274, 0.026335754118458354, 0.03641098875723612, 0.03809516050873214, 0.030115818295853653, 0.03739423294300166, 0.03489176236395174, 0.027823867873070205, 0.03931179529998888, 0.03143593369108075, 0.032761324691998696, 0.03639638179059, 0.03504647819875764, 0.033867980351081024, 0.03300839384360785, 0.05201517774660328, 0.031230885386752442, 0.03242123318139725, 0.028091454350253196, 0.026952538805528104, 0.04578718071686281, 0.03637733910527948, 0.0323983312821905, 0.03798065337968581, 0.035563550931934154, 0.038287182951094755, 0.04027047514967669, 0.037277919464527194, 0.03285580544508642, 0.03714195569150075, 0.03898513096599157, 0.03410393150718347, 0.024774205876429905, 0.04215243476839697, 0.034495131309721365, 0.033938274973021085, 0.030340070578827063, 0.03773543751643165, 0.04011085868076404, 0.03600967619459342, 0.026279430327250265]\n",
      "Starting run 4/5\n",
      "Epoch 1, Loss: 1.4185886343421839, Training Accuracy: 48.27%\n",
      "Epoch 2, Loss: 1.1002675535733744, Training Accuracy: 60.62%\n",
      "Epoch 3, Loss: 0.9715340757918784, Training Accuracy: 65.25%\n",
      "Epoch 4, Loss: 0.8847337794273405, Training Accuracy: 68.26%\n",
      "Epoch 5, Loss: 0.8084459929819912, Training Accuracy: 71.27%\n",
      "Epoch 6, Loss: 0.7501731958535626, Training Accuracy: 73.29%\n",
      "Epoch 7, Loss: 0.6951116273927567, Training Accuracy: 75.16%\n",
      "Epoch 8, Loss: 0.6408220688095483, Training Accuracy: 77.31%\n",
      "Epoch 9, Loss: 0.5948399248178048, Training Accuracy: 78.83%\n",
      "Epoch 10, Loss: 0.5497157530635214, Training Accuracy: 80.20%\n",
      "Epoch 11, Loss: 0.5126481948377531, Training Accuracy: 81.94%\n",
      "Epoch 12, Loss: 0.47521388433549716, Training Accuracy: 82.97%\n",
      "Epoch 13, Loss: 0.4464864167182342, Training Accuracy: 84.02%\n",
      "Epoch 14, Loss: 0.4125302730085295, Training Accuracy: 85.13%\n",
      "Epoch 15, Loss: 0.3769686300964916, Training Accuracy: 86.52%\n",
      "Epoch 16, Loss: 0.3526281264355725, Training Accuracy: 87.28%\n",
      "Epoch 17, Loss: 0.33275457268671305, Training Accuracy: 88.09%\n",
      "Epoch 18, Loss: 0.3048752019910709, Training Accuracy: 89.03%\n",
      "Epoch 19, Loss: 0.29345150809268206, Training Accuracy: 89.38%\n",
      "Epoch 20, Loss: 0.2786871219706505, Training Accuracy: 89.84%\n",
      "Epoch 21, Loss: 0.26208263675651283, Training Accuracy: 90.55%\n",
      "Epoch 22, Loss: 0.24031323595615603, Training Accuracy: 91.36%\n",
      "Epoch 23, Loss: 0.22238890292203944, Training Accuracy: 92.10%\n",
      "Epoch 24, Loss: 0.21090558357060413, Training Accuracy: 92.39%\n",
      "Epoch 25, Loss: 0.20623387194350554, Training Accuracy: 92.61%\n",
      "Epoch 26, Loss: 0.196350729909947, Training Accuracy: 92.96%\n",
      "Epoch 27, Loss: 0.1891120093948472, Training Accuracy: 93.30%\n",
      "Epoch 28, Loss: 0.17108706972511756, Training Accuracy: 93.86%\n",
      "Epoch 29, Loss: 0.17212707433573274, Training Accuracy: 93.90%\n",
      "Epoch 30, Loss: 0.17314486425665335, Training Accuracy: 93.95%\n",
      "Epoch 31, Loss: 0.15815274916170047, Training Accuracy: 94.48%\n",
      "Epoch 32, Loss: 0.14359026663172084, Training Accuracy: 94.82%\n",
      "Epoch 33, Loss: 0.15268531577099506, Training Accuracy: 94.69%\n",
      "Epoch 34, Loss: 0.1521423351819939, Training Accuracy: 94.78%\n",
      "Epoch 35, Loss: 0.14175348186655842, Training Accuracy: 95.07%\n",
      "Epoch 36, Loss: 0.14835431988236716, Training Accuracy: 94.96%\n",
      "Epoch 37, Loss: 0.12557772781623674, Training Accuracy: 95.54%\n",
      "Epoch 38, Loss: 0.12559897827622873, Training Accuracy: 95.69%\n",
      "Epoch 39, Loss: 0.14288318512515855, Training Accuracy: 95.13%\n",
      "Epoch 40, Loss: 0.11697709006483635, Training Accuracy: 95.99%\n",
      "Epoch 41, Loss: 0.12747225095220197, Training Accuracy: 95.74%\n",
      "Epoch 42, Loss: 0.11600895001924576, Training Accuracy: 95.99%\n",
      "Epoch 43, Loss: 0.12584369189863373, Training Accuracy: 95.83%\n",
      "Epoch 44, Loss: 0.11519048061660107, Training Accuracy: 96.05%\n",
      "Epoch 45, Loss: 0.12814499617731698, Training Accuracy: 95.85%\n",
      "Epoch 46, Loss: 0.10772713069992183, Training Accuracy: 96.38%\n",
      "Epoch 47, Loss: 0.11669963675692839, Training Accuracy: 96.17%\n",
      "Epoch 48, Loss: 0.12333759271939906, Training Accuracy: 95.98%\n",
      "Epoch 49, Loss: 0.11472174672640936, Training Accuracy: 96.10%\n",
      "Epoch 50, Loss: 0.10210524214511438, Training Accuracy: 96.56%\n",
      "Epoch 51, Loss: 0.10261161869261985, Training Accuracy: 96.59%\n",
      "Epoch 52, Loss: 0.11574679134053933, Training Accuracy: 96.24%\n",
      "Epoch 53, Loss: 0.12393011477367376, Training Accuracy: 96.01%\n",
      "Epoch 54, Loss: 0.09323562851787219, Training Accuracy: 96.88%\n",
      "Epoch 55, Loss: 0.10301193637836514, Training Accuracy: 96.63%\n",
      "Epoch 56, Loss: 0.10159374344343668, Training Accuracy: 96.82%\n",
      "Epoch 57, Loss: 0.10264236479670123, Training Accuracy: 96.69%\n",
      "Epoch 58, Loss: 0.1054041870587441, Training Accuracy: 96.66%\n",
      "Epoch 59, Loss: 0.09588234299076888, Training Accuracy: 96.93%\n",
      "Epoch 60, Loss: 0.1033589777246694, Training Accuracy: 96.74%\n",
      "Epoch 61, Loss: 0.09528393342757843, Training Accuracy: 96.98%\n",
      "Epoch 62, Loss: 0.10075141270698555, Training Accuracy: 96.80%\n",
      "Epoch 63, Loss: 0.10317153158299018, Training Accuracy: 96.83%\n",
      "Epoch 64, Loss: 0.10393954776010125, Training Accuracy: 96.77%\n",
      "Epoch 65, Loss: 0.09070725871882428, Training Accuracy: 97.12%\n",
      "Epoch 66, Loss: 0.08198616570210122, Training Accuracy: 97.40%\n",
      "Epoch 67, Loss: 0.10673597333949807, Training Accuracy: 96.80%\n",
      "Epoch 68, Loss: 0.10183683885401353, Training Accuracy: 96.92%\n",
      "Epoch 69, Loss: 0.06971713744097656, Training Accuracy: 97.76%\n",
      "Epoch 70, Loss: 0.11014168864438596, Training Accuracy: 96.64%\n",
      "Epoch 71, Loss: 0.09381626374238586, Training Accuracy: 97.10%\n",
      "Epoch 72, Loss: 0.10691623735036748, Training Accuracy: 96.83%\n",
      "Epoch 73, Loss: 0.07766257729539124, Training Accuracy: 97.66%\n",
      "Epoch 74, Loss: 0.09129860303523983, Training Accuracy: 97.29%\n",
      "Epoch 75, Loss: 0.0967758443369411, Training Accuracy: 97.15%\n",
      "Epoch 76, Loss: 0.10583464178578843, Training Accuracy: 96.95%\n",
      "Epoch 77, Loss: 0.08593379120293902, Training Accuracy: 97.43%\n",
      "Epoch 78, Loss: 0.09902028698066864, Training Accuracy: 97.17%\n",
      "Epoch 79, Loss: 0.09030457065575291, Training Accuracy: 97.31%\n",
      "Epoch 80, Loss: 0.08027325784407176, Training Accuracy: 97.62%\n",
      "Epoch 81, Loss: 0.09014421471752926, Training Accuracy: 97.39%\n",
      "Epoch 82, Loss: 0.09019008373698431, Training Accuracy: 97.37%\n",
      "Epoch 83, Loss: 0.0930272190653108, Training Accuracy: 97.37%\n",
      "Epoch 84, Loss: 0.09227805808612295, Training Accuracy: 97.31%\n",
      "Epoch 85, Loss: 0.08823180425217818, Training Accuracy: 97.42%\n",
      "Epoch 86, Loss: 0.09124050868625688, Training Accuracy: 97.38%\n",
      "Epoch 87, Loss: 0.08847661723543251, Training Accuracy: 97.53%\n",
      "Epoch 88, Loss: 0.08533578269643119, Training Accuracy: 97.53%\n",
      "Epoch 89, Loss: 0.09661941167195369, Training Accuracy: 97.34%\n",
      "Epoch 90, Loss: 0.08947428099041352, Training Accuracy: 97.44%\n",
      "Epoch 91, Loss: 0.08541053218346939, Training Accuracy: 97.59%\n",
      "Epoch 92, Loss: 0.0776039389536779, Training Accuracy: 97.73%\n",
      "Epoch 93, Loss: 0.08971400773051479, Training Accuracy: 97.45%\n",
      "Epoch 94, Loss: 0.09184752613893575, Training Accuracy: 97.40%\n",
      "Epoch 95, Loss: 0.08622464112477594, Training Accuracy: 97.57%\n",
      "Epoch 96, Loss: 0.08622700963199482, Training Accuracy: 97.64%\n",
      "Epoch 97, Loss: 0.0854894586558462, Training Accuracy: 97.60%\n",
      "Epoch 98, Loss: 0.08907737428087371, Training Accuracy: 97.61%\n",
      "Epoch 99, Loss: 0.08987389482530829, Training Accuracy: 97.53%\n",
      "Epoch 100, Loss: 0.07893518282179385, Training Accuracy: 97.71%\n",
      "Epoch 101, Loss: 0.08942854799956376, Training Accuracy: 97.63%\n",
      "Epoch 102, Loss: 0.08075675394106105, Training Accuracy: 97.71%\n",
      "Epoch 103, Loss: 0.08054286102290757, Training Accuracy: 97.77%\n",
      "Epoch 104, Loss: 0.09094973907630984, Training Accuracy: 97.53%\n",
      "Epoch 105, Loss: 0.09179124981060724, Training Accuracy: 97.60%\n",
      "Epoch 106, Loss: 0.0693504125931185, Training Accuracy: 97.99%\n",
      "Epoch 107, Loss: 0.08220097122930825, Training Accuracy: 97.77%\n",
      "Epoch 108, Loss: 0.08666932189045415, Training Accuracy: 97.68%\n",
      "Epoch 109, Loss: 0.08151013740901704, Training Accuracy: 97.75%\n",
      "Epoch 110, Loss: 0.0787916780921215, Training Accuracy: 97.86%\n",
      "Epoch 111, Loss: 0.09538012405242914, Training Accuracy: 97.55%\n",
      "Epoch 112, Loss: 0.08218388797844747, Training Accuracy: 97.87%\n",
      "Epoch 113, Loss: 0.09180594802666021, Training Accuracy: 97.60%\n",
      "Epoch 114, Loss: 0.06815254571039074, Training Accuracy: 98.14%\n",
      "Epoch 115, Loss: 0.08344641701075518, Training Accuracy: 97.81%\n",
      "Epoch 116, Loss: 0.08744847698525823, Training Accuracy: 97.70%\n",
      "Epoch 117, Loss: 0.0994800781637188, Training Accuracy: 97.55%\n",
      "Epoch 118, Loss: 0.09039287420329759, Training Accuracy: 97.78%\n",
      "Epoch 119, Loss: 0.08364207427397523, Training Accuracy: 97.84%\n",
      "Epoch 120, Loss: 0.08669748456464695, Training Accuracy: 97.83%\n",
      "Epoch 121, Loss: 0.06556786831862454, Training Accuracy: 98.16%\n",
      "Epoch 122, Loss: 0.08259492347020764, Training Accuracy: 97.87%\n",
      "Epoch 123, Loss: 0.08359543043637496, Training Accuracy: 97.92%\n",
      "Epoch 124, Loss: 0.07930776853444818, Training Accuracy: 97.91%\n",
      "Epoch 125, Loss: 0.09449278598033503, Training Accuracy: 97.59%\n",
      "Epoch 126, Loss: 0.08521636254355304, Training Accuracy: 97.92%\n",
      "Epoch 127, Loss: 0.08897102291783904, Training Accuracy: 97.84%\n",
      "Epoch 128, Loss: 0.0670155214651775, Training Accuracy: 98.33%\n",
      "Epoch 129, Loss: 0.09804810349198162, Training Accuracy: 97.62%\n",
      "Epoch 130, Loss: 0.08371885255796757, Training Accuracy: 97.86%\n",
      "Epoch 131, Loss: 0.07627528374632442, Training Accuracy: 98.07%\n",
      "Epoch 132, Loss: 0.0907349183661983, Training Accuracy: 97.85%\n",
      "Epoch 133, Loss: 0.09590146846893152, Training Accuracy: 97.73%\n",
      "Epoch 134, Loss: 0.08607871603294429, Training Accuracy: 97.96%\n",
      "Epoch 135, Loss: 0.07815043028900634, Training Accuracy: 98.03%\n",
      "Epoch 136, Loss: 0.07728938750518227, Training Accuracy: 98.10%\n",
      "Epoch 137, Loss: 0.07956754938349407, Training Accuracy: 98.07%\n",
      "Epoch 138, Loss: 0.10181271225025537, Training Accuracy: 97.62%\n",
      "Epoch 139, Loss: 0.07806762076336356, Training Accuracy: 98.07%\n",
      "Epoch 140, Loss: 0.06834923373126445, Training Accuracy: 98.19%\n",
      "Epoch 141, Loss: 0.10195630140708807, Training Accuracy: 97.68%\n",
      "Epoch 142, Loss: 0.06469283358630092, Training Accuracy: 98.40%\n",
      "Epoch 143, Loss: 0.08493756891297888, Training Accuracy: 97.95%\n",
      "Epoch 144, Loss: 0.08107189024268102, Training Accuracy: 98.12%\n",
      "Epoch 145, Loss: 0.0850530488765994, Training Accuracy: 98.04%\n",
      "Epoch 146, Loss: 0.08079426919685129, Training Accuracy: 98.18%\n",
      "Epoch 147, Loss: 0.08113841103381336, Training Accuracy: 98.04%\n",
      "Epoch 148, Loss: 0.10399346291808372, Training Accuracy: 97.66%\n",
      "Epoch 149, Loss: 0.07133085283851531, Training Accuracy: 98.20%\n",
      "Epoch 150, Loss: 0.07671261091167243, Training Accuracy: 98.22%\n",
      "Epoch 151, Loss: 0.08868002994779807, Training Accuracy: 98.07%\n",
      "Epoch 152, Loss: 0.08475462119427304, Training Accuracy: 98.11%\n",
      "Epoch 153, Loss: 0.08496781932504964, Training Accuracy: 98.08%\n",
      "Epoch 154, Loss: 0.08013400030751626, Training Accuracy: 98.09%\n",
      "Epoch 155, Loss: 0.08627330593073608, Training Accuracy: 98.04%\n",
      "Epoch 156, Loss: 0.07246282603326043, Training Accuracy: 98.29%\n",
      "Epoch 157, Loss: 0.07604243528143385, Training Accuracy: 98.28%\n",
      "Epoch 158, Loss: 0.08406931520400222, Training Accuracy: 98.06%\n",
      "Epoch 159, Loss: 0.08796900610878204, Training Accuracy: 98.04%\n",
      "Epoch 160, Loss: 0.07809462978676886, Training Accuracy: 98.24%\n",
      "Epoch 161, Loss: 0.08850171628602435, Training Accuracy: 98.06%\n",
      "Epoch 162, Loss: 0.07674664193035872, Training Accuracy: 98.30%\n",
      "Epoch 163, Loss: 0.0915969610448655, Training Accuracy: 97.94%\n",
      "Epoch 164, Loss: 0.07317026188036117, Training Accuracy: 98.35%\n",
      "Epoch 165, Loss: 0.09604768541437614, Training Accuracy: 97.97%\n",
      "Epoch 166, Loss: 0.06821667442034374, Training Accuracy: 98.39%\n",
      "Epoch 167, Loss: 0.09193139087482895, Training Accuracy: 98.03%\n",
      "Epoch 168, Loss: 0.07229256507612682, Training Accuracy: 98.34%\n",
      "Epoch 169, Loss: 0.0878710564462704, Training Accuracy: 98.15%\n",
      "Epoch 170, Loss: 0.09543194459673174, Training Accuracy: 97.98%\n",
      "Epoch 171, Loss: 0.09289777494973815, Training Accuracy: 98.02%\n",
      "Epoch 172, Loss: 0.07125509788198318, Training Accuracy: 98.34%\n",
      "Epoch 173, Loss: 0.06711622921375608, Training Accuracy: 98.52%\n",
      "Epoch 174, Loss: 0.10819060805299864, Training Accuracy: 97.88%\n",
      "Epoch 175, Loss: 0.07692208001336716, Training Accuracy: 98.38%\n",
      "Accuracy of the network on the 10000 test images: 59.94%\n",
      "Run 4 Summary:\n",
      "Training Accuracies: [48.27, 60.62, 65.254, 68.264, 71.266, 73.286, 75.162, 77.308, 78.834, 80.198, 81.942, 82.97, 84.016, 85.13, 86.52, 87.276, 88.092, 89.028, 89.376, 89.844, 90.552, 91.356, 92.098, 92.392, 92.614, 92.96, 93.296, 93.862, 93.896, 93.952, 94.476, 94.822, 94.694, 94.776, 95.074, 94.962, 95.54, 95.694, 95.126, 95.988, 95.74, 95.988, 95.834, 96.048, 95.85, 96.382, 96.172, 95.98, 96.104, 96.556, 96.594, 96.236, 96.01, 96.876, 96.634, 96.818, 96.692, 96.66, 96.93, 96.738, 96.98, 96.802, 96.828, 96.77, 97.122, 97.404, 96.804, 96.918, 97.758, 96.644, 97.102, 96.828, 97.656, 97.286, 97.148, 96.952, 97.43, 97.168, 97.31, 97.62, 97.388, 97.372, 97.366, 97.306, 97.416, 97.382, 97.534, 97.528, 97.344, 97.442, 97.592, 97.734, 97.454, 97.4, 97.57, 97.644, 97.6, 97.608, 97.526, 97.714, 97.628, 97.708, 97.774, 97.534, 97.602, 97.986, 97.774, 97.684, 97.754, 97.862, 97.552, 97.868, 97.6, 98.144, 97.806, 97.696, 97.548, 97.782, 97.836, 97.83, 98.164, 97.874, 97.92, 97.914, 97.592, 97.922, 97.84, 98.33, 97.618, 97.856, 98.07, 97.85, 97.73, 97.962, 98.03, 98.096, 98.068, 97.616, 98.068, 98.188, 97.678, 98.4, 97.948, 98.122, 98.044, 98.178, 98.036, 97.662, 98.198, 98.222, 98.066, 98.114, 98.076, 98.086, 98.044, 98.294, 98.282, 98.06, 98.036, 98.244, 98.056, 98.3, 97.94, 98.348, 97.966, 98.39, 98.034, 98.342, 98.15, 97.984, 98.018, 98.338, 98.516, 97.878, 98.38]\n",
      "Test Accuracy: 59.94%\n",
      "Losses: [1.4185886343421839, 1.1002675535733744, 0.9715340757918784, 0.8847337794273405, 0.8084459929819912, 0.7501731958535626, 0.6951116273927567, 0.6408220688095483, 0.5948399248178048, 0.5497157530635214, 0.5126481948377531, 0.47521388433549716, 0.4464864167182342, 0.4125302730085295, 0.3769686300964916, 0.3526281264355725, 0.33275457268671305, 0.3048752019910709, 0.29345150809268206, 0.2786871219706505, 0.26208263675651283, 0.24031323595615603, 0.22238890292203944, 0.21090558357060413, 0.20623387194350554, 0.196350729909947, 0.1891120093948472, 0.17108706972511756, 0.17212707433573274, 0.17314486425665335, 0.15815274916170047, 0.14359026663172084, 0.15268531577099506, 0.1521423351819939, 0.14175348186655842, 0.14835431988236716, 0.12557772781623674, 0.12559897827622873, 0.14288318512515855, 0.11697709006483635, 0.12747225095220197, 0.11600895001924576, 0.12584369189863373, 0.11519048061660107, 0.12814499617731698, 0.10772713069992183, 0.11669963675692839, 0.12333759271939906, 0.11472174672640936, 0.10210524214511438, 0.10261161869261985, 0.11574679134053933, 0.12393011477367376, 0.09323562851787219, 0.10301193637836514, 0.10159374344343668, 0.10264236479670123, 0.1054041870587441, 0.09588234299076888, 0.1033589777246694, 0.09528393342757843, 0.10075141270698555, 0.10317153158299018, 0.10393954776010125, 0.09070725871882428, 0.08198616570210122, 0.10673597333949807, 0.10183683885401353, 0.06971713744097656, 0.11014168864438596, 0.09381626374238586, 0.10691623735036748, 0.07766257729539124, 0.09129860303523983, 0.0967758443369411, 0.10583464178578843, 0.08593379120293902, 0.09902028698066864, 0.09030457065575291, 0.08027325784407176, 0.09014421471752926, 0.09019008373698431, 0.0930272190653108, 0.09227805808612295, 0.08823180425217818, 0.09124050868625688, 0.08847661723543251, 0.08533578269643119, 0.09661941167195369, 0.08947428099041352, 0.08541053218346939, 0.0776039389536779, 0.08971400773051479, 0.09184752613893575, 0.08622464112477594, 0.08622700963199482, 0.0854894586558462, 0.08907737428087371, 0.08987389482530829, 0.07893518282179385, 0.08942854799956376, 0.08075675394106105, 0.08054286102290757, 0.09094973907630984, 0.09179124981060724, 0.0693504125931185, 0.08220097122930825, 0.08666932189045415, 0.08151013740901704, 0.0787916780921215, 0.09538012405242914, 0.08218388797844747, 0.09180594802666021, 0.06815254571039074, 0.08344641701075518, 0.08744847698525823, 0.0994800781637188, 0.09039287420329759, 0.08364207427397523, 0.08669748456464695, 0.06556786831862454, 0.08259492347020764, 0.08359543043637496, 0.07930776853444818, 0.09449278598033503, 0.08521636254355304, 0.08897102291783904, 0.0670155214651775, 0.09804810349198162, 0.08371885255796757, 0.07627528374632442, 0.0907349183661983, 0.09590146846893152, 0.08607871603294429, 0.07815043028900634, 0.07728938750518227, 0.07956754938349407, 0.10181271225025537, 0.07806762076336356, 0.06834923373126445, 0.10195630140708807, 0.06469283358630092, 0.08493756891297888, 0.08107189024268102, 0.0850530488765994, 0.08079426919685129, 0.08113841103381336, 0.10399346291808372, 0.07133085283851531, 0.07671261091167243, 0.08868002994779807, 0.08475462119427304, 0.08496781932504964, 0.08013400030751626, 0.08627330593073608, 0.07246282603326043, 0.07604243528143385, 0.08406931520400222, 0.08796900610878204, 0.07809462978676886, 0.08850171628602435, 0.07674664193035872, 0.0915969610448655, 0.07317026188036117, 0.09604768541437614, 0.06821667442034374, 0.09193139087482895, 0.07229256507612682, 0.0878710564462704, 0.09543194459673174, 0.09289777494973815, 0.07125509788198318, 0.06711622921375608, 0.10819060805299864, 0.07692208001336716]\n",
      "Starting run 5/5\n",
      "Epoch 1, Loss: 1.2992200026731662, Training Accuracy: 53.27%\n",
      "Epoch 2, Loss: 0.9187794487037317, Training Accuracy: 67.41%\n",
      "Epoch 3, Loss: 0.7694361637467924, Training Accuracy: 72.79%\n",
      "Epoch 4, Loss: 0.6587444520972269, Training Accuracy: 76.97%\n",
      "Epoch 5, Loss: 0.5597083990454979, Training Accuracy: 80.31%\n",
      "Epoch 6, Loss: 0.47318260806143436, Training Accuracy: 83.53%\n",
      "Epoch 7, Loss: 0.3914894355017968, Training Accuracy: 86.41%\n",
      "Epoch 8, Loss: 0.3179039947612359, Training Accuracy: 89.02%\n",
      "Epoch 9, Loss: 0.25248669496620707, Training Accuracy: 91.36%\n",
      "Epoch 10, Loss: 0.1993598446078465, Training Accuracy: 93.25%\n",
      "Epoch 11, Loss: 0.15350684511911153, Training Accuracy: 94.88%\n",
      "Epoch 12, Loss: 0.11591982067612659, Training Accuracy: 96.18%\n",
      "Epoch 13, Loss: 0.10664838733499313, Training Accuracy: 96.29%\n",
      "Epoch 14, Loss: 0.08352738297651605, Training Accuracy: 97.33%\n",
      "Epoch 15, Loss: 0.07560431053850067, Training Accuracy: 97.52%\n",
      "Epoch 16, Loss: 0.07104272867286163, Training Accuracy: 97.62%\n",
      "Epoch 17, Loss: 0.06176472934679893, Training Accuracy: 97.89%\n",
      "Epoch 18, Loss: 0.06293602013136343, Training Accuracy: 97.88%\n",
      "Epoch 19, Loss: 0.05297672854887698, Training Accuracy: 98.23%\n",
      "Epoch 20, Loss: 0.06382554289235798, Training Accuracy: 97.82%\n",
      "Epoch 21, Loss: 0.05222725376377806, Training Accuracy: 98.21%\n",
      "Epoch 22, Loss: 0.05065345391258955, Training Accuracy: 98.29%\n",
      "Epoch 23, Loss: 0.05832170799274541, Training Accuracy: 97.99%\n",
      "Epoch 24, Loss: 0.03969361163068996, Training Accuracy: 98.63%\n",
      "Epoch 25, Loss: 0.04976832740301507, Training Accuracy: 98.35%\n",
      "Epoch 26, Loss: 0.054335514819288215, Training Accuracy: 98.18%\n",
      "Epoch 27, Loss: 0.040778570547503425, Training Accuracy: 98.64%\n",
      "Epoch 28, Loss: 0.042961072492370946, Training Accuracy: 98.57%\n",
      "Epoch 29, Loss: 0.044473026337994076, Training Accuracy: 98.55%\n",
      "Epoch 30, Loss: 0.04248253259491628, Training Accuracy: 98.65%\n",
      "Epoch 31, Loss: 0.05255854519365755, Training Accuracy: 98.26%\n",
      "Epoch 32, Loss: 0.04086039376774412, Training Accuracy: 98.58%\n",
      "Epoch 33, Loss: 0.036102924516851494, Training Accuracy: 98.85%\n",
      "Epoch 34, Loss: 0.04267319492242687, Training Accuracy: 98.60%\n",
      "Epoch 35, Loss: 0.04504674563114859, Training Accuracy: 98.51%\n",
      "Epoch 36, Loss: 0.04070089955750932, Training Accuracy: 98.62%\n",
      "Epoch 37, Loss: 0.028109776387747396, Training Accuracy: 99.10%\n",
      "Epoch 38, Loss: 0.04342420988487597, Training Accuracy: 98.54%\n",
      "Epoch 39, Loss: 0.041218850393425266, Training Accuracy: 98.70%\n",
      "Epoch 40, Loss: 0.0361945735911764, Training Accuracy: 98.87%\n",
      "Epoch 41, Loss: 0.038011802462316086, Training Accuracy: 98.78%\n",
      "Epoch 42, Loss: 0.030946170774680298, Training Accuracy: 98.96%\n",
      "Epoch 43, Loss: 0.03440068673830458, Training Accuracy: 98.86%\n",
      "Epoch 44, Loss: 0.03360298994330221, Training Accuracy: 98.94%\n",
      "Epoch 45, Loss: 0.04099837013598382, Training Accuracy: 98.71%\n",
      "Epoch 46, Loss: 0.039165669386578804, Training Accuracy: 98.68%\n",
      "Epoch 47, Loss: 0.02936685938918119, Training Accuracy: 99.06%\n",
      "Epoch 48, Loss: 0.039297154324984734, Training Accuracy: 98.77%\n",
      "Epoch 49, Loss: 0.025604886595480245, Training Accuracy: 99.18%\n",
      "Epoch 50, Loss: 0.0416866060052219, Training Accuracy: 98.71%\n",
      "Epoch 51, Loss: 0.02944462164282355, Training Accuracy: 99.03%\n",
      "Epoch 52, Loss: 0.03550161116660125, Training Accuracy: 98.92%\n",
      "Epoch 53, Loss: 0.038261070279544646, Training Accuracy: 98.89%\n",
      "Epoch 54, Loss: 0.03601661032230147, Training Accuracy: 98.84%\n",
      "Epoch 55, Loss: 0.02241447193843362, Training Accuracy: 99.26%\n",
      "Epoch 56, Loss: 0.026087540706705388, Training Accuracy: 99.16%\n",
      "Epoch 57, Loss: 0.047045940175655894, Training Accuracy: 98.65%\n",
      "Epoch 58, Loss: 0.02618351556419376, Training Accuracy: 99.22%\n",
      "Epoch 59, Loss: 0.03398252089359828, Training Accuracy: 98.96%\n",
      "Epoch 60, Loss: 0.03018937397270382, Training Accuracy: 99.13%\n",
      "Epoch 61, Loss: 0.02833795715657086, Training Accuracy: 99.13%\n",
      "Epoch 62, Loss: 0.03789285156305719, Training Accuracy: 98.91%\n",
      "Epoch 63, Loss: 0.028091331920807153, Training Accuracy: 99.14%\n",
      "Epoch 64, Loss: 0.04114765326375794, Training Accuracy: 98.83%\n",
      "Epoch 65, Loss: 0.025138197582817254, Training Accuracy: 99.23%\n",
      "Epoch 66, Loss: 0.030557735786641785, Training Accuracy: 99.08%\n",
      "Epoch 67, Loss: 0.024949876146507458, Training Accuracy: 99.29%\n",
      "Epoch 68, Loss: 0.033213741219934374, Training Accuracy: 99.01%\n",
      "Epoch 69, Loss: 0.030357528205726934, Training Accuracy: 99.12%\n",
      "Epoch 70, Loss: 0.031662935125707094, Training Accuracy: 99.04%\n",
      "Epoch 71, Loss: 0.024670455272017763, Training Accuracy: 99.26%\n",
      "Epoch 72, Loss: 0.027977286866298714, Training Accuracy: 99.19%\n",
      "Epoch 73, Loss: 0.03786156153299326, Training Accuracy: 98.90%\n",
      "Epoch 74, Loss: 0.02058523863154121, Training Accuracy: 99.34%\n",
      "Epoch 75, Loss: 0.032031541373909976, Training Accuracy: 99.09%\n",
      "Epoch 76, Loss: 0.027761684485666363, Training Accuracy: 99.22%\n",
      "Epoch 77, Loss: 0.0319199081044158, Training Accuracy: 99.07%\n",
      "Epoch 78, Loss: 0.03474175847392692, Training Accuracy: 99.03%\n",
      "Epoch 79, Loss: 0.021656454359790377, Training Accuracy: 99.34%\n",
      "Epoch 80, Loss: 0.03682906909951932, Training Accuracy: 98.99%\n",
      "Epoch 81, Loss: 0.028223629548568836, Training Accuracy: 99.18%\n",
      "Epoch 82, Loss: 0.026582445500635654, Training Accuracy: 99.22%\n",
      "Epoch 83, Loss: 0.029897063633258985, Training Accuracy: 99.19%\n",
      "Epoch 84, Loss: 0.03642708687502911, Training Accuracy: 99.00%\n",
      "Epoch 85, Loss: 0.02632188303368734, Training Accuracy: 99.31%\n",
      "Epoch 86, Loss: 0.023079699672243405, Training Accuracy: 99.33%\n",
      "Epoch 87, Loss: 0.03259276159369538, Training Accuracy: 99.15%\n",
      "Epoch 88, Loss: 0.02986875625327577, Training Accuracy: 99.16%\n",
      "Epoch 89, Loss: 0.02538068451978986, Training Accuracy: 99.30%\n",
      "Epoch 90, Loss: 0.025884108061249175, Training Accuracy: 99.32%\n",
      "Epoch 91, Loss: 0.028321768089885152, Training Accuracy: 99.16%\n",
      "Epoch 92, Loss: 0.024340193813681285, Training Accuracy: 99.29%\n",
      "Epoch 93, Loss: 0.034178213811152984, Training Accuracy: 99.09%\n",
      "Epoch 94, Loss: 0.020847821494577545, Training Accuracy: 99.44%\n",
      "Epoch 95, Loss: 0.034419215580666025, Training Accuracy: 99.05%\n",
      "Epoch 96, Loss: 0.021667070074174066, Training Accuracy: 99.41%\n",
      "Epoch 97, Loss: 0.0260199008398003, Training Accuracy: 99.32%\n",
      "Epoch 98, Loss: 0.025004693223747907, Training Accuracy: 99.29%\n",
      "Epoch 99, Loss: 0.027769716202484265, Training Accuracy: 99.21%\n",
      "Epoch 100, Loss: 0.024060086533921995, Training Accuracy: 99.34%\n",
      "Epoch 101, Loss: 0.033927407634166976, Training Accuracy: 99.09%\n",
      "Epoch 102, Loss: 0.0264968222047902, Training Accuracy: 99.31%\n",
      "Epoch 103, Loss: 0.024838234072248975, Training Accuracy: 99.37%\n",
      "Epoch 104, Loss: 0.030551623516721242, Training Accuracy: 99.18%\n",
      "Epoch 105, Loss: 0.028039902680281222, Training Accuracy: 99.33%\n",
      "Epoch 106, Loss: 0.021883021279866518, Training Accuracy: 99.41%\n",
      "Epoch 107, Loss: 0.031917798572417916, Training Accuracy: 99.18%\n",
      "Epoch 108, Loss: 0.02441268893601583, Training Accuracy: 99.36%\n",
      "Epoch 109, Loss: 0.025380382414818745, Training Accuracy: 99.35%\n",
      "Epoch 110, Loss: 0.02370530131408768, Training Accuracy: 99.37%\n",
      "Epoch 111, Loss: 0.027675070380242712, Training Accuracy: 99.32%\n",
      "Epoch 112, Loss: 0.028341165646491643, Training Accuracy: 99.28%\n",
      "Epoch 113, Loss: 0.028688555001455802, Training Accuracy: 99.31%\n",
      "Epoch 114, Loss: 0.028833517008672895, Training Accuracy: 99.27%\n",
      "Epoch 115, Loss: 0.022719437804426233, Training Accuracy: 99.39%\n",
      "Epoch 116, Loss: 0.03493802856994802, Training Accuracy: 99.17%\n",
      "Epoch 117, Loss: 0.020854911837768053, Training Accuracy: 99.45%\n",
      "Epoch 118, Loss: 0.03004918633628597, Training Accuracy: 99.30%\n",
      "Epoch 119, Loss: 0.026797872748044745, Training Accuracy: 99.26%\n",
      "Epoch 120, Loss: 0.029183581647332344, Training Accuracy: 99.28%\n",
      "Epoch 121, Loss: 0.02547696814688332, Training Accuracy: 99.36%\n",
      "Epoch 122, Loss: 0.03074472462128684, Training Accuracy: 99.28%\n",
      "Epoch 123, Loss: 0.024317900860109473, Training Accuracy: 99.40%\n",
      "Epoch 124, Loss: 0.023206249160122535, Training Accuracy: 99.43%\n",
      "Epoch 125, Loss: 0.03153465598867569, Training Accuracy: 99.27%\n",
      "Epoch 126, Loss: 0.025356815106945856, Training Accuracy: 99.38%\n",
      "Epoch 127, Loss: 0.026809279944097958, Training Accuracy: 99.34%\n",
      "Epoch 128, Loss: 0.02288162672593668, Training Accuracy: 99.47%\n",
      "Epoch 129, Loss: 0.022922429495847604, Training Accuracy: 99.45%\n",
      "Epoch 130, Loss: 0.025314612327180856, Training Accuracy: 99.37%\n",
      "Epoch 131, Loss: 0.027851863781482765, Training Accuracy: 99.32%\n",
      "Epoch 132, Loss: 0.030116872156667213, Training Accuracy: 99.30%\n",
      "Epoch 133, Loss: 0.0237451756760386, Training Accuracy: 99.39%\n",
      "Epoch 134, Loss: 0.022011804394045213, Training Accuracy: 99.48%\n",
      "Epoch 135, Loss: 0.023616468212362216, Training Accuracy: 99.41%\n",
      "Epoch 136, Loss: 0.030577365557537123, Training Accuracy: 99.33%\n",
      "Epoch 137, Loss: 0.026367267576355834, Training Accuracy: 99.38%\n",
      "Epoch 138, Loss: 0.021358166003841477, Training Accuracy: 99.45%\n",
      "Epoch 139, Loss: 0.027291027399665536, Training Accuracy: 99.39%\n",
      "Epoch 140, Loss: 0.0354060128762029, Training Accuracy: 99.22%\n",
      "Epoch 141, Loss: 0.022511926097431398, Training Accuracy: 99.52%\n",
      "Epoch 142, Loss: 0.019573028300276823, Training Accuracy: 99.51%\n",
      "Epoch 143, Loss: 0.02764676091625792, Training Accuracy: 99.39%\n",
      "Epoch 144, Loss: 0.031790136154742694, Training Accuracy: 99.31%\n",
      "Epoch 145, Loss: 0.028722321764962155, Training Accuracy: 99.36%\n",
      "Epoch 146, Loss: 0.017655792418248106, Training Accuracy: 99.55%\n",
      "Epoch 147, Loss: 0.025688044620858438, Training Accuracy: 99.39%\n",
      "Epoch 148, Loss: 0.02811617932898063, Training Accuracy: 99.30%\n",
      "Epoch 149, Loss: 0.025689740272564696, Training Accuracy: 99.39%\n",
      "Epoch 150, Loss: 0.027686220494695397, Training Accuracy: 99.43%\n",
      "Epoch 151, Loss: 0.02177301142206143, Training Accuracy: 99.52%\n",
      "Epoch 152, Loss: 0.019100751636880135, Training Accuracy: 99.55%\n",
      "Epoch 153, Loss: 0.030536932034506255, Training Accuracy: 99.33%\n",
      "Epoch 154, Loss: 0.02618272943627939, Training Accuracy: 99.47%\n",
      "Epoch 155, Loss: 0.0303266283864226, Training Accuracy: 99.34%\n",
      "Epoch 156, Loss: 0.025291248092066527, Training Accuracy: 99.43%\n",
      "Epoch 157, Loss: 0.02854724588973455, Training Accuracy: 99.44%\n",
      "Epoch 158, Loss: 0.027615249715386286, Training Accuracy: 99.44%\n",
      "Epoch 159, Loss: 0.029339297568532083, Training Accuracy: 99.40%\n",
      "Epoch 160, Loss: 0.024501726030125574, Training Accuracy: 99.50%\n",
      "Epoch 161, Loss: 0.025427197565549193, Training Accuracy: 99.47%\n",
      "Epoch 162, Loss: 0.02792381124630495, Training Accuracy: 99.49%\n",
      "Epoch 163, Loss: 0.022395971771759938, Training Accuracy: 99.56%\n",
      "Epoch 164, Loss: 0.02738021479197288, Training Accuracy: 99.45%\n",
      "Epoch 165, Loss: 0.025266462114949803, Training Accuracy: 99.48%\n",
      "Epoch 166, Loss: 0.029287645542990366, Training Accuracy: 99.41%\n",
      "Epoch 167, Loss: 0.02541587104479091, Training Accuracy: 99.46%\n",
      "Epoch 168, Loss: 0.030535500964246887, Training Accuracy: 99.36%\n",
      "Epoch 169, Loss: 0.024046268079431797, Training Accuracy: 99.50%\n",
      "Epoch 170, Loss: 0.029656409934051043, Training Accuracy: 99.39%\n",
      "Epoch 171, Loss: 0.01792851705104167, Training Accuracy: 99.58%\n",
      "Epoch 172, Loss: 0.02335221703295982, Training Accuracy: 99.47%\n",
      "Epoch 173, Loss: 0.030107422956476743, Training Accuracy: 99.42%\n",
      "Epoch 174, Loss: 0.03320447205481065, Training Accuracy: 99.37%\n",
      "Epoch 175, Loss: 0.02081167672097682, Training Accuracy: 99.57%\n",
      "Accuracy of the network on the 10000 test images: 66.98%\n",
      "Run 5 Summary:\n",
      "Training Accuracies: [53.272, 67.414, 72.792, 76.966, 80.314, 83.532, 86.406, 89.016, 91.356, 93.248, 94.878, 96.182, 96.286, 97.334, 97.516, 97.622, 97.886, 97.882, 98.232, 97.818, 98.208, 98.288, 97.994, 98.628, 98.35, 98.176, 98.642, 98.57, 98.552, 98.652, 98.264, 98.578, 98.846, 98.598, 98.506, 98.616, 99.1, 98.536, 98.704, 98.87, 98.784, 98.956, 98.862, 98.944, 98.714, 98.678, 99.06, 98.772, 99.176, 98.708, 99.026, 98.922, 98.886, 98.836, 99.256, 99.16, 98.654, 99.216, 98.956, 99.132, 99.126, 98.906, 99.144, 98.828, 99.228, 99.084, 99.288, 99.014, 99.124, 99.044, 99.262, 99.194, 98.9, 99.344, 99.09, 99.216, 99.07, 99.026, 99.338, 98.992, 99.184, 99.218, 99.19, 99.004, 99.308, 99.33, 99.148, 99.162, 99.302, 99.316, 99.158, 99.292, 99.088, 99.436, 99.052, 99.408, 99.322, 99.286, 99.21, 99.344, 99.092, 99.31, 99.368, 99.18, 99.326, 99.408, 99.178, 99.358, 99.348, 99.374, 99.316, 99.276, 99.308, 99.27, 99.388, 99.172, 99.454, 99.304, 99.262, 99.278, 99.36, 99.278, 99.404, 99.432, 99.27, 99.382, 99.342, 99.466, 99.448, 99.368, 99.316, 99.302, 99.388, 99.484, 99.408, 99.328, 99.376, 99.454, 99.386, 99.22, 99.518, 99.512, 99.394, 99.31, 99.36, 99.554, 99.392, 99.298, 99.39, 99.426, 99.522, 99.546, 99.33, 99.468, 99.342, 99.434, 99.436, 99.442, 99.398, 99.496, 99.47, 99.49, 99.556, 99.446, 99.478, 99.41, 99.456, 99.358, 99.502, 99.39, 99.582, 99.474, 99.416, 99.37, 99.566]\n",
      "Test Accuracy: 66.98%\n",
      "Losses: [1.2992200026731662, 0.9187794487037317, 0.7694361637467924, 0.6587444520972269, 0.5597083990454979, 0.47318260806143436, 0.3914894355017968, 0.3179039947612359, 0.25248669496620707, 0.1993598446078465, 0.15350684511911153, 0.11591982067612659, 0.10664838733499313, 0.08352738297651605, 0.07560431053850067, 0.07104272867286163, 0.06176472934679893, 0.06293602013136343, 0.05297672854887698, 0.06382554289235798, 0.05222725376377806, 0.05065345391258955, 0.05832170799274541, 0.03969361163068996, 0.04976832740301507, 0.054335514819288215, 0.040778570547503425, 0.042961072492370946, 0.044473026337994076, 0.04248253259491628, 0.05255854519365755, 0.04086039376774412, 0.036102924516851494, 0.04267319492242687, 0.04504674563114859, 0.04070089955750932, 0.028109776387747396, 0.04342420988487597, 0.041218850393425266, 0.0361945735911764, 0.038011802462316086, 0.030946170774680298, 0.03440068673830458, 0.03360298994330221, 0.04099837013598382, 0.039165669386578804, 0.02936685938918119, 0.039297154324984734, 0.025604886595480245, 0.0416866060052219, 0.02944462164282355, 0.03550161116660125, 0.038261070279544646, 0.03601661032230147, 0.02241447193843362, 0.026087540706705388, 0.047045940175655894, 0.02618351556419376, 0.03398252089359828, 0.03018937397270382, 0.02833795715657086, 0.03789285156305719, 0.028091331920807153, 0.04114765326375794, 0.025138197582817254, 0.030557735786641785, 0.024949876146507458, 0.033213741219934374, 0.030357528205726934, 0.031662935125707094, 0.024670455272017763, 0.027977286866298714, 0.03786156153299326, 0.02058523863154121, 0.032031541373909976, 0.027761684485666363, 0.0319199081044158, 0.03474175847392692, 0.021656454359790377, 0.03682906909951932, 0.028223629548568836, 0.026582445500635654, 0.029897063633258985, 0.03642708687502911, 0.02632188303368734, 0.023079699672243405, 0.03259276159369538, 0.02986875625327577, 0.02538068451978986, 0.025884108061249175, 0.028321768089885152, 0.024340193813681285, 0.034178213811152984, 0.020847821494577545, 0.034419215580666025, 0.021667070074174066, 0.0260199008398003, 0.025004693223747907, 0.027769716202484265, 0.024060086533921995, 0.033927407634166976, 0.0264968222047902, 0.024838234072248975, 0.030551623516721242, 0.028039902680281222, 0.021883021279866518, 0.031917798572417916, 0.02441268893601583, 0.025380382414818745, 0.02370530131408768, 0.027675070380242712, 0.028341165646491643, 0.028688555001455802, 0.028833517008672895, 0.022719437804426233, 0.03493802856994802, 0.020854911837768053, 0.03004918633628597, 0.026797872748044745, 0.029183581647332344, 0.02547696814688332, 0.03074472462128684, 0.024317900860109473, 0.023206249160122535, 0.03153465598867569, 0.025356815106945856, 0.026809279944097958, 0.02288162672593668, 0.022922429495847604, 0.025314612327180856, 0.027851863781482765, 0.030116872156667213, 0.0237451756760386, 0.022011804394045213, 0.023616468212362216, 0.030577365557537123, 0.026367267576355834, 0.021358166003841477, 0.027291027399665536, 0.0354060128762029, 0.022511926097431398, 0.019573028300276823, 0.02764676091625792, 0.031790136154742694, 0.028722321764962155, 0.017655792418248106, 0.025688044620858438, 0.02811617932898063, 0.025689740272564696, 0.027686220494695397, 0.02177301142206143, 0.019100751636880135, 0.030536932034506255, 0.02618272943627939, 0.0303266283864226, 0.025291248092066527, 0.02854724588973455, 0.027615249715386286, 0.029339297568532083, 0.024501726030125574, 0.025427197565549193, 0.02792381124630495, 0.022395971771759938, 0.02738021479197288, 0.025266462114949803, 0.029287645542990366, 0.02541587104479091, 0.030535500964246887, 0.024046268079431797, 0.029656409934051043, 0.01792851705104167, 0.02335221703295982, 0.030107422956476743, 0.03320447205481065, 0.02081167672097682]\n",
      "All Training Accuracies over Epochs for each run: [[51.17, 65.302, 70.542, 74.152, 77.658, 80.496, 83.198, 85.448, 87.762, 89.708, 91.552, 92.538, 93.748, 94.654, 95.31, 95.796, 95.826, 96.61, 96.226, 97.162, 96.634, 96.848, 97.128, 97.494, 97.406, 97.322, 97.426, 97.584, 97.43, 97.856, 97.624, 97.616, 98.018, 97.66, 97.878, 98.214, 97.906, 97.748, 97.926, 98.162, 97.692, 98.404, 98.188, 97.826, 98.462, 98.002, 98.376, 98.328, 98.298, 98.132, 98.308, 98.514, 97.948, 98.476, 98.394, 98.254, 98.37, 98.618, 98.204, 98.51, 98.362, 98.514, 98.404, 98.702, 98.556, 98.466, 98.41, 98.564, 98.616, 98.884, 98.556, 98.476, 98.41, 98.578, 98.66, 98.74, 98.61, 98.928, 98.5, 98.678, 98.748, 98.696, 98.88, 98.652, 98.708, 98.682, 98.844, 98.742, 98.804, 98.59, 98.762, 98.89, 98.764, 98.734, 98.68, 98.87, 99.058, 98.554, 98.856, 98.704, 99.018, 98.88, 98.982, 98.754, 98.682, 99.146, 98.874, 98.992, 98.684, 98.886, 98.796, 99.026, 98.772, 98.932, 98.788, 98.958, 99.046, 99.042, 98.908, 98.782, 99.182, 99.138, 98.846, 99.032, 98.722, 99.244, 98.886, 98.81, 99.282, 99.002, 98.906, 98.958, 99.024, 99.262, 98.936, 99.014, 98.938, 99.124, 99.018, 99.072, 99.218, 98.886, 99.15, 99.12, 99.004, 99.2, 98.98, 99.134, 99.04, 98.94, 99.182, 99.068, 99.13, 99.048, 99.22, 99.034, 99.156, 99.166, 99.068, 99.066, 99.122, 99.082, 99.09, 99.142, 99.146, 99.104, 98.99, 99.23, 99.01, 99.278, 99.246, 99.206, 99.1, 99.144, 99.202], [48.564, 61.384, 65.95, 69.138, 71.214, 73.1, 74.812, 76.508, 77.978, 79.44, 81.044, 82.284, 83.472, 84.776, 85.88, 87.234, 88.084, 89.042, 90.306, 91.38, 91.846, 92.78, 93.378, 94.014, 94.566, 95.152, 95.22, 95.656, 95.89, 96.272, 96.298, 96.648, 96.642, 96.688, 97.302, 96.79, 96.73, 97.452, 97.442, 97.114, 97.872, 97.378, 97.124, 97.928, 97.442, 97.778, 97.85, 97.672, 97.78, 98.0, 97.778, 97.84, 98.264, 97.668, 97.872, 98.112, 98.08, 97.782, 98.156, 97.814, 97.818, 98.462, 97.964, 98.028, 97.84, 98.276, 98.268, 98.29, 98.476, 98.148, 98.296, 98.246, 98.206, 98.118, 98.544, 98.486, 98.414, 98.214, 98.494, 98.278, 98.304, 98.322, 98.688, 98.444, 98.294, 98.598, 98.306, 98.74, 98.79, 98.146, 98.614, 98.444, 98.62, 98.594, 98.406, 98.896, 98.468, 98.358, 98.846, 98.64, 98.834, 98.47, 98.644, 98.648, 98.552, 98.726, 98.738, 98.586, 98.726, 98.724, 98.488, 98.522, 98.912, 98.572, 98.822, 98.82, 98.798, 98.78, 98.894, 98.834, 98.628, 98.804, 98.798, 98.606, 98.958, 98.574, 98.96, 98.466, 99.078, 98.722, 98.916, 98.644, 98.728, 98.906, 98.952, 98.614, 98.966, 98.908, 98.832, 98.784, 98.776, 98.874, 98.838, 98.952, 98.862, 99.074, 98.778, 98.728, 98.916, 98.946, 99.056, 98.948, 98.988, 98.788, 98.936, 98.964, 98.948, 99.086, 98.898, 99.056, 98.97, 98.996, 98.782, 98.974, 98.886, 99.13, 99.0, 98.802, 99.21, 99.046, 98.95, 98.932, 98.99, 99.084, 98.76], [51.652, 65.374, 70.834, 74.672, 78.244, 81.092, 84.202, 86.96, 88.978, 91.512, 92.786, 94.758, 95.172, 96.288, 96.48, 96.826, 96.992, 97.472, 97.35, 97.258, 97.898, 97.502, 97.88, 97.574, 98.094, 97.998, 97.786, 98.142, 98.046, 98.35, 97.992, 98.084, 98.446, 98.126, 98.444, 98.32, 98.254, 98.528, 98.458, 98.518, 98.548, 98.572, 98.434, 98.528, 98.642, 98.322, 98.588, 98.772, 98.814, 98.4, 98.802, 98.572, 98.544, 98.984, 98.608, 98.802, 98.592, 98.506, 98.818, 99.064, 98.734, 98.916, 98.674, 98.732, 98.988, 98.768, 98.87, 98.732, 99.0, 98.862, 98.766, 98.808, 99.12, 99.004, 98.568, 99.118, 98.87, 98.848, 98.952, 98.996, 99.1, 98.944, 98.894, 98.87, 99.106, 98.988, 98.846, 99.1, 99.03, 99.268, 98.894, 99.114, 99.004, 98.918, 98.91, 99.208, 99.2, 98.9, 99.07, 99.194, 98.934, 99.064, 99.148, 99.106, 99.166, 99.324, 98.906, 98.938, 99.176, 99.326, 99.078, 98.976, 99.186, 99.204, 99.176, 98.998, 99.278, 99.184, 99.166, 99.128, 99.236, 99.27, 99.118, 99.248, 99.158, 99.338, 99.042, 99.06, 99.336, 99.21, 99.146, 99.228, 99.166, 99.116, 99.216, 99.336, 99.186, 99.224, 99.31, 99.156, 99.256, 99.342, 99.106, 99.286, 99.256, 99.216, 99.242, 99.25, 99.294, 98.976, 99.332, 99.304, 99.422, 99.454, 99.09, 99.252, 99.396, 99.23, 99.302, 99.264, 99.226, 99.288, 99.334, 99.242, 99.226, 99.338, 99.492, 99.164, 99.342, 99.336, 99.384, 99.3, 99.26, 99.276, 99.444], [48.27, 60.62, 65.254, 68.264, 71.266, 73.286, 75.162, 77.308, 78.834, 80.198, 81.942, 82.97, 84.016, 85.13, 86.52, 87.276, 88.092, 89.028, 89.376, 89.844, 90.552, 91.356, 92.098, 92.392, 92.614, 92.96, 93.296, 93.862, 93.896, 93.952, 94.476, 94.822, 94.694, 94.776, 95.074, 94.962, 95.54, 95.694, 95.126, 95.988, 95.74, 95.988, 95.834, 96.048, 95.85, 96.382, 96.172, 95.98, 96.104, 96.556, 96.594, 96.236, 96.01, 96.876, 96.634, 96.818, 96.692, 96.66, 96.93, 96.738, 96.98, 96.802, 96.828, 96.77, 97.122, 97.404, 96.804, 96.918, 97.758, 96.644, 97.102, 96.828, 97.656, 97.286, 97.148, 96.952, 97.43, 97.168, 97.31, 97.62, 97.388, 97.372, 97.366, 97.306, 97.416, 97.382, 97.534, 97.528, 97.344, 97.442, 97.592, 97.734, 97.454, 97.4, 97.57, 97.644, 97.6, 97.608, 97.526, 97.714, 97.628, 97.708, 97.774, 97.534, 97.602, 97.986, 97.774, 97.684, 97.754, 97.862, 97.552, 97.868, 97.6, 98.144, 97.806, 97.696, 97.548, 97.782, 97.836, 97.83, 98.164, 97.874, 97.92, 97.914, 97.592, 97.922, 97.84, 98.33, 97.618, 97.856, 98.07, 97.85, 97.73, 97.962, 98.03, 98.096, 98.068, 97.616, 98.068, 98.188, 97.678, 98.4, 97.948, 98.122, 98.044, 98.178, 98.036, 97.662, 98.198, 98.222, 98.066, 98.114, 98.076, 98.086, 98.044, 98.294, 98.282, 98.06, 98.036, 98.244, 98.056, 98.3, 97.94, 98.348, 97.966, 98.39, 98.034, 98.342, 98.15, 97.984, 98.018, 98.338, 98.516, 97.878, 98.38], [53.272, 67.414, 72.792, 76.966, 80.314, 83.532, 86.406, 89.016, 91.356, 93.248, 94.878, 96.182, 96.286, 97.334, 97.516, 97.622, 97.886, 97.882, 98.232, 97.818, 98.208, 98.288, 97.994, 98.628, 98.35, 98.176, 98.642, 98.57, 98.552, 98.652, 98.264, 98.578, 98.846, 98.598, 98.506, 98.616, 99.1, 98.536, 98.704, 98.87, 98.784, 98.956, 98.862, 98.944, 98.714, 98.678, 99.06, 98.772, 99.176, 98.708, 99.026, 98.922, 98.886, 98.836, 99.256, 99.16, 98.654, 99.216, 98.956, 99.132, 99.126, 98.906, 99.144, 98.828, 99.228, 99.084, 99.288, 99.014, 99.124, 99.044, 99.262, 99.194, 98.9, 99.344, 99.09, 99.216, 99.07, 99.026, 99.338, 98.992, 99.184, 99.218, 99.19, 99.004, 99.308, 99.33, 99.148, 99.162, 99.302, 99.316, 99.158, 99.292, 99.088, 99.436, 99.052, 99.408, 99.322, 99.286, 99.21, 99.344, 99.092, 99.31, 99.368, 99.18, 99.326, 99.408, 99.178, 99.358, 99.348, 99.374, 99.316, 99.276, 99.308, 99.27, 99.388, 99.172, 99.454, 99.304, 99.262, 99.278, 99.36, 99.278, 99.404, 99.432, 99.27, 99.382, 99.342, 99.466, 99.448, 99.368, 99.316, 99.302, 99.388, 99.484, 99.408, 99.328, 99.376, 99.454, 99.386, 99.22, 99.518, 99.512, 99.394, 99.31, 99.36, 99.554, 99.392, 99.298, 99.39, 99.426, 99.522, 99.546, 99.33, 99.468, 99.342, 99.434, 99.436, 99.442, 99.398, 99.496, 99.47, 99.49, 99.556, 99.446, 99.478, 99.41, 99.456, 99.358, 99.502, 99.39, 99.582, 99.474, 99.416, 99.37, 99.566]]\n",
      "All Test Accuracies after each run: [[63.66], [64.02], [65.98], [59.94], [66.98]]\n",
      "All Losses over Epochs for each run: [[1.3494219994148635, 0.9814208072926992, 0.8356298074088133, 0.7299919880717002, 0.63582071055994, 0.5552120241134063, 0.4778876421815904, 0.41430928741041045, 0.34762262654921894, 0.29292867616619295, 0.24321787699561595, 0.21076269257727945, 0.18039121651007314, 0.15494676549797473, 0.13247217021434737, 0.12202590903567384, 0.1198809984779638, 0.09875401328472644, 0.10753591237422984, 0.08315114609306426, 0.09783185840747498, 0.08821104312151232, 0.08245842212327587, 0.07404946474372731, 0.07822781097253456, 0.07994936078982166, 0.07658994300207576, 0.07201124009881121, 0.07617712487785808, 0.06500426902229686, 0.06862043977990184, 0.07154680112057635, 0.05802399053594426, 0.07347977895091962, 0.06673880804108594, 0.053280079646198, 0.0630197105713396, 0.07292321335394507, 0.06310431856630846, 0.055643964284623655, 0.07402469538276672, 0.0495981971043154, 0.05888521254954078, 0.06940366407794415, 0.04867410723185774, 0.06341840127558784, 0.050416885934564074, 0.05312141296926804, 0.05580737756639201, 0.060791817370529286, 0.054809361604223825, 0.045900860011441835, 0.06751536052708891, 0.05123775020071878, 0.05433858845372062, 0.055119402584960134, 0.054167409221332036, 0.04501873624921271, 0.06143221241915348, 0.04901857899380666, 0.056822762858578506, 0.05124758591782875, 0.05191148485548992, 0.041650983319832774, 0.05077781228107743, 0.05294440287796957, 0.05372902190807424, 0.052165133042877024, 0.04629737349334573, 0.03654165578396235, 0.04924253311936114, 0.05517445135378659, 0.056883226097924566, 0.04834863272042784, 0.04582109974942754, 0.048549537841221686, 0.051910899737802656, 0.03785377249165866, 0.054603333208577104, 0.049816382440391216, 0.04403444061154237, 0.04504884399036533, 0.042707194856274146, 0.05077088317994307, 0.05036678125257762, 0.05036345093802204, 0.041722474577847814, 0.045735524100952815, 0.047661107325566866, 0.05574325227122091, 0.047344699102895224, 0.04078968727437937, 0.04569780654152745, 0.04622830067842478, 0.05022568720300326, 0.044480796817972935, 0.03507042633827776, 0.05688012767765994, 0.04616609441460064, 0.05168688289116748, 0.03915028337912453, 0.04404620183644943, 0.04073709795069825, 0.05074440307964226, 0.05150251386214125, 0.035365775348268735, 0.043002464255330775, 0.03960087072548335, 0.05426415149696697, 0.04598342836254571, 0.0496999714639055, 0.03940297967022567, 0.051378208403043295, 0.04368006373202157, 0.050380697261953045, 0.042830194948190084, 0.04137004320743181, 0.0381052736089797, 0.04517117261841201, 0.05465890958130629, 0.03047620848274327, 0.033518462584727314, 0.053905554202913646, 0.04100624394074123, 0.05580198179528944, 0.030493243830219202, 0.05214900130612922, 0.05349613634146744, 0.029411291167528164, 0.04281478731810892, 0.050433070322318384, 0.04798261029488682, 0.0409423986805132, 0.030127974398957785, 0.04733844258057082, 0.04390253207491608, 0.047692129427635156, 0.038005713739234885, 0.046226860148944025, 0.040523126328002715, 0.033968313123323976, 0.053194880481167287, 0.037532860260077, 0.043130733284628235, 0.04990667610783326, 0.0329915511816425, 0.04895344995968478, 0.03688568733232712, 0.043079762681339506, 0.05515014837203762, 0.04091230605537674, 0.04618779872338444, 0.0410433281078481, 0.04679280152276303, 0.037200092000636974, 0.048536063764177, 0.041194989453205315, 0.03897624429923384, 0.04283737533389572, 0.046002068745033646, 0.044457281379452734, 0.0435650420652791, 0.041342044808043044, 0.04322917366229957, 0.04461348667124198, 0.045515491471811685, 0.05452500201545295, 0.03732733280612245, 0.05219769632380703, 0.037968611398123846, 0.03636969861942209, 0.03992017833636889, 0.047498089535642586, 0.04600494168642561, 0.041717335035293304], [1.4163293008456754, 1.0791452498844518, 0.9499881208857612, 0.868969599228076, 0.8077892848216665, 0.7574323995034103, 0.7050806714217072, 0.6605011337934552, 0.6157228853696447, 0.5787174860610986, 0.5358124918606885, 0.498826170821324, 0.4627329172266414, 0.4293824739354041, 0.3976521817848201, 0.36425261054654867, 0.3365084996728031, 0.3098334405676026, 0.2775965527538448, 0.2477849342233842, 0.23135414888696446, 0.20851584242852142, 0.18813242553673742, 0.17103497764510114, 0.15695629473251607, 0.13975263739009494, 0.13659694797271277, 0.12389370628997036, 0.11745399211073661, 0.10892996952161574, 0.10495186207430138, 0.09722707879579509, 0.09675757439159181, 0.09243296950047983, 0.07664667776264154, 0.09028068110656441, 0.0931002419332371, 0.07434952324923233, 0.07567173732763818, 0.08145833805309433, 0.06371177329986696, 0.07705045293997545, 0.0853294745939748, 0.05961264045748269, 0.0745487960801005, 0.06392456093342151, 0.06041531441265793, 0.07132906127733576, 0.06737334943403873, 0.05797548827273376, 0.06361876758169192, 0.06254735159809174, 0.0504921095058433, 0.07000680948274753, 0.06617411371568203, 0.05537199717819812, 0.05658712785002058, 0.06270839341143813, 0.05405417049546009, 0.0650167510459346, 0.06341174769285551, 0.044876873402012504, 0.06036586427892608, 0.05902245019949899, 0.06466951696275104, 0.0499129203686942, 0.052434447090565155, 0.049594364790207186, 0.04531761277054646, 0.05630567309281733, 0.04999071792752989, 0.05138066350927522, 0.055802647567540525, 0.05858141850565335, 0.043502983307707674, 0.04576144999676675, 0.04778124669568537, 0.05541332448971734, 0.04751408734643782, 0.05138750859805906, 0.055274037081636446, 0.05234093030689426, 0.0386231567542342, 0.04664620223527779, 0.05445765020825086, 0.04238017354695414, 0.0514793188672985, 0.03879212535398356, 0.03735892436507455, 0.059592189058787104, 0.04389915318083381, 0.04991928593446801, 0.04213213166644019, 0.043571923487264304, 0.05017843400567348, 0.034666008810885114, 0.050166034081299035, 0.0543355359221705, 0.03693477928487375, 0.043181417188933924, 0.035304972659730975, 0.05326986756999224, 0.044394671768055755, 0.045180758569695355, 0.047971284665587734, 0.042366809311785554, 0.03938667629023499, 0.04477480155504093, 0.04266280805495161, 0.039037532774275456, 0.05160979400946131, 0.048253731686044146, 0.03414438625910567, 0.04752236085585402, 0.03767401121774554, 0.03534612648160485, 0.039830699953713716, 0.03753279759352935, 0.03505877554129061, 0.04060544746192803, 0.04466851417607505, 0.0411111660783407, 0.04004462841247312, 0.04791447764197408, 0.03493305637011942, 0.0468950165355506, 0.03438200550948857, 0.054467279060535036, 0.02987112625594846, 0.04547413409274012, 0.03536734345585173, 0.046829287604748485, 0.042889015587183554, 0.03435178659946571, 0.03381763690519277, 0.0473215097202733, 0.034706393702368436, 0.03899141936759276, 0.03961983401675594, 0.04302003032631905, 0.04348831962336641, 0.03873149558020558, 0.04025840557810078, 0.03533157960380944, 0.03999934721299435, 0.03260983229039547, 0.04191858359141683, 0.045293002135561894, 0.03982300975916159, 0.03841242400575657, 0.032461455211866434, 0.037656720992971945, 0.036769942569743065, 0.04465324396672949, 0.038218144282829894, 0.036383810001277134, 0.03883087651674615, 0.03121050567612663, 0.03786874829163544, 0.031818090446034585, 0.036533488055817506, 0.03459691003997745, 0.04403996599559176, 0.03606286131402567, 0.04028042828364734, 0.028572325140481795, 0.03734133949141059, 0.0444232622853133, 0.027211332173911078, 0.03625570303484099, 0.03733791235099375, 0.03846575647578197, 0.03805623113144449, 0.03350715759998225, 0.049480960629195624], [1.334589092475374, 0.9749450300965468, 0.8254432051306795, 0.7169686791003512, 0.6189110497836872, 0.5339433452509859, 0.45064001166454665, 0.37651557890731663, 0.3125065455256063, 0.24673931753200948, 0.20392838422961704, 0.15580585751863543, 0.13771913779418335, 0.10989751358685629, 0.10131781926149945, 0.0900929072309676, 0.08630223718145505, 0.07109347003502557, 0.07759925648790386, 0.07894099634879119, 0.059652544614022876, 0.06979044966007847, 0.060014518483689584, 0.07043072672517937, 0.05767723985383754, 0.05815388454118972, 0.06473925867629454, 0.0556794028372511, 0.05708471546485506, 0.04925561297501502, 0.06162466817235341, 0.05709614791318446, 0.04648693205032181, 0.05743846246172362, 0.04594403502744306, 0.05078570663464813, 0.05313572195383733, 0.04398207311280896, 0.04763752605725566, 0.04493362677196571, 0.04484021431876278, 0.044183579594333944, 0.04889711515304795, 0.04683627094183971, 0.04326125550968245, 0.055358322966444724, 0.04568549724826601, 0.040140624467991805, 0.03840590122609634, 0.053584007793990605, 0.03929476146203274, 0.04739670632361106, 0.04765509391121496, 0.03358992473844028, 0.04707975177980258, 0.03849361576447713, 0.04580956300712161, 0.05208675653434011, 0.042020576877703227, 0.029307750959965243, 0.044151141814978485, 0.035710669139425916, 0.046483689276436604, 0.04731927136051227, 0.03495536197810081, 0.04145735319283412, 0.04023817718876332, 0.0462213355752148, 0.036110142148975184, 0.04146637422304489, 0.044589179201533775, 0.04125126561668409, 0.029268305674213246, 0.031547221616967686, 0.054141148939260864, 0.029601577160684962, 0.0422806017775463, 0.04016990247279336, 0.03687381221934693, 0.03789400523253468, 0.03357993843323957, 0.0399331979604748, 0.04333878042548593, 0.0404920931345029, 0.033536393110041834, 0.03689722970165383, 0.043093080563088325, 0.033008244359934276, 0.03607754541027792, 0.028206142852867395, 0.04327622973403439, 0.03225828847272489, 0.0367656214506173, 0.04361213465135547, 0.04303866882710304, 0.02909428643704083, 0.02929968204206491, 0.04499189914304457, 0.037635077545543835, 0.02977045131344368, 0.043093462400884686, 0.036014517782442815, 0.03297480061911383, 0.036296472272245583, 0.032960147814139064, 0.025274147994165753, 0.049161286004248, 0.043875292445769584, 0.03404716463642834, 0.027775726351183867, 0.04253179481806439, 0.04284357498829417, 0.03496712074400097, 0.033402873233939305, 0.0346747573513131, 0.04145772318613909, 0.030067536123660286, 0.034974306897538915, 0.03375468372998219, 0.037959138235536795, 0.03368195285090678, 0.030605811146323957, 0.041014853893384155, 0.032670779090247314, 0.03260672624044012, 0.027754936356919543, 0.04330607079506564, 0.040654419513383555, 0.030322671349603664, 0.03189427854555434, 0.038858121217352826, 0.03551454013844226, 0.03873485607990691, 0.04057666656511157, 0.03458614366830274, 0.026335754118458354, 0.03641098875723612, 0.03809516050873214, 0.030115818295853653, 0.03739423294300166, 0.03489176236395174, 0.027823867873070205, 0.03931179529998888, 0.03143593369108075, 0.032761324691998696, 0.03639638179059, 0.03504647819875764, 0.033867980351081024, 0.03300839384360785, 0.05201517774660328, 0.031230885386752442, 0.03242123318139725, 0.028091454350253196, 0.026952538805528104, 0.04578718071686281, 0.03637733910527948, 0.0323983312821905, 0.03798065337968581, 0.035563550931934154, 0.038287182951094755, 0.04027047514967669, 0.037277919464527194, 0.03285580544508642, 0.03714195569150075, 0.03898513096599157, 0.03410393150718347, 0.024774205876429905, 0.04215243476839697, 0.034495131309721365, 0.033938274973021085, 0.030340070578827063, 0.03773543751643165, 0.04011085868076404, 0.03600967619459342, 0.026279430327250265], [1.4185886343421839, 1.1002675535733744, 0.9715340757918784, 0.8847337794273405, 0.8084459929819912, 0.7501731958535626, 0.6951116273927567, 0.6408220688095483, 0.5948399248178048, 0.5497157530635214, 0.5126481948377531, 0.47521388433549716, 0.4464864167182342, 0.4125302730085295, 0.3769686300964916, 0.3526281264355725, 0.33275457268671305, 0.3048752019910709, 0.29345150809268206, 0.2786871219706505, 0.26208263675651283, 0.24031323595615603, 0.22238890292203944, 0.21090558357060413, 0.20623387194350554, 0.196350729909947, 0.1891120093948472, 0.17108706972511756, 0.17212707433573274, 0.17314486425665335, 0.15815274916170047, 0.14359026663172084, 0.15268531577099506, 0.1521423351819939, 0.14175348186655842, 0.14835431988236716, 0.12557772781623674, 0.12559897827622873, 0.14288318512515855, 0.11697709006483635, 0.12747225095220197, 0.11600895001924576, 0.12584369189863373, 0.11519048061660107, 0.12814499617731698, 0.10772713069992183, 0.11669963675692839, 0.12333759271939906, 0.11472174672640936, 0.10210524214511438, 0.10261161869261985, 0.11574679134053933, 0.12393011477367376, 0.09323562851787219, 0.10301193637836514, 0.10159374344343668, 0.10264236479670123, 0.1054041870587441, 0.09588234299076888, 0.1033589777246694, 0.09528393342757843, 0.10075141270698555, 0.10317153158299018, 0.10393954776010125, 0.09070725871882428, 0.08198616570210122, 0.10673597333949807, 0.10183683885401353, 0.06971713744097656, 0.11014168864438596, 0.09381626374238586, 0.10691623735036748, 0.07766257729539124, 0.09129860303523983, 0.0967758443369411, 0.10583464178578843, 0.08593379120293902, 0.09902028698066864, 0.09030457065575291, 0.08027325784407176, 0.09014421471752926, 0.09019008373698431, 0.0930272190653108, 0.09227805808612295, 0.08823180425217818, 0.09124050868625688, 0.08847661723543251, 0.08533578269643119, 0.09661941167195369, 0.08947428099041352, 0.08541053218346939, 0.0776039389536779, 0.08971400773051479, 0.09184752613893575, 0.08622464112477594, 0.08622700963199482, 0.0854894586558462, 0.08907737428087371, 0.08987389482530829, 0.07893518282179385, 0.08942854799956376, 0.08075675394106105, 0.08054286102290757, 0.09094973907630984, 0.09179124981060724, 0.0693504125931185, 0.08220097122930825, 0.08666932189045415, 0.08151013740901704, 0.0787916780921215, 0.09538012405242914, 0.08218388797844747, 0.09180594802666021, 0.06815254571039074, 0.08344641701075518, 0.08744847698525823, 0.0994800781637188, 0.09039287420329759, 0.08364207427397523, 0.08669748456464695, 0.06556786831862454, 0.08259492347020764, 0.08359543043637496, 0.07930776853444818, 0.09449278598033503, 0.08521636254355304, 0.08897102291783904, 0.0670155214651775, 0.09804810349198162, 0.08371885255796757, 0.07627528374632442, 0.0907349183661983, 0.09590146846893152, 0.08607871603294429, 0.07815043028900634, 0.07728938750518227, 0.07956754938349407, 0.10181271225025537, 0.07806762076336356, 0.06834923373126445, 0.10195630140708807, 0.06469283358630092, 0.08493756891297888, 0.08107189024268102, 0.0850530488765994, 0.08079426919685129, 0.08113841103381336, 0.10399346291808372, 0.07133085283851531, 0.07671261091167243, 0.08868002994779807, 0.08475462119427304, 0.08496781932504964, 0.08013400030751626, 0.08627330593073608, 0.07246282603326043, 0.07604243528143385, 0.08406931520400222, 0.08796900610878204, 0.07809462978676886, 0.08850171628602435, 0.07674664193035872, 0.0915969610448655, 0.07317026188036117, 0.09604768541437614, 0.06821667442034374, 0.09193139087482895, 0.07229256507612682, 0.0878710564462704, 0.09543194459673174, 0.09289777494973815, 0.07125509788198318, 0.06711622921375608, 0.10819060805299864, 0.07692208001336716], [1.2992200026731662, 0.9187794487037317, 0.7694361637467924, 0.6587444520972269, 0.5597083990454979, 0.47318260806143436, 0.3914894355017968, 0.3179039947612359, 0.25248669496620707, 0.1993598446078465, 0.15350684511911153, 0.11591982067612659, 0.10664838733499313, 0.08352738297651605, 0.07560431053850067, 0.07104272867286163, 0.06176472934679893, 0.06293602013136343, 0.05297672854887698, 0.06382554289235798, 0.05222725376377806, 0.05065345391258955, 0.05832170799274541, 0.03969361163068996, 0.04976832740301507, 0.054335514819288215, 0.040778570547503425, 0.042961072492370946, 0.044473026337994076, 0.04248253259491628, 0.05255854519365755, 0.04086039376774412, 0.036102924516851494, 0.04267319492242687, 0.04504674563114859, 0.04070089955750932, 0.028109776387747396, 0.04342420988487597, 0.041218850393425266, 0.0361945735911764, 0.038011802462316086, 0.030946170774680298, 0.03440068673830458, 0.03360298994330221, 0.04099837013598382, 0.039165669386578804, 0.02936685938918119, 0.039297154324984734, 0.025604886595480245, 0.0416866060052219, 0.02944462164282355, 0.03550161116660125, 0.038261070279544646, 0.03601661032230147, 0.02241447193843362, 0.026087540706705388, 0.047045940175655894, 0.02618351556419376, 0.03398252089359828, 0.03018937397270382, 0.02833795715657086, 0.03789285156305719, 0.028091331920807153, 0.04114765326375794, 0.025138197582817254, 0.030557735786641785, 0.024949876146507458, 0.033213741219934374, 0.030357528205726934, 0.031662935125707094, 0.024670455272017763, 0.027977286866298714, 0.03786156153299326, 0.02058523863154121, 0.032031541373909976, 0.027761684485666363, 0.0319199081044158, 0.03474175847392692, 0.021656454359790377, 0.03682906909951932, 0.028223629548568836, 0.026582445500635654, 0.029897063633258985, 0.03642708687502911, 0.02632188303368734, 0.023079699672243405, 0.03259276159369538, 0.02986875625327577, 0.02538068451978986, 0.025884108061249175, 0.028321768089885152, 0.024340193813681285, 0.034178213811152984, 0.020847821494577545, 0.034419215580666025, 0.021667070074174066, 0.0260199008398003, 0.025004693223747907, 0.027769716202484265, 0.024060086533921995, 0.033927407634166976, 0.0264968222047902, 0.024838234072248975, 0.030551623516721242, 0.028039902680281222, 0.021883021279866518, 0.031917798572417916, 0.02441268893601583, 0.025380382414818745, 0.02370530131408768, 0.027675070380242712, 0.028341165646491643, 0.028688555001455802, 0.028833517008672895, 0.022719437804426233, 0.03493802856994802, 0.020854911837768053, 0.03004918633628597, 0.026797872748044745, 0.029183581647332344, 0.02547696814688332, 0.03074472462128684, 0.024317900860109473, 0.023206249160122535, 0.03153465598867569, 0.025356815106945856, 0.026809279944097958, 0.02288162672593668, 0.022922429495847604, 0.025314612327180856, 0.027851863781482765, 0.030116872156667213, 0.0237451756760386, 0.022011804394045213, 0.023616468212362216, 0.030577365557537123, 0.026367267576355834, 0.021358166003841477, 0.027291027399665536, 0.0354060128762029, 0.022511926097431398, 0.019573028300276823, 0.02764676091625792, 0.031790136154742694, 0.028722321764962155, 0.017655792418248106, 0.025688044620858438, 0.02811617932898063, 0.025689740272564696, 0.027686220494695397, 0.02177301142206143, 0.019100751636880135, 0.030536932034506255, 0.02618272943627939, 0.0303266283864226, 0.025291248092066527, 0.02854724588973455, 0.027615249715386286, 0.029339297568532083, 0.024501726030125574, 0.025427197565549193, 0.02792381124630495, 0.022395971771759938, 0.02738021479197288, 0.025266462114949803, 0.029287645542990366, 0.02541587104479091, 0.030535500964246887, 0.024046268079431797, 0.029656409934051043, 0.01792851705104167, 0.02335221703295982, 0.030107422956476743, 0.03320447205481065, 0.02081167672097682]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "def save_checkpoint(run, state, filename='checkpoint.pth.tar'):\n",
    "    os.makedirs(f'checkpoints/run_{run}', exist_ok=True)\n",
    "    filepath = os.path.join(f'checkpoints/run_{run}', filename)\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "def load_checkpoint(run, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(f'checkpoints/run_{run}', filename)\n",
    "    if os.path.exists(filepath):\n",
    "        return torch.load(filepath)\n",
    "    return None\n",
    "\n",
    "num_epochs = 175\n",
    "num_runs = 5\n",
    "\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "all_losses = []\n",
    "\n",
    "for run in range(1, num_runs + 1):\n",
    "    print(f\"Starting run {run}/{num_runs}\")\n",
    "\n",
    "    model = SimpleCNN()\n",
    "    optimizer = DAdaptAdam(model.parameters(), lr=1)\n",
    "\n",
    "    start_epoch = 0\n",
    "    checkpoint = load_checkpoint(run)\n",
    "    if checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        train_accuracies = checkpoint['train_accuracies']\n",
    "        losses = checkpoint['losses']\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    else:\n",
    "        train_accuracies = []\n",
    "        losses = []\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs): \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        losses.append(running_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            save_checkpoint(run, {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'losses': losses,\n",
    "            })\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    all_test_accuracies.append([test_accuracy])\n",
    "    print(f'Accuracy of the network on the 10000 test images: {test_accuracy:.2f}%')\n",
    "\n",
    "    all_train_accuracies.append(train_accuracies)\n",
    "    all_losses.append(losses)\n",
    "\n",
    "    print(f\"Run {run} Summary:\")\n",
    "    print(f\"Training Accuracies: {train_accuracies}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"Losses: {losses}\")\n",
    "\n",
    "    save_checkpoint(run, {\n",
    "        'epoch': num_epochs - 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'losses': losses,\n",
    "        'test_accuracy': test_accuracy\n",
    "    })\n",
    "\n",
    "all_train_accuracies = [list(train_accuracies) for train_accuracies in all_train_accuracies]\n",
    "all_test_accuracies = [list(test_accuracies) for test_accuracies in all_test_accuracies]\n",
    "all_losses = [list(losses) for losses in all_losses]\n",
    "\n",
    "print(\"All Training Accuracies over Epochs for each run:\", all_train_accuracies)\n",
    "print(\"All Test Accuracies after each run:\", all_test_accuracies)\n",
    "print(\"All Losses over Epochs for each run:\", all_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30763,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27406.216332,
   "end_time": "2024-08-28T12:53:25.583342",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-28T05:16:39.367010",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
